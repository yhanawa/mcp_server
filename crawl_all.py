import requests
from bs4 import BeautifulSoup
import html2text
import json
import time
import argparse
import re
import logging
from urllib.parse import urljoin, urlparse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class APIDocCrawler:
    def __init__(self, base_url, start_path, output_file, selector="main", delay=0.5, 
                 use_selenium=False, wait_time=5, additional_paths=None, path_pattern=None,
                 max_pages=None, headers=None):
        """
        æ±ç”¨APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼
        
        Args:
            base_url (str): ãƒ™ãƒ¼ã‚¹URL (ä¾‹: "https://ai.google.dev")
            start_path (str): ã‚¯ãƒ­ãƒ¼ãƒ«é–‹å§‹ãƒ‘ã‚¹ (ä¾‹: "/gemini-api/docs/")
            output_file (str): å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«å
            selector (str): ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å«ã‚€ä¸»è¦HTMLè¦ç´ ã®ã‚»ãƒ¬ã‚¯ã‚¿
            delay (float): ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®é…å»¶ï¼ˆç§’ï¼‰
            use_selenium (bool): Seleniumã‚’ä½¿ç”¨ã—ã¦å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã™ã‚‹ã‹
            wait_time (int): Seleniumã§ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰å¾…æ©Ÿã™ã‚‹ç§’æ•°
            additional_paths (list): ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹è¿½åŠ ãƒ‘ã‚¹ã®ãƒªã‚¹ãƒˆ
            path_pattern (str): å¯¾è±¡ã¨ã™ã‚‹ãƒ‘ã‚¹ã®æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³
            max_pages (int): ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ï¼ˆNoneã®å ´åˆã¯ç„¡åˆ¶é™ï¼‰
            headers (dict): ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ˜ãƒƒãƒ€ãƒ¼
        """
        self.base_url = base_url
        self.start_path = start_path
        self.start_url = urljoin(base_url, start_path)
        self.output_file = output_file
        self.content_selector = selector
        self.delay = delay
        self.use_selenium = use_selenium
        self.wait_time = wait_time
        self.additional_paths = additional_paths or []
        self.path_pattern = path_pattern
        self.max_pages = max_pages
        self.headers = headers or {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        self.visited = set()
        self.docs = []
        self.driver = None
        
        # ãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        self.path_regex = re.compile(self.path_pattern) if self.path_pattern else None
        
        # HTML â†’ Markdown å¤‰æ›å™¨
        self.converter = html2text.HTML2Text()
        self.converter.ignore_links = False
        self.converter.ignore_images = True
        self.converter.body_width = 0
        
        # ãƒšãƒ¼ã‚¸ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        self.page_count = 0

    def setup_selenium(self):
        """Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®è¨­å®š"""
        if not self.use_selenium:
            return
            
        chrome_options = Options()
        chrome_options.add_argument("--headless")
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        
        self.driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()),
            options=chrome_options
        )
        logger.info("Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

    def is_valid_link(self, href):
        """ãƒªãƒ³ã‚¯ãŒæœ‰åŠ¹ã‹ã¤ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã‹ã‚’åˆ¤å®š"""
        if not href:
            return False
        if href.startswith("#"):
            return False
        if "mailto:" in href or "javascript:" in href:
            return False
            
        full_url = urljoin(self.base_url, href)
        parsed = urlparse(full_url)
        
        # åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³å†…ã‹ã‚’ç¢ºèª
        if parsed.netloc != urlparse(self.base_url).netloc:
            return False
            
        # ãƒ‘ã‚¹ãŒé–‹å§‹ãƒ‘ã‚¹ã§å§‹ã¾ã‚‹ã‹ã€ã¾ãŸã¯è¿½åŠ ãƒ‘ã‚¹ã®ã„ãšã‚Œã‹ã§å§‹ã¾ã‚‹ã‹ã‚’ç¢ºèª
        starts_with_valid_path = parsed.path.startswith(self.start_path) or any(
            parsed.path.startswith(path) for path in self.additional_paths
        )
        
        # æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã«ã‚‚ãƒãƒƒãƒã™ã‚‹ã‹ç¢ºèª
        if self.path_regex:
            return starts_with_valid_path and self.path_regex.match(parsed.path)
        
        return starts_with_valid_path

    def get_page_content(self, url):
        """URLã‹ã‚‰ãƒšãƒ¼ã‚¸ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã™ã‚‹ï¼ˆé€šå¸¸ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹Seleniumã‚’ä½¿ç”¨ï¼‰"""
        if self.use_selenium:
            try:
                logger.info(f"ğŸŒ Seleniumã§ãƒ•ã‚§ãƒƒãƒä¸­: {url}")
                self.driver.get(url)
                
                # ç‰¹å®šã®ã‚»ãƒ¬ã‚¯ã‚¿ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
                selectors = self.content_selector.split(',')
                for selector in selectors:
                    selector = selector.strip()
                    try:
                        WebDriverWait(self.driver, self.wait_time).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                        )
                        logger.debug(f"ã‚»ãƒ¬ã‚¯ã‚¿ '{selector}' ã®è¡¨ç¤ºã‚’ç¢ºèª: {url}")
                        break
                    except Exception:
                        logger.debug(f"ã‚»ãƒ¬ã‚¯ã‚¿ '{selector}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€æ¬¡ã‚’è©¦ã—ã¾ã™")
                        continue
                
                # SPAã®å ´åˆã€JavaScriptã®å®Ÿè¡Œå®Œäº†ã‚’å¾…ã¤ãŸã‚ã«å°‘ã—å¾…æ©Ÿ
                time.sleep(1)
                
                # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦å…¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆSPAã®å ´åˆã«æœ‰åŠ¹ï¼‰
                self.scroll_to_bottom()
                
                html = self.driver.page_source
                return html
            except Exception as e:
                logger.error(f"âŒ Seleniumã§ã®å–å¾—ã«å¤±æ•— {url}: {e}")
                # é€šå¸¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                logger.info("é€šå¸¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¸ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
                
        # é€šå¸¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        try:
            logger.info(f"ğŸ“¥ é€šå¸¸HTTPã§ãƒ•ã‚§ãƒƒãƒä¸­: {url}")
            res = requests.get(url, headers=self.headers, timeout=30)
            res.raise_for_status()
            return res.text
        except Exception as e:
            logger.error(f"âŒ Failed to fetch {url}: {e}")
            return None
            
    def scroll_to_bottom(self):
        """ãƒšãƒ¼ã‚¸ã‚’ä¸‹ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆSPAã‚µã‚¤ãƒˆç”¨ï¼‰"""
        if not self.driver:
            return
            
        try:
            # æœ€åˆã®é«˜ã•ã‚’å–å¾—
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            
            for _ in range(3):  # æœ€å¤§3å›ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«è©¦è¡Œ
                # ä¸‹ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                
                # èª­ã¿è¾¼ã¿ã‚’å¾…ã¤
                time.sleep(2)
                
                # æ–°ã—ã„é«˜ã•ã‚’è¨ˆç®—
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                
                # é«˜ã•ãŒå¤‰ã‚ã‚‰ãªã‘ã‚Œã°èª­ã¿è¾¼ã¿å®Œäº†
                if new_height == last_height:
                    break
                    
                last_height = new_height
                
            # ãƒšãƒ¼ã‚¸ãƒˆãƒƒãƒ—ã«æˆ»ã‚‹
            self.driver.execute_script("window.scrollTo(0, 0);")
        except Exception as e:
            logger.warning(f"ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            
    def extract_links(self, soup, base_url):
        """HTMLã‹ã‚‰ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        links = []
        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            if self.is_valid_link(href):
                full_url = urljoin(base_url, href)
                # é‡è¤‡ã—ã¦ã„ãªã‘ã‚Œã°è¿½åŠ 
                if full_url not in links and full_url not in self.visited:
                    links.append(full_url)
                    
        logger.debug(f"æŠ½å‡ºã—ãŸãƒªãƒ³ã‚¯æ•°: {len(links)}")
        return links

    def crawl(self, url):
        """æŒ‡å®šURLã¨ãã®ãƒªãƒ³ã‚¯å…ˆã‚’å†å¸°çš„ã«ã‚¯ãƒ­ãƒ¼ãƒ«"""
        if url in self.visited:
            return
            
        # æœ€å¤§ãƒšãƒ¼ã‚¸æ•°ãƒã‚§ãƒƒã‚¯
        if self.max_pages is not None and self.page_count >= self.max_pages:
            logger.info(f"æœ€å¤§ãƒšãƒ¼ã‚¸æ•° {self.max_pages} ã«é”ã—ã¾ã—ãŸã€‚ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’åœæ­¢ã—ã¾ã™ã€‚")
            return
            
        self.visited.add(url)
        self.page_count += 1

        html_content = self.get_page_content(url)
        if not html_content:
            return

        soup = BeautifulSoup(html_content, "html.parser")
        
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„éƒ¨åˆ†ã‚’å–å¾—ï¼ˆæŒ‡å®šã‚»ãƒ¬ã‚¯ã‚¿ã‹bodyï¼‰
        content = None
        for selector in self.content_selector.split(','):
            selector = selector.strip()
            found_content = soup.select_one(selector)
            if found_content:
                content = found_content
                logger.debug(f"ã‚»ãƒ¬ã‚¯ã‚¿ '{selector}' ã§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¦‹ã¤ã‘ã¾ã—ãŸ: {url}")
                break
                
        if not content:
            content = soup.body
            logger.warning(f"æŒ‡å®šã‚»ãƒ¬ã‚¯ã‚¿ '{self.content_selector}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {url}")

        # ç©ºã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if not content or len(str(content).strip()) < 100:
            logger.warning(f"ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒç©ºã¾ãŸã¯çŸ­ã™ãã¾ã™: {url}")
            return

        markdown = self.converter.handle(str(content))
        title = soup.title.string.strip() if soup.title else url

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        description = ""
        meta_desc = soup.find("meta", attrs={"name": "description"})
        if meta_desc and "content" in meta_desc.attrs:
            description = meta_desc["content"]

        self.docs.append({
            "url": url,
            "title": title,
            "description": description,
            "content": markdown
        })
        
        logger.info(f"âœ“ ä¿å­˜: {title} ({url}) - {self.page_count}ãƒšãƒ¼ã‚¸ç›®")

        # æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹ã—ã¦å†å¸°çš„ã«ã‚¯ãƒ­ãƒ¼ãƒ«
        links = self.extract_links(soup, url)
        for link in links:
            if self.max_pages is not None and self.page_count >= self.max_pages:
                logger.info(f"æœ€å¤§ãƒšãƒ¼ã‚¸æ•° {self.max_pages} ã«é”ã—ã¾ã—ãŸã€‚ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’åœæ­¢ã—ã¾ã™ã€‚")
                break
                
            time.sleep(self.delay)  # ã‚µãƒ¼ãƒãƒ¼ã«å„ªã—ã
            self.crawl(link)

    def run(self):
        """ã‚¯ãƒ­ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã—çµæœã‚’ä¿å­˜"""
        if self.use_selenium:
            self.setup_selenium()
            
        try:
            # ãƒ¡ã‚¤ãƒ³é–‹å§‹URL
            self.crawl(self.start_url)
            
            # è¿½åŠ ãƒ‘ã‚¹ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚‚ã‚¯ãƒ­ãƒ¼ãƒ«
            for path in self.additional_paths:
                additional_url = urljoin(self.base_url, path)
                if additional_url not in self.visited:
                    self.crawl(additional_url)
                    
            # çµæœã‚’ä¿å­˜
            with open(self.output_file, "w", encoding="utf-8") as f:
                json.dump(self.docs, f, indent=2, ensure_ascii=False)

            logger.info(f"âœ… å…¨ {len(self.docs)} ãƒšãƒ¼ã‚¸ã‚’ä¿å­˜å®Œäº†: {self.output_file}")
            return len(self.docs)
        finally:
            # Seleniumã‚’ä½¿ç”¨ã—ãŸå ´åˆã¯ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if self.driver:
                self.driver.quit()
                logger.info("Seleniumãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã‚’çµ‚äº†ã—ã¾ã—ãŸ")



def anthropic_crawl():
    """Anthropic APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¯ãƒ­ãƒ¼ãƒ«ç”¨è¨­å®š"""
    return {
        "base_url": "https://docs.anthropic.com",
        "start_path": "/en/api/getting-started",
        "additional_paths": [
            "/en/api/messages",
            "/en/api/rate-limits",
            "/en/api/system-prompts",
            "/en/api/human-in-the-loop"
        ],
        "output_file": "document/anthropic_docs.json",
        "selector": ".docs-content, main, article, .content-wrapper, .content",
        "use_selenium": True,
        "path_pattern": r"^/en/api/.*",
        "max_pages": 200,
        "wait_time": 8,
        "delay": 1.0
    }

def gemini_crawl():
    """Google Gemini APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ã‚¯ãƒ­ãƒ¼ãƒ«ç”¨è¨­å®š"""
    return {
        "base_url": "https://ai.google.dev",
        "start_path": "/gemini-api/docs/",
        "additional_paths": [
            "/gemini-api/docs/models",
            "/gemini-api/docs/quickstart"
        ],
        "output_file": "document/gemini_docs.json",
        "selector": "main, article, .devsite-article-body, .devsite-article-inner",
        "use_selenium": False,
        "path_pattern": r"^/gemini-api/docs/.*",
        "max_pages": 200,
        "delay": 0.5
    }

# ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®š
PRESETS = {
    "anthropic": anthropic_crawl,
    "gemini": gemini_crawl
}

if __name__ == "__main__":
    # Anthropic APIãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”¨ã®è¨­å®šã‚’æ›´æ–°
    anthropic_config = anthropic_crawl()
    anthropic_config.update({
        "selector": ".docs-content, main, article, .content-wrapper",
        "use_selenium": True,
        "delay": 1.0
    })
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è§£æ
    parser = argparse.ArgumentParser(description="API Documentation Crawler")
    
    # ãƒ—ãƒªã‚»ãƒƒãƒˆè¨­å®šã‹è©³ç´°è¨­å®šã‹ã®ã‚°ãƒ«ãƒ¼ãƒ—
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument("--preset", choices=PRESETS.keys(), help="äº‹å‰å®šç¾©ã•ã‚ŒãŸè¨­å®šã‚’ä½¿ç”¨ (anthropic, gemini)")
    group.add_argument("--base-url", help="ãƒ™ãƒ¼ã‚¹URL (ä¾‹: https://ai.google.dev)")
    
    # ãã®ä»–ã®å¼•æ•°
    parser.add_argument("--start-path", help="é–‹å§‹ãƒ‘ã‚¹ (ä¾‹: /gemini-api/docs/)")
    parser.add_argument("--output_file", help="å‡ºåŠ›JSONãƒ•ã‚¡ã‚¤ãƒ«å")
    parser.add_argument("--selector", default="main", help="ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å«ã‚€è¦ç´ ã®CSSã‚»ãƒ¬ã‚¯ã‚¿ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: main)")
    parser.add_argument("--delay", type=float, default=0.5, help="ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã®é…å»¶ï¼ˆç§’ï¼‰(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 0.5)")
    parser.add_argument("--use-selenium", action="store_true", help="å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«Seleniumã‚’ä½¿ç”¨ã™ã‚‹")
    parser.add_argument("--wait-time", type=int, default=5, help="Seleniumã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰(ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5)")
    parser.add_argument("--additional-paths", nargs="+", help="ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã®è¿½åŠ ãƒ‘ã‚¹")
    parser.add_argument("--path-pattern", help="ã‚¯ãƒ­ãƒ¼ãƒ«å¯¾è±¡ã®ãƒ‘ã‚¹æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³")
    parser.add_argument("--debug", action="store_true", help="ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–")
    parser.add_argument("--max-pages", type=int, default=None, help="ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹æœ€å¤§ãƒšãƒ¼ã‚¸æ•°")
    
    args = parser.parse_args()
    
    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®è¨­å®š
    if args.debug:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # è¨­å®šã‚’æ±ºå®š
    if args.preset:
        config = PRESETS[args.preset]()
    else:
        if not args.base_url or not args.start_path or not args.output_file:
            parser.error("--preset ã‚’ä½¿ç”¨ã—ãªã„å ´åˆã¯ --base-url, --start-path, --output ãŒå¿…é ˆã§ã™")
            
        config = {
            "base_url": args.base_url,
            "start_path": args.start_path,
            "output_file": args.output_file,
            "selector": args.selector,
            "delay": args.delay,
            "use_selenium": args.use_selenium,
            "wait_time": args.wait_time,
            "additional_paths": args.additional_paths,
            "path_pattern": args.path_pattern,
            "max_pages": args.max_pages
        }
    
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§ä¸Šæ›¸ã
    for key, value in vars(args).items():
        if key not in ["preset", "debug"] and value is not None:
            snake_key = key.replace("-", "_")
            config[snake_key] = value
    
    logger.info(f"è¨­å®š: {config}")
    
    # ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã®ä½œæˆã¨å®Ÿè¡Œ
    crawler = APIDocCrawler(**config)
    crawler.run()