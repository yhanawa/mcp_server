[
  {
    "url": "https://docs.anthropic.com/en/api/getting-started",
    "title": "Getting started - Anthropic",
    "description": "",
    "content": "[Anthropic home page](/)\n\nEnglish\n\nSearch...\n\n  * [Research](https://www.anthropic.com/research)\n  * [News](https://www.anthropic.com/news)\n  * [Go to claude.ai](https://claude.ai/)\n  * [Go to claude.ai](https://claude.ai/)\n\n\n\nSearch...\n\nNavigation\n\nUsing the API\n\nGetting started\n\n[Welcome](/en/home)[User Guides](/en/docs/welcome)[API Reference](/en/api/getting-started)[Prompt Library](/en/prompt-library/library)[Release Notes](/en/release-notes/overview)\n\n* [Developer Console](https://console.anthropic.com/)\n* [Developer Discord](https://www.anthropic.com/discord)\n* [Support](https://support.anthropic.com/)\n\n##### Using the API\n\n  * [Getting started](/en/api/getting-started)\n  * [IP addresses](/en/api/ip-addresses)\n  * [Versions](/en/api/versioning)\n  * [Errors](/en/api/errors)\n  * [Rate limits](/en/api/rate-limits)\n  * [Client SDKs](/en/api/client-sdks)\n  * [Supported regions](/en/api/supported-regions)\n  * [Getting help](/en/api/getting-help)\n\n\n\n##### Anthropic APIs\n\n  * Messages\n\n  * Models\n\n  * Message Batches\n\n  * Text Completions (Legacy)\n\n  * Admin API\n\n\n\n\n##### OpenAI SDK compatibility\n\n  * [OpenAI SDK compatibility (beta)](/en/api/openai-sdk)\n\n\n\n##### Experimental APIs\n\n  * Prompt tools\n\n\n\n\n##### Amazon Bedrock API\n\n  * [Amazon Bedrock API](/en/api/claude-on-amazon-bedrock)\n\n\n\n##### Vertex AI\n\n  * [Vertex AI API](/en/api/claude-on-vertex-ai)\n\n\n\nUsing the API\n\n# Getting started\n\n## \n\n​\n\nAccessing the API\n\nThe API is made available via our web [Console](https://console.anthropic.com/). You can use the [Workbench](https://console.anthropic.com/workbench/3b57d80a-99f2-4760-8316-d3bb14fbfb1e) to try out the API in the browser and then generate API keys in [Account Settings](https://console.anthropic.com/account/keys). Use [workspaces](https://console.anthropic.com/settings/workspaces) to segment your API keys and [control spend](/en/api/rate-limits) by use case.\n\n## \n\n​\n\nAuthentication\n\nAll requests to the Anthropic API must include an `x-api-key` header with your API key. If you are using the Client SDKs, you will set the API when constructing a client, and then the SDK will send the header on your behalf with every request. If integrating directly with the API, you’ll need to send this header yourself.\n\n## \n\n​\n\nContent types\n\nThe Anthropic API always accepts JSON in request bodies and returns JSON in response bodies. You will need to send the `content-type: application/json` header in requests. If you are using the Client SDKs, this will be taken care of automatically.\n\n## \n\n​\n\nResponse Headers\n\nThe Anthropic API includes the following headers in every response:\n\n  * `request-id`: A globally unique identifier for the request.\n\n  * `anthropic-organization-id`: The organization ID associated with the API key used in the request.\n\n\n\n\n## \n\n​\n\nExamples\n\n  * curl\n  * Python\n  * TypeScript\n\n\n\nShell\n    \n    \n    curl https://api.anthropic.com/v1/messages \\\n         --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n         --header \"anthropic-version: 2023-06-01\" \\\n         --header \"content-type: application/json\" \\\n         --data \\\n    '{\n        \"model\": \"claude-3-7-sonnet-20250219\",\n        \"max_tokens\": 1024,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello, world\"}\n        ]\n    }'\n    \n\nWas this page helpful?\n\nYesNo\n\n[IP addresses](/en/api/ip-addresses)\n\n[x](https://x.com/AnthropicAI)[linkedin](https://www.linkedin.com/company/anthropicresearch)\n\nOn this page\n\n  * Accessing the API\n  * Authentication\n  * Content types\n  * Response Headers\n  * Examples\n\n\n"
  },
  {
    "url": "https://docs.anthropic.com/en/api/rate-limits",
    "title": "Rate limits - Anthropic",
    "description": "To mitigate misuse and manage capacity on our API, we have implemented limits on how much an organization can use the Claude API.",
    "content": "[Anthropic home page](/)\n\nEnglish\n\nSearch...\n\n  * [Research](https://www.anthropic.com/research)\n  * [News](https://www.anthropic.com/news)\n  * [Go to claude.ai](https://claude.ai/)\n  * [Go to claude.ai](https://claude.ai/)\n\n\n\nSearch...\n\nNavigation\n\nUsing the API\n\nRate limits\n\n[Welcome](/en/home)[User Guides](/en/docs/welcome)[API Reference](/en/api/getting-started)[Prompt Library](/en/prompt-library/library)[Release Notes](/en/release-notes/overview)\n\n* [Developer Console](https://console.anthropic.com/)\n* [Developer Discord](https://www.anthropic.com/discord)\n* [Support](https://support.anthropic.com/)\n\n##### Using the API\n\n  * [Getting started](/en/api/getting-started)\n  * [IP addresses](/en/api/ip-addresses)\n  * [Versions](/en/api/versioning)\n  * [Errors](/en/api/errors)\n  * [Rate limits](/en/api/rate-limits)\n  * [Client SDKs](/en/api/client-sdks)\n  * [Supported regions](/en/api/supported-regions)\n  * [Getting help](/en/api/getting-help)\n\n\n\n##### Anthropic APIs\n\n  * Messages\n\n  * Models\n\n  * Message Batches\n\n  * Text Completions (Legacy)\n\n  * Admin API\n\n\n\n\n##### OpenAI SDK compatibility\n\n  * [OpenAI SDK compatibility (beta)](/en/api/openai-sdk)\n\n\n\n##### Experimental APIs\n\n  * Prompt tools\n\n\n\n\n##### Amazon Bedrock API\n\n  * [Amazon Bedrock API](/en/api/claude-on-amazon-bedrock)\n\n\n\n##### Vertex AI\n\n  * [Vertex AI API](/en/api/claude-on-vertex-ai)\n\n\n\nUsing the API\n\n# Rate limits\n\nTo mitigate misuse and manage capacity on our API, we have implemented limits on how much an organization can use the Claude API.\n\nWe have two types of limits:\n\n  1. **Spend limits** set a maximum monthly cost an organization can incur for API usage.\n  2. **Rate limits** set the maximum number of API requests an organization can make over a defined period of time.\n\n\n\nWe enforce service-configured limits at the organization level, but you may also set user-configurable limits for your organization’s workspaces.\n\n## \n\n​\n\nAbout our limits\n\n  * Limits are designed to prevent API abuse, while minimizing impact on common customer usage patterns.\n  * Limits are defined by usage tier, where each tier is associated with a different set of spend and rate limits.\n  * Your organization will increase tiers automatically as you reach certain thresholds while using the API. Limits are set at the organization level. You can see your organization’s limits in the [Limits page](https://console.anthropic.com/settings/limits) in the [Anthropic Console](https://console.anthropic.com/).\n  * You may hit rate limits over shorter time intervals. For instance, a rate of 60 requests per minute (RPM) may be enforced as 1 request per second. Short bursts of requests at a high volume can surpass the rate limit and result in rate limit errors.\n  * The limits outlined below are our standard limits. If you’re seeking higher, custom limits, contact sales through the [Anthropic Console](https://console.anthropic.com/settings/limits).\n  * We use the [token bucket algorithm](https://en.wikipedia.org/wiki/Token_bucket) to do rate limiting. This means that your capacity is continuously replenished up to your maximum limit, rather than being reset at fixed intervals.\n  * All limits described here represent maximum allowed usage, not guaranteed minimums. These limits are intended to reduce unintentional overspend and ensure fair distribution of resources among users.\n\n\n\n## \n\n​\n\nSpend limits\n\nEach usage tier has a limit on how much you can spend on the API each calendar month. Once you reach the spend limit of your tier, until you qualify for the next tier, you will have to wait until the next month to be able to use the API again.\n\nTo qualify for the next tier, you must meet a deposit requirement. To minimize the risk of overfunding your account, you cannot deposit more than your monthly spend limit.\n\n### \n\n​\n\nRequirements to advance tier\n\nUsage Tier| Credit Purchase| Max Usage per Month  \n---|---|---  \nTier 1| $5| $100  \nTier 2| $40| $500  \nTier 3| $200| $1,000  \nTier 4| $400| $5,000  \nMonthly Invoicing| N/A| N/A  \n  \n## \n\n​\n\nRate limits\n\nOur rate limits for the Messages API are measured in requests per minute (RPM), input tokens per minute (ITPM), and output tokens per minute (OTPM) for each model class. If you exceed any of the rate limits you will get a [429 error](/en/api/errors) describing which rate limit was exceeded, along with a `retry-after` header indicating how long to wait.\n\nITPM rate limits are estimated at the beginning of each request, and the estimate is adjusted during the request to reflect the actual number of input tokens used. The final adjustment counts [`input_tokens`](/en/api/messages#response-usage-input-tokens) and [`cache_creation_input_tokens`](/en/api/messages#response-usage-cache-creation-input-tokens) towards ITPM rate limits, while [`cache_read_input_tokens`](/en/api/messages#response-usage-cache-read-input-tokens) are not (though they are still billed). In some instances, [`cache_read_input_tokens`](/en/api/messages#response-usage-cache-read-input-tokens) are counted towards ITPM rate limits.\n\nOTPM rate limits are estimated based on `max_tokens` at the beginning of each request, and the estimate is adjusted at the end of the request to reflect the actual number of output tokens used. If you’re hitting OTPM limits earlier than expected, try reducing `max_tokens` to better approximate the size of your completions.\n\nRate limits are applied separately for each model; therefore you can use different models up to their respective limits simultaneously. You can check your current rate limits and behavior in the [Anthropic Console](https://console.anthropic.com/settings/limits).\n\n  * Tier 1\n  * Tier 2\n  * Tier 3\n  * Tier 4\n  * Custom\n\n\n\nModel| Maximum requests per minute (RPM)| Maximum input tokens per minute (ITPM)| Maximum output tokens per minute (OTPM)  \n---|---|---|---  \nClaude 3.7 Sonnet| 50| 20,000| 8,000  \nClaude 3.5 Sonnet   \n2024-10-22| 50| 40,000*| 8,000  \nClaude 3.5 Sonnet   \n2024-06-20| 50| 40,000*| 8,000  \nClaude 3.5 Haiku| 50| 50,000*| 10,000  \nClaude 3 Opus| 50| 20,000*| 4,000  \nClaude 3 Sonnet| 50| 40,000*| 8,000  \nClaude 3 Haiku| 50| 50,000*| 10,000  \n  \nLimits marked with asterisks (*) count [`cache_read_input_tokens`](/en/api/messages#response-usage-cache-read-input-tokens) towards ITPM usage.\n\n### \n\n​\n\nMessage Batches API\n\nThe Message Batches API has its own set of rate limits which are shared across all models. These include a requests per minute (RPM) limit to all API endpoints and a limit on the number of batch requests that can be in the processing queue at the same time. A “batch request” here refers to part of a Message Batch. You may create a Message Batch containing thousands of batch requests, each of which count towards this limit. A batch request is considered part of the processing queue when it has yet to be successfully processed by the model.\n\n  * Tier 1\n  * Tier 2\n  * Tier 3\n  * Tier 4\n  * Custom\n\n\n\nMaximum requests per minute (RPM)| Maximum batch requests in processing queue| Maximum batch requests per batch  \n---|---|---  \n50| 100,000| 100,000  \n  \n## \n\n​\n\nSetting lower limits for Workspaces\n\nIn order to protect Workspaces in your Organization from potential overuse, you can set custom spend and rate limits per Workspace.\n\nExample: If your Organization’s limit is 40,000 input tokens per minute and 8,000 output tokens per minute, you might limit one Workspace to 30,000 total tokens per minute. This protects other Workspaces from potential overuse and ensures a more equitable distribution of resources across your Organization. The remaining unused tokens per minute (or more, if that Workspace doesn’t use the limit) are then available for other Workspaces to use.\n\nNote:\n\n  * You can’t set limits on the default Workspace.\n  * If not set, Workspace limits match the Organization’s limit.\n  * Organization-wide limits always apply, even if Workspace limits add up to more.\n  * Support for input and output token limits will be added to Workspaces in the future.\n\n\n\n## \n\n​\n\nResponse headers\n\nThe API response includes headers that show you the rate limit enforced, current usage, and when the limit will be reset.\n\nThe following headers are returned:\n\nHeader| Description  \n---|---  \n`retry-after`| The number of seconds to wait until you can retry the request. Earlier retries will fail.  \n`anthropic-ratelimit-requests-limit`| The maximum number of requests allowed within any rate limit period.  \n`anthropic-ratelimit-requests-remaining`| The number of requests remaining before being rate limited.  \n`anthropic-ratelimit-requests-reset`| The time when the request rate limit will be fully replenished, provided in RFC 3339 format.  \n`anthropic-ratelimit-tokens-limit`| The maximum number of tokens allowed within any rate limit period.  \n`anthropic-ratelimit-tokens-remaining`| The number of tokens remaining (rounded to the nearest thousand) before being rate limited.  \n`anthropic-ratelimit-tokens-reset`| The time when the token rate limit will be fully replenished, provided in RFC 3339 format.  \n`anthropic-ratelimit-input-tokens-limit`| The maximum number of input tokens allowed within any rate limit period.  \n`anthropic-ratelimit-input-tokens-remaining`| The number of input tokens remaining (rounded to the nearest thousand) before being rate limited.  \n`anthropic-ratelimit-input-tokens-reset`| The time when the input token rate limit will be fully replenished, provided in RFC 3339 format.  \n`anthropic-ratelimit-output-tokens-limit`| The maximum number of output tokens allowed within any rate limit period.  \n`anthropic-ratelimit-output-tokens-remaining`| The number of output tokens remaining (rounded to the nearest thousand) before being rate limited.  \n`anthropic-ratelimit-output-tokens-reset`| The time when the output token rate limit will be fully replenished, provided in RFC 3339 format.  \n  \nThe `anthropic-ratelimit-tokens-*` headers display the values for the most restrictive limit currently in effect. For instance, if you have exceeded the Workspace per-minute token limit, the headers will contain the Workspace per-minute token rate limit values. If Workspace limits do not apply, the headers will return the total tokens remaining, where total is the sum of input and output tokens. This approach ensures that you have visibility into the most relevant constraint on your current API usage.\n\nWas this page helpful?\n\nYesNo\n\n[Errors](/en/api/errors)[Client SDKs](/en/api/client-sdks)\n\n[x](https://x.com/AnthropicAI)[linkedin](https://www.linkedin.com/company/anthropicresearch)\n\nOn this page\n\n  * About our limits\n  * Spend limits\n  * Requirements to advance tier\n  * Rate limits\n  * Message Batches API\n  * Setting lower limits for Workspaces\n  * Response headers\n\n\n"
  },
  {
    "url": "https://docs.anthropic.com/en/api/messages#response-usage-input-tokens",
    "title": "Messages - Anthropic",
    "description": "Send a structured list of input messages with text and/or image content, and the model will generate the next message in the conversation.\n\nThe Messages API can be used for either single queries or stateless multi-turn conversations.\n\nLearn more about the Messages API in our [user guide](/en/docs/initial-setup)",
    "content": "[Anthropic home page](/)\n\nEnglish\n\nSearch...\n\n  * [Research](https://www.anthropic.com/research)\n  * [News](https://www.anthropic.com/news)\n  * [Go to claude.ai](https://claude.ai/)\n  * [Go to claude.ai](https://claude.ai/)\n\n\n\nSearch...\n\nNavigation\n\nMessages\n\nMessages\n\n[Welcome](/en/home)[User Guides](/en/docs/welcome)[API Reference](/en/api/getting-started)[Prompt Library](/en/prompt-library/library)[Release Notes](/en/release-notes/overview)\n\n* [Developer Console](https://console.anthropic.com/)\n* [Developer Discord](https://www.anthropic.com/discord)\n* [Support](https://support.anthropic.com/)\n\n##### Using the API\n\n  * [Getting started](/en/api/getting-started)\n  * [IP addresses](/en/api/ip-addresses)\n  * [Versions](/en/api/versioning)\n  * [Errors](/en/api/errors)\n  * [Rate limits](/en/api/rate-limits)\n  * [Client SDKs](/en/api/client-sdks)\n  * [Supported regions](/en/api/supported-regions)\n  * [Getting help](/en/api/getting-help)\n\n\n\n##### Anthropic APIs\n\n  * Messages\n\n    * [POSTMessages](/en/api/messages)\n    * [POSTCount Message tokens](/en/api/messages-count-tokens)\n    * [Streaming Messages](/en/api/messages-streaming)\n    * [Migrating from Text Completions](/en/api/migrating-from-text-completions-to-messages)\n    * [Messages examples](/en/api/messages-examples)\n  * Models\n\n  * Message Batches\n\n  * Text Completions (Legacy)\n\n  * Admin API\n\n\n\n\n##### OpenAI SDK compatibility\n\n  * [OpenAI SDK compatibility (beta)](/en/api/openai-sdk)\n\n\n\n##### Experimental APIs\n\n  * Prompt tools\n\n\n\n\n##### Amazon Bedrock API\n\n  * [Amazon Bedrock API](/en/api/claude-on-amazon-bedrock)\n\n\n\n##### Vertex AI\n\n  * [Vertex AI API](/en/api/claude-on-vertex-ai)\n\n\n\nMessages\n\n# Messages\n\nSend a structured list of input messages with text and/or image content, and the model will generate the next message in the conversation.\n\nThe Messages API can be used for either single queries or stateless multi-turn conversations.\n\nLearn more about the Messages API in our [user guide](/en/docs/initial-setup)\n\nPOST\n\n/\n\nv1\n\n/\n\nmessages\n\n#### Headers\n\n​\n\nanthropic-beta\n\nstring[]\n\nOptional header to specify the beta version(s) you want to use.\n\nTo use multiple betas, use a comma separated list like `beta1,beta2` or specify the header multiple times for each beta.\n\n​\n\nanthropic-version\n\nstring\n\nrequired\n\nThe version of the Anthropic API you want to use.\n\nRead more about versioning and our version history [here](https://docs.anthropic.com/en/api/versioning).\n\n​\n\nx-api-key\n\nstring\n\nrequired\n\nYour unique API key for authentication.\n\nThis key is required in the header of all API requests, to authenticate your account and access Anthropic's services. Get your API key through the [Console](https://console.anthropic.com/settings/keys). Each key is scoped to a Workspace.\n\n#### Body\n\napplication/json\n\n​\n\nmax_tokens\n\ninteger\n\nrequired\n\nThe maximum number of tokens to generate before stopping.\n\nNote that our models may stop _before_ reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.\n\nDifferent models have different maximum values for this parameter. See [models](https://docs.anthropic.com/en/docs/models-overview) for details.\n\nRequired range: `x > 1`\n\n​\n\nmessages\n\nobject[]\n\nrequired\n\nInput messages.\n\nOur models are trained to operate on alternating `user` and `assistant` conversational turns. When creating a new `Message`, you specify the prior conversational turns with the `messages` parameter, and the model then generates the next `Message` in the conversation. Consecutive `user` or `assistant` turns in your request will be combined into a single turn.\n\nEach input message must be an object with a `role` and `content`. You can specify a single `user`-role message, or you can include multiple `user` and `assistant` messages.\n\nIf the final message uses the `assistant` role, the response content will continue immediately from the content in that message. This can be used to constrain part of the model's response.\n\nExample with a single `user` message:\n    \n    \n    [{\"role\": \"user\", \"content\": \"Hello, Claude\"}]\n    \n\nExample with multiple conversational turns:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"Hello there.\"},\n      {\"role\": \"assistant\", \"content\": \"Hi, I'm Claude. How can I help you?\"},\n      {\"role\": \"user\", \"content\": \"Can you explain LLMs in plain English?\"},\n    ]\n    \n\nExample with a partially-filled response from Claude:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},\n      {\"role\": \"assistant\", \"content\": \"The best answer is (\"},\n    ]\n    \n\nEach input message `content` may be either a single `string` or an array of content blocks, where each block has a specific `type`. Using a `string` for `content` is shorthand for an array of one content block of type `\"text\"`. The following input messages are equivalent:\n    \n    \n    {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n    \n    \n    \n    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, Claude\"}]}\n    \n\nStarting with Claude 3 models, you can also send image content blocks:\n    \n    \n    {\"role\": \"user\", \"content\": [\n      {\n        \"type\": \"image\",\n        \"source\": {\n          \"type\": \"base64\",\n          \"media_type\": \"image/jpeg\",\n          \"data\": \"/9j/4AAQSkZJRg...\",\n        }\n      },\n      {\"type\": \"text\", \"text\": \"What is in this image?\"}\n    ]}\n    \n\nWe currently support the `base64` source type for images, and the `image/jpeg`, `image/png`, `image/gif`, and `image/webp` media types.\n\nSee [examples](https://docs.anthropic.com/en/api/messages-examples#vision) for more input examples.\n\nNote that if you want to include a [system prompt](https://docs.anthropic.com/en/docs/system-prompts), you can use the top-level `system` parameter — there is no `\"system\"` role for input messages in the Messages API.\n\nThere is a limit of 100000 messages in a single request.\n\nShow child attributes\n\n​\n\nmessages.content\n\nstringobject[]\n\nrequired\n\n​\n\nmessages.role\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`user`,\n\n`assistant`\n\n​\n\nmodel\n\nstring\n\nrequired\n\nThe model that will complete your prompt.\n\nSee [models](https://docs.anthropic.com/en/docs/models-overview) for additional details and options.\n\nRequired string length: `1 - 256`\n\n​\n\nmetadata\n\nobject\n\nAn object describing metadata about the request.\n\nShow child attributes\n\n​\n\nmetadata.user_id\n\nstring | null\n\nAn external identifier for the user who is associated with the request.\n\nThis should be a uuid, hash value, or other opaque identifier. Anthropic may use this id to help detect abuse. Do not include any identifying information such as name, email address, or phone number.\n\nMaximum length: `256`\n\n​\n\nstop_sequences\n\nstring[]\n\nCustom text sequences that will cause the model to stop generating.\n\nOur models will normally stop when they have naturally completed their turn, which will result in a response `stop_reason` of `\"end_turn\"`.\n\nIf you want the model to stop generating when it encounters custom strings of text, you can use the `stop_sequences` parameter. If the model encounters one of the custom sequences, the response `stop_reason` value will be `\"stop_sequence\"` and the response `stop_sequence` value will contain the matched stop sequence.\n\n​\n\nstream\n\nboolean\n\nWhether to incrementally stream the response using server-sent events.\n\nSee [streaming](https://docs.anthropic.com/en/api/messages-streaming) for details.\n\n​\n\nsystem\n\nstringobject[]\n\nSystem prompt.\n\nA system prompt is a way of providing context and instructions to Claude, such as specifying a particular goal or role. See our [guide to system prompts](https://docs.anthropic.com/en/docs/system-prompts).\n\n​\n\ntemperature\n\nnumber\n\nAmount of randomness injected into the response.\n\nDefaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to `1.0` for creative and generative tasks.\n\nNote that even with `temperature` of `0.0`, the results will not be fully deterministic.\n\nRequired range: `0 < x < 1`\n\n​\n\nthinking\n\nobject\n\nConfiguration for enabling Claude's extended thinking.\n\nWhen enabled, responses include `thinking` content blocks showing Claude's thinking process before the final answer. Requires a minimum budget of 1,024 tokens and counts towards your `max_tokens` limit.\n\nSee [extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for details.\n\n  * Enabled\n  * Disabled\n\n\n\nShow child attributes\n\n​\n\nthinking.budget_tokens\n\ninteger\n\nrequired\n\nDetermines how many tokens Claude can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality.\n\nMust be ≥1024 and less than `max_tokens`.\n\nSee [extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for details.\n\nRequired range: `x > 1024`\n\n​\n\nthinking.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`enabled`\n\n​\n\ntool_choice\n\nobject\n\nHow the model should use the provided tools. The model can use a specific tool, any available tool, decide by itself, or not use tools at all.\n\n  * Auto\n  * Any\n  * Tool\n  * ToolChoiceNone\n\n\n\nShow child attributes\n\n​\n\ntool_choice.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`auto`\n\n​\n\ntool_choice.disable_parallel_tool_use\n\nboolean\n\nWhether to disable parallel tool use.\n\nDefaults to `false`. If set to `true`, the model will output at most one tool use.\n\n​\n\ntools\n\nobject[]\n\nDefinitions of tools that the model may use.\n\nIf you include `tools` in your API request, the model may return `tool_use` content blocks that represent the model's use of those tools. You can then run those tools using the tool input generated by the model and then optionally return results back to the model using `tool_result` content blocks.\n\nEach tool definition includes:\n\n  * `name`: Name of the tool.\n  * `description`: Optional, but strongly-recommended description of the tool.\n  * `input_schema`: [JSON schema](https://json-schema.org/draft/2020-12) for the tool `input` shape that the model will produce in `tool_use` output content blocks.\n\n\n\nFor example, if you defined `tools` as:\n    \n    \n    [\n      {\n        \"name\": \"get_stock_price\",\n        \"description\": \"Get the current stock price for a given ticker symbol.\",\n        \"input_schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"ticker\": {\n              \"type\": \"string\",\n              \"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\"\n            }\n          },\n          \"required\": [\"ticker\"]\n        }\n      }\n    ]\n    \n\nAnd then asked the model \"What's the S&P 500 at today?\", the model might produce `tool_use` content blocks in the response like this:\n    \n    \n    [\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"toolu_01D7FLrfh4GYq7yT1ULFeyMV\",\n        \"name\": \"get_stock_price\",\n        \"input\": { \"ticker\": \"^GSPC\" }\n      }\n    ]\n    \n\nYou might then run your `get_stock_price` tool with `{\"ticker\": \"^GSPC\"}` as an input, and return the following back to the model in a subsequent `user` message:\n    \n    \n    [\n      {\n        \"type\": \"tool_result\",\n        \"tool_use_id\": \"toolu_01D7FLrfh4GYq7yT1ULFeyMV\",\n        \"content\": \"259.75 USD\"\n      }\n    ]\n    \n\nTools can be used for workflows that include running client-side tools and functions, or more generally whenever you want the model to produce a particular JSON structure of output.\n\nSee our [guide](https://docs.anthropic.com/en/docs/tool-use) for more details.\n\n  * Custom Tool\n  * ComputerUseTool_20241022\n  * BashTool_20241022\n  * TextEditor_20241022\n  * ComputerUseTool_20250124\n  * BashTool_20250124\n  * TextEditor_20250124\n\n\n\nShow child attributes\n\n​\n\ntools.input_schema\n\nobject\n\nrequired\n\n[JSON schema](https://json-schema.org/draft/2020-12) for this tool's input.\n\nThis defines the shape of the `input` that your tool accepts and that the model will produce.\n\nShow child attributes\n\n​\n\ntools.input_schema.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`object`\n\n​\n\ntools.input_schema.properties\n\nobject | null\n\n​\n\ntools.name\n\nstring\n\nrequired\n\nName of the tool.\n\nThis is how the tool will be called by the model and in tool_use blocks.\n\nRequired string length: `1 - 64`\n\n​\n\ntools.cache_control\n\nobject | null\n\nShow child attributes\n\n​\n\ntools.cache_control.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`ephemeral`\n\n​\n\ntools.description\n\nstring\n\nDescription of what this tool does.\n\nTool descriptions should be as detailed as possible. The more information that the model has about what the tool is and how to use it, the better it will perform. You can use natural language descriptions to reinforce important aspects of the tool input JSON schema.\n\n​\n\ntools.type\n\nenum<string> | null\n\nAvailable options:\n\n`custom`\n\n​\n\ntop_k\n\ninteger\n\nOnly sample from the top K options for each subsequent token.\n\nUsed to remove \"long tail\" low probability responses. [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).\n\nRecommended for advanced use cases only. You usually only need to use `temperature`.\n\nRequired range: `x > 0`\n\n​\n\ntop_p\n\nnumber\n\nUse nucleus sampling.\n\nIn nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by `top_p`. You should either alter `temperature` or `top_p`, but not both.\n\nRecommended for advanced use cases only. You usually only need to use `temperature`.\n\nRequired range: `0 < x < 1`\n\n#### Response\n\n200 - application/json\n\n​\n\ncontent\n\nobject[]\n\nrequired\n\nContent generated by the model.\n\nThis is an array of content blocks, each of which has a `type` that determines its shape.\n\nExample:\n    \n    \n    [{\"type\": \"text\", \"text\": \"Hi, I'm Claude.\"}]\n    \n\nIf the request input `messages` ended with an `assistant` turn, then the response `content` will continue directly from that last turn. You can use this to constrain the model's output.\n\nFor example, if the input `messages` were:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},\n      {\"role\": \"assistant\", \"content\": \"The best answer is (\"}\n    ]\n    \n\nThen the response `content` might be:\n    \n    \n    [{\"type\": \"text\", \"text\": \"B)\"}]\n    \n\n  * Text\n  * Tool Use\n  * Thinking\n  * Redacted Thinking\n\n\n\nShow child attributes\n\n​\n\ncontent.citations\n\nobject[] | null\n\nrequired\n\nCitations supporting the text block.\n\nThe type of citation returned will depend on the type of document being cited. Citing a PDF results in `page_location`, plain text results in `char_location`, and content document results in `content_block_location`.\n\n  * Character Location\n  * Page Location\n  * Content Block Location\n\n\n\nShow child attributes\n\n​\n\ncontent.citations.cited_text\n\nstring\n\nrequired\n\n​\n\ncontent.citations.document_index\n\ninteger\n\nrequired\n\nRequired range: `x > 0`\n\n​\n\ncontent.citations.document_title\n\nstring | null\n\nrequired\n\n​\n\ncontent.citations.end_char_index\n\ninteger\n\nrequired\n\n​\n\ncontent.citations.start_char_index\n\ninteger\n\nrequired\n\nRequired range: `x > 0`\n\n​\n\ncontent.citations.type\n\nenum<string>\n\ndefault:\n\nchar_location\n\nrequired\n\nAvailable options:\n\n`char_location`\n\n​\n\ncontent.text\n\nstring\n\nrequired\n\nMaximum length: `5000000`\n\n​\n\ncontent.type\n\nenum<string>\n\ndefault:\n\ntext\n\nrequired\n\nAvailable options:\n\n`text`\n\n​\n\nid\n\nstring\n\nrequired\n\nUnique object identifier.\n\nThe format and length of IDs may change over time.\n\n​\n\nmodel\n\nstring\n\nrequired\n\nThe model that handled the request.\n\nRequired string length: `1 - 256`\n\n​\n\nrole\n\nenum<string>\n\ndefault:\n\nassistant\n\nrequired\n\nConversational role of the generated message.\n\nThis will always be `\"assistant\"`.\n\nAvailable options:\n\n`assistant`\n\n​\n\nstop_reason\n\nenum<string> | null\n\nrequired\n\nThe reason that we stopped.\n\nThis may be one the following values:\n\n  * `\"end_turn\"`: the model reached a natural stopping point\n  * `\"max_tokens\"`: we exceeded the requested `max_tokens` or the model's maximum\n  * `\"stop_sequence\"`: one of your provided custom `stop_sequences` was generated\n  * `\"tool_use\"`: the model invoked one or more tools\n\n\n\nIn non-streaming mode this value is always non-null. In streaming mode, it is null in the `message_start` event and non-null otherwise.\n\nAvailable options:\n\n`end_turn`,\n\n`max_tokens`,\n\n`stop_sequence`,\n\n`tool_use`\n\n​\n\nstop_sequence\n\nstring | null\n\nrequired\n\nWhich custom stop sequence was generated, if any.\n\nThis value will be a non-null string if one of your custom stop sequences was generated.\n\n​\n\ntype\n\nenum<string>\n\ndefault:\n\nmessage\n\nrequired\n\nObject type.\n\nFor Messages, this is always `\"message\"`.\n\nAvailable options:\n\n`message`\n\n​\n\nusage\n\nobject\n\nrequired\n\nBilling and rate-limit usage.\n\nAnthropic's API bills and rate-limits by token counts, as tokens represent the underlying cost to our systems.\n\nUnder the hood, the API transforms requests into a format suitable for the model. The model's output then goes through a parsing stage before becoming an API response. As a result, the token counts in `usage` will not match one-to-one with the exact visible content of an API request or response.\n\nFor example, `output_tokens` will be non-zero, even for an empty string response from Claude.\n\nTotal input tokens in a request is the summation of `input_tokens`, `cache_creation_input_tokens`, and `cache_read_input_tokens`.\n\nShow child attributes\n\n​\n\nusage.cache_creation_input_tokens\n\ninteger | null\n\nrequired\n\nThe number of input tokens used to create the cache entry.\n\nRequired range: `x > 0`\n\n​\n\nusage.cache_read_input_tokens\n\ninteger | null\n\nrequired\n\nThe number of input tokens read from the cache.\n\nRequired range: `x > 0`\n\n​\n\nusage.input_tokens\n\ninteger\n\nrequired\n\nThe number of input tokens which were used.\n\nRequired range: `x > 0`\n\n​\n\nusage.output_tokens\n\ninteger\n\nrequired\n\nThe number of output tokens which were used.\n\nRequired range: `x > 0`\n\nWas this page helpful?\n\nYesNo\n\n[Getting help](/en/api/getting-help)[Count Message tokens](/en/api/messages-count-tokens)\n\n[x](https://x.com/AnthropicAI)[linkedin](https://www.linkedin.com/company/anthropicresearch)\n"
  },
  {
    "url": "https://docs.anthropic.com/en/api/messages",
    "title": "Messages - Anthropic",
    "description": "Send a structured list of input messages with text and/or image content, and the model will generate the next message in the conversation.\n\nThe Messages API can be used for either single queries or stateless multi-turn conversations.\n\nLearn more about the Messages API in our [user guide](/en/docs/initial-setup)",
    "content": "[Anthropic home page](/)\n\nEnglish\n\nSearch...\n\n  * [Research](https://www.anthropic.com/research)\n  * [News](https://www.anthropic.com/news)\n  * [Go to claude.ai](https://claude.ai/)\n  * [Go to claude.ai](https://claude.ai/)\n\n\n\nSearch...\n\nNavigation\n\nMessages\n\nMessages\n\n[Welcome](/en/home)[User Guides](/en/docs/welcome)[API Reference](/en/api/getting-started)[Prompt Library](/en/prompt-library/library)[Release Notes](/en/release-notes/overview)\n\n* [Developer Console](https://console.anthropic.com/)\n* [Developer Discord](https://www.anthropic.com/discord)\n* [Support](https://support.anthropic.com/)\n\n##### Using the API\n\n  * [Getting started](/en/api/getting-started)\n  * [IP addresses](/en/api/ip-addresses)\n  * [Versions](/en/api/versioning)\n  * [Errors](/en/api/errors)\n  * [Rate limits](/en/api/rate-limits)\n  * [Client SDKs](/en/api/client-sdks)\n  * [Supported regions](/en/api/supported-regions)\n  * [Getting help](/en/api/getting-help)\n\n\n\n##### Anthropic APIs\n\n  * Messages\n\n    * [POSTMessages](/en/api/messages)\n    * [POSTCount Message tokens](/en/api/messages-count-tokens)\n    * [Streaming Messages](/en/api/messages-streaming)\n    * [Migrating from Text Completions](/en/api/migrating-from-text-completions-to-messages)\n    * [Messages examples](/en/api/messages-examples)\n  * Models\n\n  * Message Batches\n\n  * Text Completions (Legacy)\n\n  * Admin API\n\n\n\n\n##### OpenAI SDK compatibility\n\n  * [OpenAI SDK compatibility (beta)](/en/api/openai-sdk)\n\n\n\n##### Experimental APIs\n\n  * Prompt tools\n\n\n\n\n##### Amazon Bedrock API\n\n  * [Amazon Bedrock API](/en/api/claude-on-amazon-bedrock)\n\n\n\n##### Vertex AI\n\n  * [Vertex AI API](/en/api/claude-on-vertex-ai)\n\n\n\nMessages\n\n# Messages\n\nSend a structured list of input messages with text and/or image content, and the model will generate the next message in the conversation.\n\nThe Messages API can be used for either single queries or stateless multi-turn conversations.\n\nLearn more about the Messages API in our [user guide](/en/docs/initial-setup)\n\nPOST\n\n/\n\nv1\n\n/\n\nmessages\n\n#### Headers\n\n​\n\nanthropic-beta\n\nstring[]\n\nOptional header to specify the beta version(s) you want to use.\n\nTo use multiple betas, use a comma separated list like `beta1,beta2` or specify the header multiple times for each beta.\n\n​\n\nanthropic-version\n\nstring\n\nrequired\n\nThe version of the Anthropic API you want to use.\n\nRead more about versioning and our version history [here](https://docs.anthropic.com/en/api/versioning).\n\n​\n\nx-api-key\n\nstring\n\nrequired\n\nYour unique API key for authentication.\n\nThis key is required in the header of all API requests, to authenticate your account and access Anthropic's services. Get your API key through the [Console](https://console.anthropic.com/settings/keys). Each key is scoped to a Workspace.\n\n#### Body\n\napplication/json\n\n​\n\nmax_tokens\n\ninteger\n\nrequired\n\nThe maximum number of tokens to generate before stopping.\n\nNote that our models may stop _before_ reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.\n\nDifferent models have different maximum values for this parameter. See [models](https://docs.anthropic.com/en/docs/models-overview) for details.\n\nRequired range: `x > 1`\n\n​\n\nmessages\n\nobject[]\n\nrequired\n\nInput messages.\n\nOur models are trained to operate on alternating `user` and `assistant` conversational turns. When creating a new `Message`, you specify the prior conversational turns with the `messages` parameter, and the model then generates the next `Message` in the conversation. Consecutive `user` or `assistant` turns in your request will be combined into a single turn.\n\nEach input message must be an object with a `role` and `content`. You can specify a single `user`-role message, or you can include multiple `user` and `assistant` messages.\n\nIf the final message uses the `assistant` role, the response content will continue immediately from the content in that message. This can be used to constrain part of the model's response.\n\nExample with a single `user` message:\n    \n    \n    [{\"role\": \"user\", \"content\": \"Hello, Claude\"}]\n    \n\nExample with multiple conversational turns:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"Hello there.\"},\n      {\"role\": \"assistant\", \"content\": \"Hi, I'm Claude. How can I help you?\"},\n      {\"role\": \"user\", \"content\": \"Can you explain LLMs in plain English?\"},\n    ]\n    \n\nExample with a partially-filled response from Claude:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},\n      {\"role\": \"assistant\", \"content\": \"The best answer is (\"},\n    ]\n    \n\nEach input message `content` may be either a single `string` or an array of content blocks, where each block has a specific `type`. Using a `string` for `content` is shorthand for an array of one content block of type `\"text\"`. The following input messages are equivalent:\n    \n    \n    {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n    \n    \n    \n    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, Claude\"}]}\n    \n\nStarting with Claude 3 models, you can also send image content blocks:\n    \n    \n    {\"role\": \"user\", \"content\": [\n      {\n        \"type\": \"image\",\n        \"source\": {\n          \"type\": \"base64\",\n          \"media_type\": \"image/jpeg\",\n          \"data\": \"/9j/4AAQSkZJRg...\",\n        }\n      },\n      {\"type\": \"text\", \"text\": \"What is in this image?\"}\n    ]}\n    \n\nWe currently support the `base64` source type for images, and the `image/jpeg`, `image/png`, `image/gif`, and `image/webp` media types.\n\nSee [examples](https://docs.anthropic.com/en/api/messages-examples#vision) for more input examples.\n\nNote that if you want to include a [system prompt](https://docs.anthropic.com/en/docs/system-prompts), you can use the top-level `system` parameter — there is no `\"system\"` role for input messages in the Messages API.\n\nThere is a limit of 100000 messages in a single request.\n\nShow child attributes\n\n​\n\nmessages.content\n\nstringobject[]\n\nrequired\n\n​\n\nmessages.role\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`user`,\n\n`assistant`\n\n​\n\nmodel\n\nstring\n\nrequired\n\nThe model that will complete your prompt.\n\nSee [models](https://docs.anthropic.com/en/docs/models-overview) for additional details and options.\n\nRequired string length: `1 - 256`\n\n​\n\nmetadata\n\nobject\n\nAn object describing metadata about the request.\n\nShow child attributes\n\n​\n\nmetadata.user_id\n\nstring | null\n\nAn external identifier for the user who is associated with the request.\n\nThis should be a uuid, hash value, or other opaque identifier. Anthropic may use this id to help detect abuse. Do not include any identifying information such as name, email address, or phone number.\n\nMaximum length: `256`\n\n​\n\nstop_sequences\n\nstring[]\n\nCustom text sequences that will cause the model to stop generating.\n\nOur models will normally stop when they have naturally completed their turn, which will result in a response `stop_reason` of `\"end_turn\"`.\n\nIf you want the model to stop generating when it encounters custom strings of text, you can use the `stop_sequences` parameter. If the model encounters one of the custom sequences, the response `stop_reason` value will be `\"stop_sequence\"` and the response `stop_sequence` value will contain the matched stop sequence.\n\n​\n\nstream\n\nboolean\n\nWhether to incrementally stream the response using server-sent events.\n\nSee [streaming](https://docs.anthropic.com/en/api/messages-streaming) for details.\n\n​\n\nsystem\n\nstringobject[]\n\nSystem prompt.\n\nA system prompt is a way of providing context and instructions to Claude, such as specifying a particular goal or role. See our [guide to system prompts](https://docs.anthropic.com/en/docs/system-prompts).\n\n​\n\ntemperature\n\nnumber\n\nAmount of randomness injected into the response.\n\nDefaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to `1.0` for creative and generative tasks.\n\nNote that even with `temperature` of `0.0`, the results will not be fully deterministic.\n\nRequired range: `0 < x < 1`\n\n​\n\nthinking\n\nobject\n\nConfiguration for enabling Claude's extended thinking.\n\nWhen enabled, responses include `thinking` content blocks showing Claude's thinking process before the final answer. Requires a minimum budget of 1,024 tokens and counts towards your `max_tokens` limit.\n\nSee [extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for details.\n\n  * Enabled\n  * Disabled\n\n\n\nShow child attributes\n\n​\n\nthinking.budget_tokens\n\ninteger\n\nrequired\n\nDetermines how many tokens Claude can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality.\n\nMust be ≥1024 and less than `max_tokens`.\n\nSee [extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for details.\n\nRequired range: `x > 1024`\n\n​\n\nthinking.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`enabled`\n\n​\n\ntool_choice\n\nobject\n\nHow the model should use the provided tools. The model can use a specific tool, any available tool, decide by itself, or not use tools at all.\n\n  * Auto\n  * Any\n  * Tool\n  * ToolChoiceNone\n\n\n\nShow child attributes\n\n​\n\ntool_choice.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`auto`\n\n​\n\ntool_choice.disable_parallel_tool_use\n\nboolean\n\nWhether to disable parallel tool use.\n\nDefaults to `false`. If set to `true`, the model will output at most one tool use.\n\n​\n\ntools\n\nobject[]\n\nDefinitions of tools that the model may use.\n\nIf you include `tools` in your API request, the model may return `tool_use` content blocks that represent the model's use of those tools. You can then run those tools using the tool input generated by the model and then optionally return results back to the model using `tool_result` content blocks.\n\nEach tool definition includes:\n\n  * `name`: Name of the tool.\n  * `description`: Optional, but strongly-recommended description of the tool.\n  * `input_schema`: [JSON schema](https://json-schema.org/draft/2020-12) for the tool `input` shape that the model will produce in `tool_use` output content blocks.\n\n\n\nFor example, if you defined `tools` as:\n    \n    \n    [\n      {\n        \"name\": \"get_stock_price\",\n        \"description\": \"Get the current stock price for a given ticker symbol.\",\n        \"input_schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"ticker\": {\n              \"type\": \"string\",\n              \"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\"\n            }\n          },\n          \"required\": [\"ticker\"]\n        }\n      }\n    ]\n    \n\nAnd then asked the model \"What's the S&P 500 at today?\", the model might produce `tool_use` content blocks in the response like this:\n    \n    \n    [\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"toolu_01D7FLrfh4GYq7yT1ULFeyMV\",\n        \"name\": \"get_stock_price\",\n        \"input\": { \"ticker\": \"^GSPC\" }\n      }\n    ]\n    \n\nYou might then run your `get_stock_price` tool with `{\"ticker\": \"^GSPC\"}` as an input, and return the following back to the model in a subsequent `user` message:\n    \n    \n    [\n      {\n        \"type\": \"tool_result\",\n        \"tool_use_id\": \"toolu_01D7FLrfh4GYq7yT1ULFeyMV\",\n        \"content\": \"259.75 USD\"\n      }\n    ]\n    \n\nTools can be used for workflows that include running client-side tools and functions, or more generally whenever you want the model to produce a particular JSON structure of output.\n\nSee our [guide](https://docs.anthropic.com/en/docs/tool-use) for more details.\n\n  * Custom Tool\n  * ComputerUseTool_20241022\n  * BashTool_20241022\n  * TextEditor_20241022\n  * ComputerUseTool_20250124\n  * BashTool_20250124\n  * TextEditor_20250124\n\n\n\nShow child attributes\n\n​\n\ntools.input_schema\n\nobject\n\nrequired\n\n[JSON schema](https://json-schema.org/draft/2020-12) for this tool's input.\n\nThis defines the shape of the `input` that your tool accepts and that the model will produce.\n\nShow child attributes\n\n​\n\ntools.input_schema.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`object`\n\n​\n\ntools.input_schema.properties\n\nobject | null\n\n​\n\ntools.name\n\nstring\n\nrequired\n\nName of the tool.\n\nThis is how the tool will be called by the model and in tool_use blocks.\n\nRequired string length: `1 - 64`\n\n​\n\ntools.cache_control\n\nobject | null\n\nShow child attributes\n\n​\n\ntools.cache_control.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`ephemeral`\n\n​\n\ntools.description\n\nstring\n\nDescription of what this tool does.\n\nTool descriptions should be as detailed as possible. The more information that the model has about what the tool is and how to use it, the better it will perform. You can use natural language descriptions to reinforce important aspects of the tool input JSON schema.\n\n​\n\ntools.type\n\nenum<string> | null\n\nAvailable options:\n\n`custom`\n\n​\n\ntop_k\n\ninteger\n\nOnly sample from the top K options for each subsequent token.\n\nUsed to remove \"long tail\" low probability responses. [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).\n\nRecommended for advanced use cases only. You usually only need to use `temperature`.\n\nRequired range: `x > 0`\n\n​\n\ntop_p\n\nnumber\n\nUse nucleus sampling.\n\nIn nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by `top_p`. You should either alter `temperature` or `top_p`, but not both.\n\nRecommended for advanced use cases only. You usually only need to use `temperature`.\n\nRequired range: `0 < x < 1`\n\n#### Response\n\n200 - application/json\n\n​\n\ncontent\n\nobject[]\n\nrequired\n\nContent generated by the model.\n\nThis is an array of content blocks, each of which has a `type` that determines its shape.\n\nExample:\n    \n    \n    [{\"type\": \"text\", \"text\": \"Hi, I'm Claude.\"}]\n    \n\nIf the request input `messages` ended with an `assistant` turn, then the response `content` will continue directly from that last turn. You can use this to constrain the model's output.\n\nFor example, if the input `messages` were:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},\n      {\"role\": \"assistant\", \"content\": \"The best answer is (\"}\n    ]\n    \n\nThen the response `content` might be:\n    \n    \n    [{\"type\": \"text\", \"text\": \"B)\"}]\n    \n\n  * Text\n  * Tool Use\n  * Thinking\n  * Redacted Thinking\n\n\n\nShow child attributes\n\n​\n\ncontent.citations\n\nobject[] | null\n\nrequired\n\nCitations supporting the text block.\n\nThe type of citation returned will depend on the type of document being cited. Citing a PDF results in `page_location`, plain text results in `char_location`, and content document results in `content_block_location`.\n\n  * Character Location\n  * Page Location\n  * Content Block Location\n\n\n\nShow child attributes\n\n​\n\ncontent.citations.cited_text\n\nstring\n\nrequired\n\n​\n\ncontent.citations.document_index\n\ninteger\n\nrequired\n\nRequired range: `x > 0`\n\n​\n\ncontent.citations.document_title\n\nstring | null\n\nrequired\n\n​\n\ncontent.citations.end_char_index\n\ninteger\n\nrequired\n\n​\n\ncontent.citations.start_char_index\n\ninteger\n\nrequired\n\nRequired range: `x > 0`\n\n​\n\ncontent.citations.type\n\nenum<string>\n\ndefault:\n\nchar_location\n\nrequired\n\nAvailable options:\n\n`char_location`\n\n​\n\ncontent.text\n\nstring\n\nrequired\n\nMaximum length: `5000000`\n\n​\n\ncontent.type\n\nenum<string>\n\ndefault:\n\ntext\n\nrequired\n\nAvailable options:\n\n`text`\n\n​\n\nid\n\nstring\n\nrequired\n\nUnique object identifier.\n\nThe format and length of IDs may change over time.\n\n​\n\nmodel\n\nstring\n\nrequired\n\nThe model that handled the request.\n\nRequired string length: `1 - 256`\n\n​\n\nrole\n\nenum<string>\n\ndefault:\n\nassistant\n\nrequired\n\nConversational role of the generated message.\n\nThis will always be `\"assistant\"`.\n\nAvailable options:\n\n`assistant`\n\n​\n\nstop_reason\n\nenum<string> | null\n\nrequired\n\nThe reason that we stopped.\n\nThis may be one the following values:\n\n  * `\"end_turn\"`: the model reached a natural stopping point\n  * `\"max_tokens\"`: we exceeded the requested `max_tokens` or the model's maximum\n  * `\"stop_sequence\"`: one of your provided custom `stop_sequences` was generated\n  * `\"tool_use\"`: the model invoked one or more tools\n\n\n\nIn non-streaming mode this value is always non-null. In streaming mode, it is null in the `message_start` event and non-null otherwise.\n\nAvailable options:\n\n`end_turn`,\n\n`max_tokens`,\n\n`stop_sequence`,\n\n`tool_use`\n\n​\n\nstop_sequence\n\nstring | null\n\nrequired\n\nWhich custom stop sequence was generated, if any.\n\nThis value will be a non-null string if one of your custom stop sequences was generated.\n\n​\n\ntype\n\nenum<string>\n\ndefault:\n\nmessage\n\nrequired\n\nObject type.\n\nFor Messages, this is always `\"message\"`.\n\nAvailable options:\n\n`message`\n\n​\n\nusage\n\nobject\n\nrequired\n\nBilling and rate-limit usage.\n\nAnthropic's API bills and rate-limits by token counts, as tokens represent the underlying cost to our systems.\n\nUnder the hood, the API transforms requests into a format suitable for the model. The model's output then goes through a parsing stage before becoming an API response. As a result, the token counts in `usage` will not match one-to-one with the exact visible content of an API request or response.\n\nFor example, `output_tokens` will be non-zero, even for an empty string response from Claude.\n\nTotal input tokens in a request is the summation of `input_tokens`, `cache_creation_input_tokens`, and `cache_read_input_tokens`.\n\nShow child attributes\n\n​\n\nusage.cache_creation_input_tokens\n\ninteger | null\n\nrequired\n\nThe number of input tokens used to create the cache entry.\n\nRequired range: `x > 0`\n\n​\n\nusage.cache_read_input_tokens\n\ninteger | null\n\nrequired\n\nThe number of input tokens read from the cache.\n\nRequired range: `x > 0`\n\n​\n\nusage.input_tokens\n\ninteger\n\nrequired\n\nThe number of input tokens which were used.\n\nRequired range: `x > 0`\n\n​\n\nusage.output_tokens\n\ninteger\n\nrequired\n\nThe number of output tokens which were used.\n\nRequired range: `x > 0`\n\nWas this page helpful?\n\nYesNo\n\n[Getting help](/en/api/getting-help)[Count Message tokens](/en/api/messages-count-tokens)\n\n[x](https://x.com/AnthropicAI)[linkedin](https://www.linkedin.com/company/anthropicresearch)\n"
  },
  {
    "url": "https://docs.anthropic.com/en/api/messages-count-tokens",
    "title": "Count Message tokens - Anthropic",
    "description": "Count the number of tokens in a Message.\n\nThe Token Count API can be used to count the number of tokens in a Message, including tools, images, and documents, without creating it.\n\nLearn more about token counting in our [user guide](/en/docs/build-with-claude/token-counting)",
    "content": "[Anthropic home page](/)\n\nEnglish\n\nSearch...\n\n  * [Research](https://www.anthropic.com/research)\n  * [News](https://www.anthropic.com/news)\n  * [Go to claude.ai](https://claude.ai/)\n  * [Go to claude.ai](https://claude.ai/)\n\n\n\nSearch...\n\nNavigation\n\nMessages\n\nCount Message tokens\n\n[Welcome](/en/home)[User Guides](/en/docs/welcome)[API Reference](/en/api/getting-started)[Prompt Library](/en/prompt-library/library)[Release Notes](/en/release-notes/overview)\n\n* [Developer Console](https://console.anthropic.com/)\n* [Developer Discord](https://www.anthropic.com/discord)\n* [Support](https://support.anthropic.com/)\n\n##### Using the API\n\n  * [Getting started](/en/api/getting-started)\n  * [IP addresses](/en/api/ip-addresses)\n  * [Versions](/en/api/versioning)\n  * [Errors](/en/api/errors)\n  * [Rate limits](/en/api/rate-limits)\n  * [Client SDKs](/en/api/client-sdks)\n  * [Supported regions](/en/api/supported-regions)\n  * [Getting help](/en/api/getting-help)\n\n\n\n##### Anthropic APIs\n\n  * Messages\n\n    * [POSTMessages](/en/api/messages)\n    * [POSTCount Message tokens](/en/api/messages-count-tokens)\n    * [Streaming Messages](/en/api/messages-streaming)\n    * [Migrating from Text Completions](/en/api/migrating-from-text-completions-to-messages)\n    * [Messages examples](/en/api/messages-examples)\n  * Models\n\n  * Message Batches\n\n  * Text Completions (Legacy)\n\n  * Admin API\n\n\n\n\n##### OpenAI SDK compatibility\n\n  * [OpenAI SDK compatibility (beta)](/en/api/openai-sdk)\n\n\n\n##### Experimental APIs\n\n  * Prompt tools\n\n\n\n\n##### Amazon Bedrock API\n\n  * [Amazon Bedrock API](/en/api/claude-on-amazon-bedrock)\n\n\n\n##### Vertex AI\n\n  * [Vertex AI API](/en/api/claude-on-vertex-ai)\n\n\n\nMessages\n\n# Count Message tokens\n\nCount the number of tokens in a Message.\n\nThe Token Count API can be used to count the number of tokens in a Message, including tools, images, and documents, without creating it.\n\nLearn more about token counting in our [user guide](/en/docs/build-with-claude/token-counting)\n\nPOST\n\n/\n\nv1\n\n/\n\nmessages\n\n/\n\ncount_tokens\n\n#### Headers\n\n​\n\nanthropic-beta\n\nstring[]\n\nOptional header to specify the beta version(s) you want to use.\n\nTo use multiple betas, use a comma separated list like `beta1,beta2` or specify the header multiple times for each beta.\n\n​\n\nanthropic-version\n\nstring\n\nrequired\n\nThe version of the Anthropic API you want to use.\n\nRead more about versioning and our version history [here](https://docs.anthropic.com/en/api/versioning).\n\n​\n\nx-api-key\n\nstring\n\nrequired\n\nYour unique API key for authentication.\n\nThis key is required in the header of all API requests, to authenticate your account and access Anthropic's services. Get your API key through the [Console](https://console.anthropic.com/settings/keys). Each key is scoped to a Workspace.\n\n#### Body\n\napplication/json\n\n​\n\nmessages\n\nobject[]\n\nrequired\n\nInput messages.\n\nOur models are trained to operate on alternating `user` and `assistant` conversational turns. When creating a new `Message`, you specify the prior conversational turns with the `messages` parameter, and the model then generates the next `Message` in the conversation. Consecutive `user` or `assistant` turns in your request will be combined into a single turn.\n\nEach input message must be an object with a `role` and `content`. You can specify a single `user`-role message, or you can include multiple `user` and `assistant` messages.\n\nIf the final message uses the `assistant` role, the response content will continue immediately from the content in that message. This can be used to constrain part of the model's response.\n\nExample with a single `user` message:\n    \n    \n    [{\"role\": \"user\", \"content\": \"Hello, Claude\"}]\n    \n\nExample with multiple conversational turns:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"Hello there.\"},\n      {\"role\": \"assistant\", \"content\": \"Hi, I'm Claude. How can I help you?\"},\n      {\"role\": \"user\", \"content\": \"Can you explain LLMs in plain English?\"},\n    ]\n    \n\nExample with a partially-filled response from Claude:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},\n      {\"role\": \"assistant\", \"content\": \"The best answer is (\"},\n    ]\n    \n\nEach input message `content` may be either a single `string` or an array of content blocks, where each block has a specific `type`. Using a `string` for `content` is shorthand for an array of one content block of type `\"text\"`. The following input messages are equivalent:\n    \n    \n    {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n    \n    \n    \n    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, Claude\"}]}\n    \n\nStarting with Claude 3 models, you can also send image content blocks:\n    \n    \n    {\"role\": \"user\", \"content\": [\n      {\n        \"type\": \"image\",\n        \"source\": {\n          \"type\": \"base64\",\n          \"media_type\": \"image/jpeg\",\n          \"data\": \"/9j/4AAQSkZJRg...\",\n        }\n      },\n      {\"type\": \"text\", \"text\": \"What is in this image?\"}\n    ]}\n    \n\nWe currently support the `base64` source type for images, and the `image/jpeg`, `image/png`, `image/gif`, and `image/webp` media types.\n\nSee [examples](https://docs.anthropic.com/en/api/messages-examples#vision) for more input examples.\n\nNote that if you want to include a [system prompt](https://docs.anthropic.com/en/docs/system-prompts), you can use the top-level `system` parameter — there is no `\"system\"` role for input messages in the Messages API.\n\nThere is a limit of 100000 messages in a single request.\n\nShow child attributes\n\n​\n\nmessages.content\n\nstringobject[]\n\nrequired\n\n​\n\nmessages.role\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`user`,\n\n`assistant`\n\n​\n\nmodel\n\nstring\n\nrequired\n\nThe model that will complete your prompt.\n\nSee [models](https://docs.anthropic.com/en/docs/models-overview) for additional details and options.\n\nRequired string length: `1 - 256`\n\n​\n\nsystem\n\nstringobject[]\n\nSystem prompt.\n\nA system prompt is a way of providing context and instructions to Claude, such as specifying a particular goal or role. See our [guide to system prompts](https://docs.anthropic.com/en/docs/system-prompts).\n\n​\n\nthinking\n\nobject\n\nConfiguration for enabling Claude's extended thinking.\n\nWhen enabled, responses include `thinking` content blocks showing Claude's thinking process before the final answer. Requires a minimum budget of 1,024 tokens and counts towards your `max_tokens` limit.\n\nSee [extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for details.\n\n  * Enabled\n  * Disabled\n\n\n\nShow child attributes\n\n​\n\nthinking.budget_tokens\n\ninteger\n\nrequired\n\nDetermines how many tokens Claude can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality.\n\nMust be ≥1024 and less than `max_tokens`.\n\nSee [extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for details.\n\nRequired range: `x > 1024`\n\n​\n\nthinking.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`enabled`\n\n​\n\ntool_choice\n\nobject\n\nHow the model should use the provided tools. The model can use a specific tool, any available tool, decide by itself, or not use tools at all.\n\n  * Auto\n  * Any\n  * Tool\n  * ToolChoiceNone\n\n\n\nShow child attributes\n\n​\n\ntool_choice.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`auto`\n\n​\n\ntool_choice.disable_parallel_tool_use\n\nboolean\n\nWhether to disable parallel tool use.\n\nDefaults to `false`. If set to `true`, the model will output at most one tool use.\n\n​\n\ntools\n\nobject[]\n\nDefinitions of tools that the model may use.\n\nIf you include `tools` in your API request, the model may return `tool_use` content blocks that represent the model's use of those tools. You can then run those tools using the tool input generated by the model and then optionally return results back to the model using `tool_result` content blocks.\n\nEach tool definition includes:\n\n  * `name`: Name of the tool.\n  * `description`: Optional, but strongly-recommended description of the tool.\n  * `input_schema`: [JSON schema](https://json-schema.org/draft/2020-12) for the tool `input` shape that the model will produce in `tool_use` output content blocks.\n\n\n\nFor example, if you defined `tools` as:\n    \n    \n    [\n      {\n        \"name\": \"get_stock_price\",\n        \"description\": \"Get the current stock price for a given ticker symbol.\",\n        \"input_schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"ticker\": {\n              \"type\": \"string\",\n              \"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\"\n            }\n          },\n          \"required\": [\"ticker\"]\n        }\n      }\n    ]\n    \n\nAnd then asked the model \"What's the S&P 500 at today?\", the model might produce `tool_use` content blocks in the response like this:\n    \n    \n    [\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"toolu_01D7FLrfh4GYq7yT1ULFeyMV\",\n        \"name\": \"get_stock_price\",\n        \"input\": { \"ticker\": \"^GSPC\" }\n      }\n    ]\n    \n\nYou might then run your `get_stock_price` tool with `{\"ticker\": \"^GSPC\"}` as an input, and return the following back to the model in a subsequent `user` message:\n    \n    \n    [\n      {\n        \"type\": \"tool_result\",\n        \"tool_use_id\": \"toolu_01D7FLrfh4GYq7yT1ULFeyMV\",\n        \"content\": \"259.75 USD\"\n      }\n    ]\n    \n\nTools can be used for workflows that include running client-side tools and functions, or more generally whenever you want the model to produce a particular JSON structure of output.\n\nSee our [guide](https://docs.anthropic.com/en/docs/tool-use) for more details.\n\n  * Custom Tool\n  * ComputerUseTool_20241022\n  * BashTool_20241022\n  * TextEditor_20241022\n  * ComputerUseTool_20250124\n  * BashTool_20250124\n  * TextEditor_20250124\n\n\n\nShow child attributes\n\n​\n\ntools.input_schema\n\nobject\n\nrequired\n\n[JSON schema](https://json-schema.org/draft/2020-12) for this tool's input.\n\nThis defines the shape of the `input` that your tool accepts and that the model will produce.\n\nShow child attributes\n\n​\n\ntools.input_schema.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`object`\n\n​\n\ntools.input_schema.properties\n\nobject | null\n\n​\n\ntools.name\n\nstring\n\nrequired\n\nName of the tool.\n\nThis is how the tool will be called by the model and in tool_use blocks.\n\nRequired string length: `1 - 64`\n\n​\n\ntools.cache_control\n\nobject | null\n\nShow child attributes\n\n​\n\ntools.cache_control.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`ephemeral`\n\n​\n\ntools.description\n\nstring\n\nDescription of what this tool does.\n\nTool descriptions should be as detailed as possible. The more information that the model has about what the tool is and how to use it, the better it will perform. You can use natural language descriptions to reinforce important aspects of the tool input JSON schema.\n\n​\n\ntools.type\n\nenum<string> | null\n\nAvailable options:\n\n`custom`\n\n#### Response\n\n200 - application/json\n\n​\n\ninput_tokens\n\ninteger\n\nrequired\n\nThe total number of tokens across the provided list of messages, system prompt, and tools.\n\nWas this page helpful?\n\nYesNo\n\n[Messages](/en/api/messages)[Streaming Messages](/en/api/messages-streaming)\n\n[x](https://x.com/AnthropicAI)[linkedin](https://www.linkedin.com/company/anthropicresearch)\n"
  },
  {
    "url": "https://docs.anthropic.com/en/api/messages-streaming",
    "title": "Streaming Messages - Anthropic",
    "description": "",
    "content": "[Anthropic home page](/)\n\nEnglish\n\nSearch...\n\n  * [Research](https://www.anthropic.com/research)\n  * [News](https://www.anthropic.com/news)\n  * [Go to claude.ai](https://claude.ai/)\n  * [Go to claude.ai](https://claude.ai/)\n\n\n\nSearch...\n\nNavigation\n\nMessages\n\nStreaming Messages\n\n[Welcome](/en/home)[User Guides](/en/docs/welcome)[API Reference](/en/api/getting-started)[Prompt Library](/en/prompt-library/library)[Release Notes](/en/release-notes/overview)\n\n* [Developer Console](https://console.anthropic.com/)\n* [Developer Discord](https://www.anthropic.com/discord)\n* [Support](https://support.anthropic.com/)\n\n##### Using the API\n\n  * [Getting started](/en/api/getting-started)\n  * [IP addresses](/en/api/ip-addresses)\n  * [Versions](/en/api/versioning)\n  * [Errors](/en/api/errors)\n  * [Rate limits](/en/api/rate-limits)\n  * [Client SDKs](/en/api/client-sdks)\n  * [Supported regions](/en/api/supported-regions)\n  * [Getting help](/en/api/getting-help)\n\n\n\n##### Anthropic APIs\n\n  * Messages\n\n    * [POSTMessages](/en/api/messages)\n    * [POSTCount Message tokens](/en/api/messages-count-tokens)\n    * [Streaming Messages](/en/api/messages-streaming)\n    * [Migrating from Text Completions](/en/api/migrating-from-text-completions-to-messages)\n    * [Messages examples](/en/api/messages-examples)\n  * Models\n\n  * Message Batches\n\n  * Text Completions (Legacy)\n\n  * Admin API\n\n\n\n\n##### OpenAI SDK compatibility\n\n  * [OpenAI SDK compatibility (beta)](/en/api/openai-sdk)\n\n\n\n##### Experimental APIs\n\n  * Prompt tools\n\n\n\n\n##### Amazon Bedrock API\n\n  * [Amazon Bedrock API](/en/api/claude-on-amazon-bedrock)\n\n\n\n##### Vertex AI\n\n  * [Vertex AI API](/en/api/claude-on-vertex-ai)\n\n\n\nMessages\n\n# Streaming Messages\n\nWhen creating a Message, you can set `\"stream\": true` to incrementally stream the response using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents) (SSE).\n\n## \n\n​\n\nStreaming with SDKs\n\nOur [Python](https://github.com/anthropics/anthropic-sdk-python) and [TypeScript](https://github.com/anthropics/anthropic-sdk-typescript) SDKs offer multiple ways of streaming. The Python SDK allows both sync and async streams. See the documentation in each SDK for details.\n\n## \n\n​\n\nEvent types\n\nEach server-sent event includes a named event type and associated JSON data. Each event will use an SSE event name (e.g. `event: message_stop`), and include the matching event `type` in its data.\n\nEach stream uses the following event flow:\n\n  1. `message_start`: contains a `Message` object with empty `content`.\n  2. A series of content blocks, each of which have a `content_block_start`, one or more `content_block_delta` events, and a `content_block_stop` event. Each content block will have an `index` that corresponds to its index in the final Message `content` array.\n  3. One or more `message_delta` events, indicating top-level changes to the final `Message` object.\n  4. A final `message_stop` event.\n\n\n\n### \n\n​\n\nPing events\n\nEvent streams may also include any number of `ping` events.\n\n### \n\n​\n\nError events\n\nWe may occasionally send [errors](/en/api/errors) in the event stream. For example, during periods of high usage, you may receive an `overloaded_error`, which would normally correspond to an HTTP 529 in a non-streaming context:\n\nExample error\n    \n    \n    event: error\n    data: {\"type\": \"error\", \"error\": {\"type\": \"overloaded_error\", \"message\": \"Overloaded\"}}\n    \n\n### \n\n​\n\nOther events\n\nIn accordance with our [versioning policy](/en/api/versioning), we may add new event types, and your code should handle unknown event types gracefully.\n\n## \n\n​\n\nDelta types\n\nEach `content_block_delta` event contains a `delta` of a type that updates the `content` block at a given `index`.\n\n### \n\n​\n\nText delta\n\nA `text` content block delta looks like:\n\nText delta\n    \n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\",\"index\": 0,\"delta\": {\"type\": \"text_delta\", \"text\": \"ello frien\"}}\n    \n\n### \n\n​\n\nInput JSON delta\n\nThe deltas for `tool_use` content blocks correspond to updates for the `input` field of the block. To support maximum granularity, the deltas are _partial JSON strings_ , whereas the final `tool_use.input` is always an _object_.\n\nYou can accumulate the string deltas and parse the JSON once you receive a `content_block_stop` event, by using a library like [Pydantic](https://docs.pydantic.dev/latest/concepts/json/#partial-json-parsing) to do partial JSON parsing, or by using our [SDKs](https://docs.anthropic.com/en/api/client-sdks), which provide helpers to access parsed incremental values.\n\nA `tool_use` content block delta looks like:\n\nInput JSON delta\n    \n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\",\"index\": 1,\"delta\": {\"type\": \"input_json_delta\",\"partial_json\": \"{\\\"location\\\": \\\"San Fra\"}}}\n    \n\nNote: Our current models only support emitting one complete key and value property from `input` at a time. As such, when using tools, there may be delays between streaming events while the model is working. Once an `input` key and value are accumulated, we emit them as multiple `content_block_delta` events with chunked partial json so that the format can automatically support finer granularity in future models.\n\n### \n\n​\n\nThinking delta\n\nWhen using [extended thinking](/en/docs/build-with-claude/extended-thinking#streaming-extended-thinking) with streaming enabled, you’ll receive thinking content via `thinking_delta` events. These deltas correspond to the `thinking` field of the `thinking` content blocks.\n\nFor thinking content, a special `signature_delta` event is sent just before the `content_block_stop` event. This signature is used to verify the integrity of the thinking block.\n\nA typical thinking delta looks like:\n\nThinking delta\n    \n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"thinking_delta\", \"thinking\": \"Let me solve this step by step:\\n\\n1. First break down 27 * 453\"}}\n    \n\nThe signature delta looks like:\n\nSignature delta\n    \n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"signature_delta\", \"signature\": \"EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds...\"}}\n    \n\n## \n\n​\n\nRaw HTTP Stream response\n\nWe strongly recommend that use our [client SDKs](/en/api/client-sdks) when using streaming mode. However, if you are building a direct API integration, you will need to handle these events yourself.\n\nA stream response is comprised of:\n\n  1. A `message_start` event\n  2. Potentially multiple content blocks, each of which contains: a. A `content_block_start` event b. Potentially multiple `content_block_delta` events c. A `content_block_stop` event\n  3. A `message_delta` event\n  4. A `message_stop` event\n\n\n\nThere may be `ping` events dispersed throughout the response as well. See [Event types](/_sites/docs.anthropic.com/en/api/messages-streaming#event-types) for more details on the format.\n\n### \n\n​\n\nBasic streaming request\n\nRequest\n    \n    \n    curl https://api.anthropic.com/v1/messages \\\n         --header \"anthropic-version: 2023-06-01\" \\\n         --header \"content-type: application/json\" \\\n         --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n         --data \\\n    '{\n      \"model\": \"claude-3-7-sonnet-20250219\",\n      \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}],\n      \"max_tokens\": 256,\n      \"stream\": true\n    }'\n    \n\nResponse\n    \n    \n    event: message_start\n    data: {\"type\": \"message_start\", \"message\": {\"id\": \"msg_1nZdL29xx5MUA1yADyHTEsnR8uuvGzszyY\", \"type\": \"message\", \"role\": \"assistant\", \"content\": [], \"model\": \"claude-3-7-sonnet-20250219\", \"stop_reason\": null, \"stop_sequence\": null, \"usage\": {\"input_tokens\": 25, \"output_tokens\": 1}}}\n    \n    event: content_block_start\n    data: {\"type\": \"content_block_start\", \"index\": 0, \"content_block\": {\"type\": \"text\", \"text\": \"\"}}\n    \n    event: ping\n    data: {\"type\": \"ping\"}\n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"text_delta\", \"text\": \"Hello\"}}\n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"text_delta\", \"text\": \"!\"}}\n    \n    event: content_block_stop\n    data: {\"type\": \"content_block_stop\", \"index\": 0}\n    \n    event: message_delta\n    data: {\"type\": \"message_delta\", \"delta\": {\"stop_reason\": \"end_turn\", \"stop_sequence\":null}, \"usage\": {\"output_tokens\": 15}}\n    \n    event: message_stop\n    data: {\"type\": \"message_stop\"}\n    \n    \n\n### \n\n​\n\nStreaming request with tool use\n\nIn this request, we ask Claude to use a tool to tell us the weather.\n\nRequest\n    \n    \n      curl https://api.anthropic.com/v1/messages \\\n        -H \"content-type: application/json\" \\\n        -H \"x-api-key: $ANTHROPIC_API_KEY\" \\\n        -H \"anthropic-version: 2023-06-01\" \\\n        -d '{\n          \"model\": \"claude-3-7-sonnet-20250219\",\n          \"max_tokens\": 1024,\n          \"tools\": [\n            {\n              \"name\": \"get_weather\",\n              \"description\": \"Get the current weather in a given location\",\n              \"input_schema\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n                  }\n                },\n                \"required\": [\"location\"]\n              }\n            }\n          ],\n          \"tool_choice\": {\"type\": \"any\"},\n          \"messages\": [\n            {\n              \"role\": \"user\",\n              \"content\": \"What is the weather like in San Francisco?\"\n            }\n          ],\n          \"stream\": true\n        }'\n    \n\nResponse\n    \n    \n    event: message_start\n    data: {\"type\":\"message_start\",\"message\":{\"id\":\"msg_014p7gG3wDgGV9EUtLvnow3U\",\"type\":\"message\",\"role\":\"assistant\",\"model\":\"claude-3-haiku-20240307\",\"stop_sequence\":null,\"usage\":{\"input_tokens\":472,\"output_tokens\":2},\"content\":[],\"stop_reason\":null}}\n    \n    event: content_block_start\n    data: {\"type\":\"content_block_start\",\"index\":0,\"content_block\":{\"type\":\"text\",\"text\":\"\"}}\n    \n    event: ping\n    data: {\"type\": \"ping\"}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"Okay\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\",\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\" let\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\"'s\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\" check\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\" the\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\" weather\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\" for\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\" San\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\" Francisco\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\",\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\" CA\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":0,\"delta\":{\"type\":\"text_delta\",\"text\":\":\"}}\n    \n    event: content_block_stop\n    data: {\"type\":\"content_block_stop\",\"index\":0}\n    \n    event: content_block_start\n    data: {\"type\":\"content_block_start\",\"index\":1,\"content_block\":{\"type\":\"tool_use\",\"id\":\"toolu_01T1x1fJ34qAmk2tNTrN7Up6\",\"name\":\"get_weather\",\"input\":{}}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":1,\"delta\":{\"type\":\"input_json_delta\",\"partial_json\":\"\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":1,\"delta\":{\"type\":\"input_json_delta\",\"partial_json\":\"{\\\"location\\\":\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":1,\"delta\":{\"type\":\"input_json_delta\",\"partial_json\":\" \\\"San\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":1,\"delta\":{\"type\":\"input_json_delta\",\"partial_json\":\" Francisc\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":1,\"delta\":{\"type\":\"input_json_delta\",\"partial_json\":\"o,\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":1,\"delta\":{\"type\":\"input_json_delta\",\"partial_json\":\" CA\\\"\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":1,\"delta\":{\"type\":\"input_json_delta\",\"partial_json\":\", \"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":1,\"delta\":{\"type\":\"input_json_delta\",\"partial_json\":\"\\\"unit\\\": \\\"fah\"}}\n    \n    event: content_block_delta\n    data: {\"type\":\"content_block_delta\",\"index\":1,\"delta\":{\"type\":\"input_json_delta\",\"partial_json\":\"renheit\\\"}\"}}\n    \n    event: content_block_stop\n    data: {\"type\":\"content_block_stop\",\"index\":1}\n    \n    event: message_delta\n    data: {\"type\":\"message_delta\",\"delta\":{\"stop_reason\":\"tool_use\",\"stop_sequence\":null},\"usage\":{\"output_tokens\":89}}\n    \n    event: message_stop\n    data: {\"type\":\"message_stop\"}\n    \n\n### \n\n​\n\nStreaming request with extended thinking\n\nIn this request, we enable extended thinking with streaming to see Claude’s step-by-step reasoning.\n\nRequest\n    \n    \n    curl https://api.anthropic.com/v1/messages \\\n         --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n         --header \"anthropic-version: 2023-06-01\" \\\n         --header \"content-type: application/json\" \\\n         --data \\\n    '{\n        \"model\": \"claude-3-7-sonnet-20250219\",\n        \"max_tokens\": 20000,\n        \"stream\": true,\n        \"thinking\": {\n            \"type\": \"enabled\",\n            \"budget_tokens\": 16000\n        },\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"What is 27 * 453?\"\n            }\n        ]\n    }'\n    \n\nResponse\n    \n    \n    event: message_start\n    data: {\"type\": \"message_start\", \"message\": {\"id\": \"msg_01...\", \"type\": \"message\", \"role\": \"assistant\", \"content\": [], \"model\": \"claude-3-7-sonnet-20250219\", \"stop_reason\": null, \"stop_sequence\": null}}\n    \n    event: content_block_start\n    data: {\"type\": \"content_block_start\", \"index\": 0, \"content_block\": {\"type\": \"thinking\", \"thinking\": \"\"}}\n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"thinking_delta\", \"thinking\": \"Let me solve this step by step:\\n\\n1. First break down 27 * 453\"}}\n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"thinking_delta\", \"thinking\": \"\\n2. 453 = 400 + 50 + 3\"}}\n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"thinking_delta\", \"thinking\": \"\\n3. 27 * 400 = 10,800\"}}\n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"thinking_delta\", \"thinking\": \"\\n4. 27 * 50 = 1,350\"}}\n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"thinking_delta\", \"thinking\": \"\\n5. 27 * 3 = 81\"}}\n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"thinking_delta\", \"thinking\": \"\\n6. 10,800 + 1,350 + 81 = 12,231\"}}\n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\", \"index\": 0, \"delta\": {\"type\": \"signature_delta\", \"signature\": \"EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds...\"}}\n    \n    event: content_block_stop\n    data: {\"type\": \"content_block_stop\", \"index\": 0}\n    \n    event: content_block_start\n    data: {\"type\": \"content_block_start\", \"index\": 1, \"content_block\": {\"type\": \"text\", \"text\": \"\"}}\n    \n    event: content_block_delta\n    data: {\"type\": \"content_block_delta\", \"index\": 1, \"delta\": {\"type\": \"text_delta\", \"text\": \"27 * 453 = 12,231\"}}\n    \n    event: content_block_stop\n    data: {\"type\": \"content_block_stop\", \"index\": 1}\n    \n    event: message_delta\n    data: {\"type\": \"message_delta\", \"delta\": {\"stop_reason\": \"end_turn\", \"stop_sequence\": null}}\n    \n    event: message_stop\n    data: {\"type\": \"message_stop\"}\n    \n\nWas this page helpful?\n\nYesNo\n\n[Count Message tokens](/en/api/messages-count-tokens)[Migrating from Text Completions](/en/api/migrating-from-text-completions-to-messages)\n\n[x](https://x.com/AnthropicAI)[linkedin](https://www.linkedin.com/company/anthropicresearch)\n\nOn this page\n\n  * Streaming with SDKs\n  * Event types\n  * Ping events\n  * Error events\n  * Other events\n  * Delta types\n  * Text delta\n  * Input JSON delta\n  * Thinking delta\n  * Raw HTTP Stream response\n  * Basic streaming request\n  * Streaming request with tool use\n  * Streaming request with extended thinking\n\n\n"
  },
  {
    "url": "https://docs.anthropic.com/en/api/messages-examples",
    "title": "Messages examples - Anthropic",
    "description": "Request and response examples for the Messages API",
    "content": "[Anthropic home page](/)\n\nEnglish\n\nSearch...\n\n  * [Research](https://www.anthropic.com/research)\n  * [News](https://www.anthropic.com/news)\n  * [Go to claude.ai](https://claude.ai/)\n  * [Go to claude.ai](https://claude.ai/)\n\n\n\nSearch...\n\nNavigation\n\nMessages\n\nMessages examples\n\n[Welcome](/en/home)[User Guides](/en/docs/welcome)[API Reference](/en/api/getting-started)[Prompt Library](/en/prompt-library/library)[Release Notes](/en/release-notes/overview)\n\n* [Developer Console](https://console.anthropic.com/)\n* [Developer Discord](https://www.anthropic.com/discord)\n* [Support](https://support.anthropic.com/)\n\n##### Using the API\n\n  * [Getting started](/en/api/getting-started)\n  * [IP addresses](/en/api/ip-addresses)\n  * [Versions](/en/api/versioning)\n  * [Errors](/en/api/errors)\n  * [Rate limits](/en/api/rate-limits)\n  * [Client SDKs](/en/api/client-sdks)\n  * [Supported regions](/en/api/supported-regions)\n  * [Getting help](/en/api/getting-help)\n\n\n\n##### Anthropic APIs\n\n  * Messages\n\n    * [POSTMessages](/en/api/messages)\n    * [POSTCount Message tokens](/en/api/messages-count-tokens)\n    * [Streaming Messages](/en/api/messages-streaming)\n    * [Migrating from Text Completions](/en/api/migrating-from-text-completions-to-messages)\n    * [Messages examples](/en/api/messages-examples)\n  * Models\n\n  * Message Batches\n\n  * Text Completions (Legacy)\n\n  * Admin API\n\n\n\n\n##### OpenAI SDK compatibility\n\n  * [OpenAI SDK compatibility (beta)](/en/api/openai-sdk)\n\n\n\n##### Experimental APIs\n\n  * Prompt tools\n\n\n\n\n##### Amazon Bedrock API\n\n  * [Amazon Bedrock API](/en/api/claude-on-amazon-bedrock)\n\n\n\n##### Vertex AI\n\n  * [Vertex AI API](/en/api/claude-on-vertex-ai)\n\n\n\nMessages\n\n# Messages examples\n\nRequest and response examples for the Messages API\n\nSee the [API reference](/en/api/messages) for full documentation on available parameters.\n\n## \n\n​\n\nBasic request and response\n\nJSON\n    \n    \n    {\n      \"id\": \"msg_01XFDUDYJgAACzvnptvVoYEL\",\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"Hello!\"\n        }\n      ],\n      \"model\": \"claude-3-7-sonnet-20250219\",\n      \"stop_reason\": \"end_turn\",\n      \"stop_sequence\": null,\n      \"usage\": {\n        \"input_tokens\": 12,\n        \"output_tokens\": 6\n      }\n    }\n    \n\n## \n\n​\n\nMultiple conversational turns\n\nThe Messages API is stateless, which means that you always send the full conversational history to the API. You can use this pattern to build up a conversation over time. Earlier conversational turns don’t necessarily need to actually originate from Claude — you can use synthetic `assistant` messages.\n\nShell\n    \n    \n    #!/bin/sh\n    curl https://api.anthropic.com/v1/messages \\\n         --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n         --header \"anthropic-version: 2023-06-01\" \\\n         --header \"content-type: application/json\" \\\n         --data \\\n    '{\n        \"model\": \"claude-3-7-sonnet-20250219\",\n        \"max_tokens\": 1024,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello, Claude\"},\n            {\"role\": \"assistant\", \"content\": \"Hello!\"},\n            {\"role\": \"user\", \"content\": \"Can you describe LLMs to me?\"}\n    \n        ]\n    }'\n    \n\nPython\n    \n    \n    import anthropic\n    \n    message = anthropic.Anthropic().messages.create(\n        model=\"claude-3-7-sonnet-20250219\",\n        max_tokens=1024,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Hello, Claude\"},\n            {\"role\": \"assistant\", \"content\": \"Hello!\"},\n            {\"role\": \"user\", \"content\": \"Can you describe LLMs to me?\"}\n        ],\n    )\n    print(message)\n    \n    \n\nTypeScript\n    \n    \n    import Anthropic from '@anthropic-ai/sdk';\n    \n    const anthropic = new Anthropic();\n    \n    await anthropic.messages.create({\n      model: 'claude-3-7-sonnet-20250219',\n      max_tokens: 1024,\n      messages: [\n        {\"role\": \"user\", \"content\": \"Hello, Claude\"},\n        {\"role\": \"assistant\", \"content\": \"Hello!\"},\n        {\"role\": \"user\", \"content\": \"Can you describe LLMs to me?\"}\n      ]\n    });\n    \n\nJSON\n    \n    \n    {\n        \"id\": \"msg_018gCsTGsXkYJVqYPxTgDHBU\",\n        \"type\": \"message\",\n        \"role\": \"assistant\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Sure, I'd be happy to provide...\"\n            }\n        ],\n        \"stop_reason\": \"end_turn\",\n        \"stop_sequence\": null,\n        \"usage\": {\n          \"input_tokens\": 30,\n          \"output_tokens\": 309\n        }\n    }\n    \n\n## \n\n​\n\nPutting words in Claude’s mouth\n\nYou can pre-fill part of Claude’s response in the last position of the input messages list. This can be used to shape Claude’s response. The example below uses `\"max_tokens\": 1` to get a single multiple choice answer from Claude.\n\nJSON\n    \n    \n    {\n      \"id\": \"msg_01Q8Faay6S7QPTvEUUQARt7h\",\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"C\"\n        }\n      ],\n      \"model\": \"claude-3-7-sonnet-20250219\",\n      \"stop_reason\": \"max_tokens\",\n      \"stop_sequence\": null,\n      \"usage\": {\n        \"input_tokens\": 42,\n        \"output_tokens\": 1\n      }\n    }\n    \n\n## \n\n​\n\nVision\n\nClaude can read both text and images in requests. We support both `base64` and `url` source types for images, and the `image/jpeg`, `image/png`, `image/gif`, and `image/webp` media types. See our [vision guide](/en/docs/vision) for more details.\n\nJSON\n    \n    \n    {\n      \"id\": \"msg_01EcyWo6m4hyW8KHs2y2pei5\",\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"This image shows an ant, specifically a close-up view of an ant. The ant is shown in detail, with its distinct head, antennae, and legs clearly visible. The image is focused on capturing the intricate details and features of the ant, likely taken with a macro lens to get an extreme close-up perspective.\"\n        }\n      ],\n      \"model\": \"claude-3-7-sonnet-20250219\",\n      \"stop_reason\": \"end_turn\",\n      \"stop_sequence\": null,\n      \"usage\": {\n        \"input_tokens\": 1551,\n        \"output_tokens\": 71\n      }\n    }\n    \n\n## \n\n​\n\nTool use, JSON mode, and computer use (beta)\n\nSee our [guide](/en/docs/build-with-claude/tool-use) for examples for how to use tools with the Messages API. See our [computer use (beta) guide](/en/docs/build-with-claude/computer-use) for examples of how to control desktop computer environments with the Messages API.\n\nWas this page helpful?\n\nYesNo\n\n[Migrating from Text Completions](/en/api/migrating-from-text-completions-to-messages)[List Models](/en/api/models-list)\n\n[x](https://x.com/AnthropicAI)[linkedin](https://www.linkedin.com/company/anthropicresearch)\n\nOn this page\n\n  * Basic request and response\n  * Multiple conversational turns\n  * Putting words in Claude’s mouth\n  * Vision\n  * Tool use, JSON mode, and computer use (beta)\n\n\n"
  },
  {
    "url": "https://docs.anthropic.com/en/api/messages-examples#vision",
    "title": "Messages examples - Anthropic",
    "description": "Request and response examples for the Messages API",
    "content": "[Anthropic home page](/)\n\nEnglish\n\nSearch...\n\n  * [Research](https://www.anthropic.com/research)\n  * [News](https://www.anthropic.com/news)\n  * [Go to claude.ai](https://claude.ai/)\n  * [Go to claude.ai](https://claude.ai/)\n\n\n\nSearch...\n\nNavigation\n\nMessages\n\nMessages examples\n\n[Welcome](/en/home)[User Guides](/en/docs/welcome)[API Reference](/en/api/getting-started)[Prompt Library](/en/prompt-library/library)[Release Notes](/en/release-notes/overview)\n\n* [Developer Console](https://console.anthropic.com/)\n* [Developer Discord](https://www.anthropic.com/discord)\n* [Support](https://support.anthropic.com/)\n\n##### Using the API\n\n  * [Getting started](/en/api/getting-started)\n  * [IP addresses](/en/api/ip-addresses)\n  * [Versions](/en/api/versioning)\n  * [Errors](/en/api/errors)\n  * [Rate limits](/en/api/rate-limits)\n  * [Client SDKs](/en/api/client-sdks)\n  * [Supported regions](/en/api/supported-regions)\n  * [Getting help](/en/api/getting-help)\n\n\n\n##### Anthropic APIs\n\n  * Messages\n\n    * [POSTMessages](/en/api/messages)\n    * [POSTCount Message tokens](/en/api/messages-count-tokens)\n    * [Streaming Messages](/en/api/messages-streaming)\n    * [Migrating from Text Completions](/en/api/migrating-from-text-completions-to-messages)\n    * [Messages examples](/en/api/messages-examples)\n  * Models\n\n  * Message Batches\n\n  * Text Completions (Legacy)\n\n  * Admin API\n\n\n\n\n##### OpenAI SDK compatibility\n\n  * [OpenAI SDK compatibility (beta)](/en/api/openai-sdk)\n\n\n\n##### Experimental APIs\n\n  * Prompt tools\n\n\n\n\n##### Amazon Bedrock API\n\n  * [Amazon Bedrock API](/en/api/claude-on-amazon-bedrock)\n\n\n\n##### Vertex AI\n\n  * [Vertex AI API](/en/api/claude-on-vertex-ai)\n\n\n\nMessages\n\n# Messages examples\n\nRequest and response examples for the Messages API\n\nSee the [API reference](/en/api/messages) for full documentation on available parameters.\n\n## \n\n​\n\nBasic request and response\n\nJSON\n    \n    \n    {\n      \"id\": \"msg_01XFDUDYJgAACzvnptvVoYEL\",\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"Hello!\"\n        }\n      ],\n      \"model\": \"claude-3-7-sonnet-20250219\",\n      \"stop_reason\": \"end_turn\",\n      \"stop_sequence\": null,\n      \"usage\": {\n        \"input_tokens\": 12,\n        \"output_tokens\": 6\n      }\n    }\n    \n\n## \n\n​\n\nMultiple conversational turns\n\nThe Messages API is stateless, which means that you always send the full conversational history to the API. You can use this pattern to build up a conversation over time. Earlier conversational turns don’t necessarily need to actually originate from Claude — you can use synthetic `assistant` messages.\n\nShell\n    \n    \n    #!/bin/sh\n    curl https://api.anthropic.com/v1/messages \\\n         --header \"x-api-key: $ANTHROPIC_API_KEY\" \\\n         --header \"anthropic-version: 2023-06-01\" \\\n         --header \"content-type: application/json\" \\\n         --data \\\n    '{\n        \"model\": \"claude-3-7-sonnet-20250219\",\n        \"max_tokens\": 1024,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello, Claude\"},\n            {\"role\": \"assistant\", \"content\": \"Hello!\"},\n            {\"role\": \"user\", \"content\": \"Can you describe LLMs to me?\"}\n    \n        ]\n    }'\n    \n\nPython\n    \n    \n    import anthropic\n    \n    message = anthropic.Anthropic().messages.create(\n        model=\"claude-3-7-sonnet-20250219\",\n        max_tokens=1024,\n        messages=[\n            {\"role\": \"user\", \"content\": \"Hello, Claude\"},\n            {\"role\": \"assistant\", \"content\": \"Hello!\"},\n            {\"role\": \"user\", \"content\": \"Can you describe LLMs to me?\"}\n        ],\n    )\n    print(message)\n    \n    \n\nTypeScript\n    \n    \n    import Anthropic from '@anthropic-ai/sdk';\n    \n    const anthropic = new Anthropic();\n    \n    await anthropic.messages.create({\n      model: 'claude-3-7-sonnet-20250219',\n      max_tokens: 1024,\n      messages: [\n        {\"role\": \"user\", \"content\": \"Hello, Claude\"},\n        {\"role\": \"assistant\", \"content\": \"Hello!\"},\n        {\"role\": \"user\", \"content\": \"Can you describe LLMs to me?\"}\n      ]\n    });\n    \n\nJSON\n    \n    \n    {\n        \"id\": \"msg_018gCsTGsXkYJVqYPxTgDHBU\",\n        \"type\": \"message\",\n        \"role\": \"assistant\",\n        \"content\": [\n            {\n                \"type\": \"text\",\n                \"text\": \"Sure, I'd be happy to provide...\"\n            }\n        ],\n        \"stop_reason\": \"end_turn\",\n        \"stop_sequence\": null,\n        \"usage\": {\n          \"input_tokens\": 30,\n          \"output_tokens\": 309\n        }\n    }\n    \n\n## \n\n​\n\nPutting words in Claude’s mouth\n\nYou can pre-fill part of Claude’s response in the last position of the input messages list. This can be used to shape Claude’s response. The example below uses `\"max_tokens\": 1` to get a single multiple choice answer from Claude.\n\nJSON\n    \n    \n    {\n      \"id\": \"msg_01Q8Faay6S7QPTvEUUQARt7h\",\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"C\"\n        }\n      ],\n      \"model\": \"claude-3-7-sonnet-20250219\",\n      \"stop_reason\": \"max_tokens\",\n      \"stop_sequence\": null,\n      \"usage\": {\n        \"input_tokens\": 42,\n        \"output_tokens\": 1\n      }\n    }\n    \n\n## \n\n​\n\nVision\n\nClaude can read both text and images in requests. We support both `base64` and `url` source types for images, and the `image/jpeg`, `image/png`, `image/gif`, and `image/webp` media types. See our [vision guide](/en/docs/vision) for more details.\n\nJSON\n    \n    \n    {\n      \"id\": \"msg_01EcyWo6m4hyW8KHs2y2pei5\",\n      \"type\": \"message\",\n      \"role\": \"assistant\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"This image shows an ant, specifically a close-up view of an ant. The ant is shown in detail, with its distinct head, antennae, and legs clearly visible. The image is focused on capturing the intricate details and features of the ant, likely taken with a macro lens to get an extreme close-up perspective.\"\n        }\n      ],\n      \"model\": \"claude-3-7-sonnet-20250219\",\n      \"stop_reason\": \"end_turn\",\n      \"stop_sequence\": null,\n      \"usage\": {\n        \"input_tokens\": 1551,\n        \"output_tokens\": 71\n      }\n    }\n    \n\n## \n\n​\n\nTool use, JSON mode, and computer use (beta)\n\nSee our [guide](/en/docs/build-with-claude/tool-use) for examples for how to use tools with the Messages API. See our [computer use (beta) guide](/en/docs/build-with-claude/computer-use) for examples of how to control desktop computer environments with the Messages API.\n\nWas this page helpful?\n\nYesNo\n\n[Migrating from Text Completions](/en/api/migrating-from-text-completions-to-messages)[List Models](/en/api/models-list)\n\n[x](https://x.com/AnthropicAI)[linkedin](https://www.linkedin.com/company/anthropicresearch)\n\nOn this page\n\n  * Basic request and response\n  * Multiple conversational turns\n  * Putting words in Claude’s mouth\n  * Vision\n  * Tool use, JSON mode, and computer use (beta)\n\n\n"
  },
  {
    "url": "https://docs.anthropic.com/en/api/messages#response-usage-cache-creation-input-tokens",
    "title": "Messages - Anthropic",
    "description": "Send a structured list of input messages with text and/or image content, and the model will generate the next message in the conversation.\n\nThe Messages API can be used for either single queries or stateless multi-turn conversations.\n\nLearn more about the Messages API in our [user guide](/en/docs/initial-setup)",
    "content": "[Anthropic home page](/)\n\nEnglish\n\nSearch...\n\n  * [Research](https://www.anthropic.com/research)\n  * [News](https://www.anthropic.com/news)\n  * [Go to claude.ai](https://claude.ai/)\n  * [Go to claude.ai](https://claude.ai/)\n\n\n\nSearch...\n\nNavigation\n\nMessages\n\nMessages\n\n[Welcome](/en/home)[User Guides](/en/docs/welcome)[API Reference](/en/api/getting-started)[Prompt Library](/en/prompt-library/library)[Release Notes](/en/release-notes/overview)\n\n* [Developer Console](https://console.anthropic.com/)\n* [Developer Discord](https://www.anthropic.com/discord)\n* [Support](https://support.anthropic.com/)\n\n##### Using the API\n\n  * [Getting started](/en/api/getting-started)\n  * [IP addresses](/en/api/ip-addresses)\n  * [Versions](/en/api/versioning)\n  * [Errors](/en/api/errors)\n  * [Rate limits](/en/api/rate-limits)\n  * [Client SDKs](/en/api/client-sdks)\n  * [Supported regions](/en/api/supported-regions)\n  * [Getting help](/en/api/getting-help)\n\n\n\n##### Anthropic APIs\n\n  * Messages\n\n    * [POSTMessages](/en/api/messages)\n    * [POSTCount Message tokens](/en/api/messages-count-tokens)\n    * [Streaming Messages](/en/api/messages-streaming)\n    * [Migrating from Text Completions](/en/api/migrating-from-text-completions-to-messages)\n    * [Messages examples](/en/api/messages-examples)\n  * Models\n\n  * Message Batches\n\n  * Text Completions (Legacy)\n\n  * Admin API\n\n\n\n\n##### OpenAI SDK compatibility\n\n  * [OpenAI SDK compatibility (beta)](/en/api/openai-sdk)\n\n\n\n##### Experimental APIs\n\n  * Prompt tools\n\n\n\n\n##### Amazon Bedrock API\n\n  * [Amazon Bedrock API](/en/api/claude-on-amazon-bedrock)\n\n\n\n##### Vertex AI\n\n  * [Vertex AI API](/en/api/claude-on-vertex-ai)\n\n\n\nMessages\n\n# Messages\n\nSend a structured list of input messages with text and/or image content, and the model will generate the next message in the conversation.\n\nThe Messages API can be used for either single queries or stateless multi-turn conversations.\n\nLearn more about the Messages API in our [user guide](/en/docs/initial-setup)\n\nPOST\n\n/\n\nv1\n\n/\n\nmessages\n\n#### Headers\n\n​\n\nanthropic-beta\n\nstring[]\n\nOptional header to specify the beta version(s) you want to use.\n\nTo use multiple betas, use a comma separated list like `beta1,beta2` or specify the header multiple times for each beta.\n\n​\n\nanthropic-version\n\nstring\n\nrequired\n\nThe version of the Anthropic API you want to use.\n\nRead more about versioning and our version history [here](https://docs.anthropic.com/en/api/versioning).\n\n​\n\nx-api-key\n\nstring\n\nrequired\n\nYour unique API key for authentication.\n\nThis key is required in the header of all API requests, to authenticate your account and access Anthropic's services. Get your API key through the [Console](https://console.anthropic.com/settings/keys). Each key is scoped to a Workspace.\n\n#### Body\n\napplication/json\n\n​\n\nmax_tokens\n\ninteger\n\nrequired\n\nThe maximum number of tokens to generate before stopping.\n\nNote that our models may stop _before_ reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.\n\nDifferent models have different maximum values for this parameter. See [models](https://docs.anthropic.com/en/docs/models-overview) for details.\n\nRequired range: `x > 1`\n\n​\n\nmessages\n\nobject[]\n\nrequired\n\nInput messages.\n\nOur models are trained to operate on alternating `user` and `assistant` conversational turns. When creating a new `Message`, you specify the prior conversational turns with the `messages` parameter, and the model then generates the next `Message` in the conversation. Consecutive `user` or `assistant` turns in your request will be combined into a single turn.\n\nEach input message must be an object with a `role` and `content`. You can specify a single `user`-role message, or you can include multiple `user` and `assistant` messages.\n\nIf the final message uses the `assistant` role, the response content will continue immediately from the content in that message. This can be used to constrain part of the model's response.\n\nExample with a single `user` message:\n    \n    \n    [{\"role\": \"user\", \"content\": \"Hello, Claude\"}]\n    \n\nExample with multiple conversational turns:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"Hello there.\"},\n      {\"role\": \"assistant\", \"content\": \"Hi, I'm Claude. How can I help you?\"},\n      {\"role\": \"user\", \"content\": \"Can you explain LLMs in plain English?\"},\n    ]\n    \n\nExample with a partially-filled response from Claude:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},\n      {\"role\": \"assistant\", \"content\": \"The best answer is (\"},\n    ]\n    \n\nEach input message `content` may be either a single `string` or an array of content blocks, where each block has a specific `type`. Using a `string` for `content` is shorthand for an array of one content block of type `\"text\"`. The following input messages are equivalent:\n    \n    \n    {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n    \n    \n    \n    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, Claude\"}]}\n    \n\nStarting with Claude 3 models, you can also send image content blocks:\n    \n    \n    {\"role\": \"user\", \"content\": [\n      {\n        \"type\": \"image\",\n        \"source\": {\n          \"type\": \"base64\",\n          \"media_type\": \"image/jpeg\",\n          \"data\": \"/9j/4AAQSkZJRg...\",\n        }\n      },\n      {\"type\": \"text\", \"text\": \"What is in this image?\"}\n    ]}\n    \n\nWe currently support the `base64` source type for images, and the `image/jpeg`, `image/png`, `image/gif`, and `image/webp` media types.\n\nSee [examples](https://docs.anthropic.com/en/api/messages-examples#vision) for more input examples.\n\nNote that if you want to include a [system prompt](https://docs.anthropic.com/en/docs/system-prompts), you can use the top-level `system` parameter — there is no `\"system\"` role for input messages in the Messages API.\n\nThere is a limit of 100000 messages in a single request.\n\nShow child attributes\n\n​\n\nmessages.content\n\nstringobject[]\n\nrequired\n\n​\n\nmessages.role\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`user`,\n\n`assistant`\n\n​\n\nmodel\n\nstring\n\nrequired\n\nThe model that will complete your prompt.\n\nSee [models](https://docs.anthropic.com/en/docs/models-overview) for additional details and options.\n\nRequired string length: `1 - 256`\n\n​\n\nmetadata\n\nobject\n\nAn object describing metadata about the request.\n\nShow child attributes\n\n​\n\nmetadata.user_id\n\nstring | null\n\nAn external identifier for the user who is associated with the request.\n\nThis should be a uuid, hash value, or other opaque identifier. Anthropic may use this id to help detect abuse. Do not include any identifying information such as name, email address, or phone number.\n\nMaximum length: `256`\n\n​\n\nstop_sequences\n\nstring[]\n\nCustom text sequences that will cause the model to stop generating.\n\nOur models will normally stop when they have naturally completed their turn, which will result in a response `stop_reason` of `\"end_turn\"`.\n\nIf you want the model to stop generating when it encounters custom strings of text, you can use the `stop_sequences` parameter. If the model encounters one of the custom sequences, the response `stop_reason` value will be `\"stop_sequence\"` and the response `stop_sequence` value will contain the matched stop sequence.\n\n​\n\nstream\n\nboolean\n\nWhether to incrementally stream the response using server-sent events.\n\nSee [streaming](https://docs.anthropic.com/en/api/messages-streaming) for details.\n\n​\n\nsystem\n\nstringobject[]\n\nSystem prompt.\n\nA system prompt is a way of providing context and instructions to Claude, such as specifying a particular goal or role. See our [guide to system prompts](https://docs.anthropic.com/en/docs/system-prompts).\n\n​\n\ntemperature\n\nnumber\n\nAmount of randomness injected into the response.\n\nDefaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to `1.0` for creative and generative tasks.\n\nNote that even with `temperature` of `0.0`, the results will not be fully deterministic.\n\nRequired range: `0 < x < 1`\n\n​\n\nthinking\n\nobject\n\nConfiguration for enabling Claude's extended thinking.\n\nWhen enabled, responses include `thinking` content blocks showing Claude's thinking process before the final answer. Requires a minimum budget of 1,024 tokens and counts towards your `max_tokens` limit.\n\nSee [extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for details.\n\n  * Enabled\n  * Disabled\n\n\n\nShow child attributes\n\n​\n\nthinking.budget_tokens\n\ninteger\n\nrequired\n\nDetermines how many tokens Claude can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality.\n\nMust be ≥1024 and less than `max_tokens`.\n\nSee [extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for details.\n\nRequired range: `x > 1024`\n\n​\n\nthinking.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`enabled`\n\n​\n\ntool_choice\n\nobject\n\nHow the model should use the provided tools. The model can use a specific tool, any available tool, decide by itself, or not use tools at all.\n\n  * Auto\n  * Any\n  * Tool\n  * ToolChoiceNone\n\n\n\nShow child attributes\n\n​\n\ntool_choice.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`auto`\n\n​\n\ntool_choice.disable_parallel_tool_use\n\nboolean\n\nWhether to disable parallel tool use.\n\nDefaults to `false`. If set to `true`, the model will output at most one tool use.\n\n​\n\ntools\n\nobject[]\n\nDefinitions of tools that the model may use.\n\nIf you include `tools` in your API request, the model may return `tool_use` content blocks that represent the model's use of those tools. You can then run those tools using the tool input generated by the model and then optionally return results back to the model using `tool_result` content blocks.\n\nEach tool definition includes:\n\n  * `name`: Name of the tool.\n  * `description`: Optional, but strongly-recommended description of the tool.\n  * `input_schema`: [JSON schema](https://json-schema.org/draft/2020-12) for the tool `input` shape that the model will produce in `tool_use` output content blocks.\n\n\n\nFor example, if you defined `tools` as:\n    \n    \n    [\n      {\n        \"name\": \"get_stock_price\",\n        \"description\": \"Get the current stock price for a given ticker symbol.\",\n        \"input_schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"ticker\": {\n              \"type\": \"string\",\n              \"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\"\n            }\n          },\n          \"required\": [\"ticker\"]\n        }\n      }\n    ]\n    \n\nAnd then asked the model \"What's the S&P 500 at today?\", the model might produce `tool_use` content blocks in the response like this:\n    \n    \n    [\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"toolu_01D7FLrfh4GYq7yT1ULFeyMV\",\n        \"name\": \"get_stock_price\",\n        \"input\": { \"ticker\": \"^GSPC\" }\n      }\n    ]\n    \n\nYou might then run your `get_stock_price` tool with `{\"ticker\": \"^GSPC\"}` as an input, and return the following back to the model in a subsequent `user` message:\n    \n    \n    [\n      {\n        \"type\": \"tool_result\",\n        \"tool_use_id\": \"toolu_01D7FLrfh4GYq7yT1ULFeyMV\",\n        \"content\": \"259.75 USD\"\n      }\n    ]\n    \n\nTools can be used for workflows that include running client-side tools and functions, or more generally whenever you want the model to produce a particular JSON structure of output.\n\nSee our [guide](https://docs.anthropic.com/en/docs/tool-use) for more details.\n\n  * Custom Tool\n  * ComputerUseTool_20241022\n  * BashTool_20241022\n  * TextEditor_20241022\n  * ComputerUseTool_20250124\n  * BashTool_20250124\n  * TextEditor_20250124\n\n\n\nShow child attributes\n\n​\n\ntools.input_schema\n\nobject\n\nrequired\n\n[JSON schema](https://json-schema.org/draft/2020-12) for this tool's input.\n\nThis defines the shape of the `input` that your tool accepts and that the model will produce.\n\nShow child attributes\n\n​\n\ntools.input_schema.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`object`\n\n​\n\ntools.input_schema.properties\n\nobject | null\n\n​\n\ntools.name\n\nstring\n\nrequired\n\nName of the tool.\n\nThis is how the tool will be called by the model and in tool_use blocks.\n\nRequired string length: `1 - 64`\n\n​\n\ntools.cache_control\n\nobject | null\n\nShow child attributes\n\n​\n\ntools.cache_control.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`ephemeral`\n\n​\n\ntools.description\n\nstring\n\nDescription of what this tool does.\n\nTool descriptions should be as detailed as possible. The more information that the model has about what the tool is and how to use it, the better it will perform. You can use natural language descriptions to reinforce important aspects of the tool input JSON schema.\n\n​\n\ntools.type\n\nenum<string> | null\n\nAvailable options:\n\n`custom`\n\n​\n\ntop_k\n\ninteger\n\nOnly sample from the top K options for each subsequent token.\n\nUsed to remove \"long tail\" low probability responses. [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).\n\nRecommended for advanced use cases only. You usually only need to use `temperature`.\n\nRequired range: `x > 0`\n\n​\n\ntop_p\n\nnumber\n\nUse nucleus sampling.\n\nIn nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by `top_p`. You should either alter `temperature` or `top_p`, but not both.\n\nRecommended for advanced use cases only. You usually only need to use `temperature`.\n\nRequired range: `0 < x < 1`\n\n#### Response\n\n200 - application/json\n\n​\n\ncontent\n\nobject[]\n\nrequired\n\nContent generated by the model.\n\nThis is an array of content blocks, each of which has a `type` that determines its shape.\n\nExample:\n    \n    \n    [{\"type\": \"text\", \"text\": \"Hi, I'm Claude.\"}]\n    \n\nIf the request input `messages` ended with an `assistant` turn, then the response `content` will continue directly from that last turn. You can use this to constrain the model's output.\n\nFor example, if the input `messages` were:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},\n      {\"role\": \"assistant\", \"content\": \"The best answer is (\"}\n    ]\n    \n\nThen the response `content` might be:\n    \n    \n    [{\"type\": \"text\", \"text\": \"B)\"}]\n    \n\n  * Text\n  * Tool Use\n  * Thinking\n  * Redacted Thinking\n\n\n\nShow child attributes\n\n​\n\ncontent.citations\n\nobject[] | null\n\nrequired\n\nCitations supporting the text block.\n\nThe type of citation returned will depend on the type of document being cited. Citing a PDF results in `page_location`, plain text results in `char_location`, and content document results in `content_block_location`.\n\n  * Character Location\n  * Page Location\n  * Content Block Location\n\n\n\nShow child attributes\n\n​\n\ncontent.citations.cited_text\n\nstring\n\nrequired\n\n​\n\ncontent.citations.document_index\n\ninteger\n\nrequired\n\nRequired range: `x > 0`\n\n​\n\ncontent.citations.document_title\n\nstring | null\n\nrequired\n\n​\n\ncontent.citations.end_char_index\n\ninteger\n\nrequired\n\n​\n\ncontent.citations.start_char_index\n\ninteger\n\nrequired\n\nRequired range: `x > 0`\n\n​\n\ncontent.citations.type\n\nenum<string>\n\ndefault:\n\nchar_location\n\nrequired\n\nAvailable options:\n\n`char_location`\n\n​\n\ncontent.text\n\nstring\n\nrequired\n\nMaximum length: `5000000`\n\n​\n\ncontent.type\n\nenum<string>\n\ndefault:\n\ntext\n\nrequired\n\nAvailable options:\n\n`text`\n\n​\n\nid\n\nstring\n\nrequired\n\nUnique object identifier.\n\nThe format and length of IDs may change over time.\n\n​\n\nmodel\n\nstring\n\nrequired\n\nThe model that handled the request.\n\nRequired string length: `1 - 256`\n\n​\n\nrole\n\nenum<string>\n\ndefault:\n\nassistant\n\nrequired\n\nConversational role of the generated message.\n\nThis will always be `\"assistant\"`.\n\nAvailable options:\n\n`assistant`\n\n​\n\nstop_reason\n\nenum<string> | null\n\nrequired\n\nThe reason that we stopped.\n\nThis may be one the following values:\n\n  * `\"end_turn\"`: the model reached a natural stopping point\n  * `\"max_tokens\"`: we exceeded the requested `max_tokens` or the model's maximum\n  * `\"stop_sequence\"`: one of your provided custom `stop_sequences` was generated\n  * `\"tool_use\"`: the model invoked one or more tools\n\n\n\nIn non-streaming mode this value is always non-null. In streaming mode, it is null in the `message_start` event and non-null otherwise.\n\nAvailable options:\n\n`end_turn`,\n\n`max_tokens`,\n\n`stop_sequence`,\n\n`tool_use`\n\n​\n\nstop_sequence\n\nstring | null\n\nrequired\n\nWhich custom stop sequence was generated, if any.\n\nThis value will be a non-null string if one of your custom stop sequences was generated.\n\n​\n\ntype\n\nenum<string>\n\ndefault:\n\nmessage\n\nrequired\n\nObject type.\n\nFor Messages, this is always `\"message\"`.\n\nAvailable options:\n\n`message`\n\n​\n\nusage\n\nobject\n\nrequired\n\nBilling and rate-limit usage.\n\nAnthropic's API bills and rate-limits by token counts, as tokens represent the underlying cost to our systems.\n\nUnder the hood, the API transforms requests into a format suitable for the model. The model's output then goes through a parsing stage before becoming an API response. As a result, the token counts in `usage` will not match one-to-one with the exact visible content of an API request or response.\n\nFor example, `output_tokens` will be non-zero, even for an empty string response from Claude.\n\nTotal input tokens in a request is the summation of `input_tokens`, `cache_creation_input_tokens`, and `cache_read_input_tokens`.\n\nShow child attributes\n\n​\n\nusage.cache_creation_input_tokens\n\ninteger | null\n\nrequired\n\nThe number of input tokens used to create the cache entry.\n\nRequired range: `x > 0`\n\n​\n\nusage.cache_read_input_tokens\n\ninteger | null\n\nrequired\n\nThe number of input tokens read from the cache.\n\nRequired range: `x > 0`\n\n​\n\nusage.input_tokens\n\ninteger\n\nrequired\n\nThe number of input tokens which were used.\n\nRequired range: `x > 0`\n\n​\n\nusage.output_tokens\n\ninteger\n\nrequired\n\nThe number of output tokens which were used.\n\nRequired range: `x > 0`\n\nWas this page helpful?\n\nYesNo\n\n[Getting help](/en/api/getting-help)[Count Message tokens](/en/api/messages-count-tokens)\n\n[x](https://x.com/AnthropicAI)[linkedin](https://www.linkedin.com/company/anthropicresearch)\n"
  },
  {
    "url": "https://docs.anthropic.com/en/api/messages#response-usage-cache-read-input-tokens",
    "title": "Messages - Anthropic",
    "description": "Send a structured list of input messages with text and/or image content, and the model will generate the next message in the conversation.\n\nThe Messages API can be used for either single queries or stateless multi-turn conversations.\n\nLearn more about the Messages API in our [user guide](/en/docs/initial-setup)",
    "content": "[Anthropic home page](/)\n\nEnglish\n\nSearch...\n\n  * [Research](https://www.anthropic.com/research)\n  * [News](https://www.anthropic.com/news)\n  * [Go to claude.ai](https://claude.ai/)\n  * [Go to claude.ai](https://claude.ai/)\n\n\n\nSearch...\n\nNavigation\n\nMessages\n\nMessages\n\n[Welcome](/en/home)[User Guides](/en/docs/welcome)[API Reference](/en/api/getting-started)[Prompt Library](/en/prompt-library/library)[Release Notes](/en/release-notes/overview)\n\n* [Developer Console](https://console.anthropic.com/)\n* [Developer Discord](https://www.anthropic.com/discord)\n* [Support](https://support.anthropic.com/)\n\n##### Using the API\n\n  * [Getting started](/en/api/getting-started)\n  * [IP addresses](/en/api/ip-addresses)\n  * [Versions](/en/api/versioning)\n  * [Errors](/en/api/errors)\n  * [Rate limits](/en/api/rate-limits)\n  * [Client SDKs](/en/api/client-sdks)\n  * [Supported regions](/en/api/supported-regions)\n  * [Getting help](/en/api/getting-help)\n\n\n\n##### Anthropic APIs\n\n  * Messages\n\n    * [POSTMessages](/en/api/messages)\n    * [POSTCount Message tokens](/en/api/messages-count-tokens)\n    * [Streaming Messages](/en/api/messages-streaming)\n    * [Migrating from Text Completions](/en/api/migrating-from-text-completions-to-messages)\n    * [Messages examples](/en/api/messages-examples)\n  * Models\n\n  * Message Batches\n\n  * Text Completions (Legacy)\n\n  * Admin API\n\n\n\n\n##### OpenAI SDK compatibility\n\n  * [OpenAI SDK compatibility (beta)](/en/api/openai-sdk)\n\n\n\n##### Experimental APIs\n\n  * Prompt tools\n\n\n\n\n##### Amazon Bedrock API\n\n  * [Amazon Bedrock API](/en/api/claude-on-amazon-bedrock)\n\n\n\n##### Vertex AI\n\n  * [Vertex AI API](/en/api/claude-on-vertex-ai)\n\n\n\nMessages\n\n# Messages\n\nSend a structured list of input messages with text and/or image content, and the model will generate the next message in the conversation.\n\nThe Messages API can be used for either single queries or stateless multi-turn conversations.\n\nLearn more about the Messages API in our [user guide](/en/docs/initial-setup)\n\nPOST\n\n/\n\nv1\n\n/\n\nmessages\n\n#### Headers\n\n​\n\nanthropic-beta\n\nstring[]\n\nOptional header to specify the beta version(s) you want to use.\n\nTo use multiple betas, use a comma separated list like `beta1,beta2` or specify the header multiple times for each beta.\n\n​\n\nanthropic-version\n\nstring\n\nrequired\n\nThe version of the Anthropic API you want to use.\n\nRead more about versioning and our version history [here](https://docs.anthropic.com/en/api/versioning).\n\n​\n\nx-api-key\n\nstring\n\nrequired\n\nYour unique API key for authentication.\n\nThis key is required in the header of all API requests, to authenticate your account and access Anthropic's services. Get your API key through the [Console](https://console.anthropic.com/settings/keys). Each key is scoped to a Workspace.\n\n#### Body\n\napplication/json\n\n​\n\nmax_tokens\n\ninteger\n\nrequired\n\nThe maximum number of tokens to generate before stopping.\n\nNote that our models may stop _before_ reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate.\n\nDifferent models have different maximum values for this parameter. See [models](https://docs.anthropic.com/en/docs/models-overview) for details.\n\nRequired range: `x > 1`\n\n​\n\nmessages\n\nobject[]\n\nrequired\n\nInput messages.\n\nOur models are trained to operate on alternating `user` and `assistant` conversational turns. When creating a new `Message`, you specify the prior conversational turns with the `messages` parameter, and the model then generates the next `Message` in the conversation. Consecutive `user` or `assistant` turns in your request will be combined into a single turn.\n\nEach input message must be an object with a `role` and `content`. You can specify a single `user`-role message, or you can include multiple `user` and `assistant` messages.\n\nIf the final message uses the `assistant` role, the response content will continue immediately from the content in that message. This can be used to constrain part of the model's response.\n\nExample with a single `user` message:\n    \n    \n    [{\"role\": \"user\", \"content\": \"Hello, Claude\"}]\n    \n\nExample with multiple conversational turns:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"Hello there.\"},\n      {\"role\": \"assistant\", \"content\": \"Hi, I'm Claude. How can I help you?\"},\n      {\"role\": \"user\", \"content\": \"Can you explain LLMs in plain English?\"},\n    ]\n    \n\nExample with a partially-filled response from Claude:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},\n      {\"role\": \"assistant\", \"content\": \"The best answer is (\"},\n    ]\n    \n\nEach input message `content` may be either a single `string` or an array of content blocks, where each block has a specific `type`. Using a `string` for `content` is shorthand for an array of one content block of type `\"text\"`. The following input messages are equivalent:\n    \n    \n    {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n    \n    \n    \n    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Hello, Claude\"}]}\n    \n\nStarting with Claude 3 models, you can also send image content blocks:\n    \n    \n    {\"role\": \"user\", \"content\": [\n      {\n        \"type\": \"image\",\n        \"source\": {\n          \"type\": \"base64\",\n          \"media_type\": \"image/jpeg\",\n          \"data\": \"/9j/4AAQSkZJRg...\",\n        }\n      },\n      {\"type\": \"text\", \"text\": \"What is in this image?\"}\n    ]}\n    \n\nWe currently support the `base64` source type for images, and the `image/jpeg`, `image/png`, `image/gif`, and `image/webp` media types.\n\nSee [examples](https://docs.anthropic.com/en/api/messages-examples#vision) for more input examples.\n\nNote that if you want to include a [system prompt](https://docs.anthropic.com/en/docs/system-prompts), you can use the top-level `system` parameter — there is no `\"system\"` role for input messages in the Messages API.\n\nThere is a limit of 100000 messages in a single request.\n\nShow child attributes\n\n​\n\nmessages.content\n\nstringobject[]\n\nrequired\n\n​\n\nmessages.role\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`user`,\n\n`assistant`\n\n​\n\nmodel\n\nstring\n\nrequired\n\nThe model that will complete your prompt.\n\nSee [models](https://docs.anthropic.com/en/docs/models-overview) for additional details and options.\n\nRequired string length: `1 - 256`\n\n​\n\nmetadata\n\nobject\n\nAn object describing metadata about the request.\n\nShow child attributes\n\n​\n\nmetadata.user_id\n\nstring | null\n\nAn external identifier for the user who is associated with the request.\n\nThis should be a uuid, hash value, or other opaque identifier. Anthropic may use this id to help detect abuse. Do not include any identifying information such as name, email address, or phone number.\n\nMaximum length: `256`\n\n​\n\nstop_sequences\n\nstring[]\n\nCustom text sequences that will cause the model to stop generating.\n\nOur models will normally stop when they have naturally completed their turn, which will result in a response `stop_reason` of `\"end_turn\"`.\n\nIf you want the model to stop generating when it encounters custom strings of text, you can use the `stop_sequences` parameter. If the model encounters one of the custom sequences, the response `stop_reason` value will be `\"stop_sequence\"` and the response `stop_sequence` value will contain the matched stop sequence.\n\n​\n\nstream\n\nboolean\n\nWhether to incrementally stream the response using server-sent events.\n\nSee [streaming](https://docs.anthropic.com/en/api/messages-streaming) for details.\n\n​\n\nsystem\n\nstringobject[]\n\nSystem prompt.\n\nA system prompt is a way of providing context and instructions to Claude, such as specifying a particular goal or role. See our [guide to system prompts](https://docs.anthropic.com/en/docs/system-prompts).\n\n​\n\ntemperature\n\nnumber\n\nAmount of randomness injected into the response.\n\nDefaults to `1.0`. Ranges from `0.0` to `1.0`. Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to `1.0` for creative and generative tasks.\n\nNote that even with `temperature` of `0.0`, the results will not be fully deterministic.\n\nRequired range: `0 < x < 1`\n\n​\n\nthinking\n\nobject\n\nConfiguration for enabling Claude's extended thinking.\n\nWhen enabled, responses include `thinking` content blocks showing Claude's thinking process before the final answer. Requires a minimum budget of 1,024 tokens and counts towards your `max_tokens` limit.\n\nSee [extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for details.\n\n  * Enabled\n  * Disabled\n\n\n\nShow child attributes\n\n​\n\nthinking.budget_tokens\n\ninteger\n\nrequired\n\nDetermines how many tokens Claude can use for its internal reasoning process. Larger budgets can enable more thorough analysis for complex problems, improving response quality.\n\nMust be ≥1024 and less than `max_tokens`.\n\nSee [extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for details.\n\nRequired range: `x > 1024`\n\n​\n\nthinking.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`enabled`\n\n​\n\ntool_choice\n\nobject\n\nHow the model should use the provided tools. The model can use a specific tool, any available tool, decide by itself, or not use tools at all.\n\n  * Auto\n  * Any\n  * Tool\n  * ToolChoiceNone\n\n\n\nShow child attributes\n\n​\n\ntool_choice.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`auto`\n\n​\n\ntool_choice.disable_parallel_tool_use\n\nboolean\n\nWhether to disable parallel tool use.\n\nDefaults to `false`. If set to `true`, the model will output at most one tool use.\n\n​\n\ntools\n\nobject[]\n\nDefinitions of tools that the model may use.\n\nIf you include `tools` in your API request, the model may return `tool_use` content blocks that represent the model's use of those tools. You can then run those tools using the tool input generated by the model and then optionally return results back to the model using `tool_result` content blocks.\n\nEach tool definition includes:\n\n  * `name`: Name of the tool.\n  * `description`: Optional, but strongly-recommended description of the tool.\n  * `input_schema`: [JSON schema](https://json-schema.org/draft/2020-12) for the tool `input` shape that the model will produce in `tool_use` output content blocks.\n\n\n\nFor example, if you defined `tools` as:\n    \n    \n    [\n      {\n        \"name\": \"get_stock_price\",\n        \"description\": \"Get the current stock price for a given ticker symbol.\",\n        \"input_schema\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"ticker\": {\n              \"type\": \"string\",\n              \"description\": \"The stock ticker symbol, e.g. AAPL for Apple Inc.\"\n            }\n          },\n          \"required\": [\"ticker\"]\n        }\n      }\n    ]\n    \n\nAnd then asked the model \"What's the S&P 500 at today?\", the model might produce `tool_use` content blocks in the response like this:\n    \n    \n    [\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"toolu_01D7FLrfh4GYq7yT1ULFeyMV\",\n        \"name\": \"get_stock_price\",\n        \"input\": { \"ticker\": \"^GSPC\" }\n      }\n    ]\n    \n\nYou might then run your `get_stock_price` tool with `{\"ticker\": \"^GSPC\"}` as an input, and return the following back to the model in a subsequent `user` message:\n    \n    \n    [\n      {\n        \"type\": \"tool_result\",\n        \"tool_use_id\": \"toolu_01D7FLrfh4GYq7yT1ULFeyMV\",\n        \"content\": \"259.75 USD\"\n      }\n    ]\n    \n\nTools can be used for workflows that include running client-side tools and functions, or more generally whenever you want the model to produce a particular JSON structure of output.\n\nSee our [guide](https://docs.anthropic.com/en/docs/tool-use) for more details.\n\n  * Custom Tool\n  * ComputerUseTool_20241022\n  * BashTool_20241022\n  * TextEditor_20241022\n  * ComputerUseTool_20250124\n  * BashTool_20250124\n  * TextEditor_20250124\n\n\n\nShow child attributes\n\n​\n\ntools.input_schema\n\nobject\n\nrequired\n\n[JSON schema](https://json-schema.org/draft/2020-12) for this tool's input.\n\nThis defines the shape of the `input` that your tool accepts and that the model will produce.\n\nShow child attributes\n\n​\n\ntools.input_schema.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`object`\n\n​\n\ntools.input_schema.properties\n\nobject | null\n\n​\n\ntools.name\n\nstring\n\nrequired\n\nName of the tool.\n\nThis is how the tool will be called by the model and in tool_use blocks.\n\nRequired string length: `1 - 64`\n\n​\n\ntools.cache_control\n\nobject | null\n\nShow child attributes\n\n​\n\ntools.cache_control.type\n\nenum<string>\n\nrequired\n\nAvailable options:\n\n`ephemeral`\n\n​\n\ntools.description\n\nstring\n\nDescription of what this tool does.\n\nTool descriptions should be as detailed as possible. The more information that the model has about what the tool is and how to use it, the better it will perform. You can use natural language descriptions to reinforce important aspects of the tool input JSON schema.\n\n​\n\ntools.type\n\nenum<string> | null\n\nAvailable options:\n\n`custom`\n\n​\n\ntop_k\n\ninteger\n\nOnly sample from the top K options for each subsequent token.\n\nUsed to remove \"long tail\" low probability responses. [Learn more technical details here](https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277).\n\nRecommended for advanced use cases only. You usually only need to use `temperature`.\n\nRequired range: `x > 0`\n\n​\n\ntop_p\n\nnumber\n\nUse nucleus sampling.\n\nIn nucleus sampling, we compute the cumulative distribution over all the options for each subsequent token in decreasing probability order and cut it off once it reaches a particular probability specified by `top_p`. You should either alter `temperature` or `top_p`, but not both.\n\nRecommended for advanced use cases only. You usually only need to use `temperature`.\n\nRequired range: `0 < x < 1`\n\n#### Response\n\n200 - application/json\n\n​\n\ncontent\n\nobject[]\n\nrequired\n\nContent generated by the model.\n\nThis is an array of content blocks, each of which has a `type` that determines its shape.\n\nExample:\n    \n    \n    [{\"type\": \"text\", \"text\": \"Hi, I'm Claude.\"}]\n    \n\nIf the request input `messages` ended with an `assistant` turn, then the response `content` will continue directly from that last turn. You can use this to constrain the model's output.\n\nFor example, if the input `messages` were:\n    \n    \n    [\n      {\"role\": \"user\", \"content\": \"What's the Greek name for Sun? (A) Sol (B) Helios (C) Sun\"},\n      {\"role\": \"assistant\", \"content\": \"The best answer is (\"}\n    ]\n    \n\nThen the response `content` might be:\n    \n    \n    [{\"type\": \"text\", \"text\": \"B)\"}]\n    \n\n  * Text\n  * Tool Use\n  * Thinking\n  * Redacted Thinking\n\n\n\nShow child attributes\n\n​\n\ncontent.citations\n\nobject[] | null\n\nrequired\n\nCitations supporting the text block.\n\nThe type of citation returned will depend on the type of document being cited. Citing a PDF results in `page_location`, plain text results in `char_location`, and content document results in `content_block_location`.\n\n  * Character Location\n  * Page Location\n  * Content Block Location\n\n\n\nShow child attributes\n\n​\n\ncontent.citations.cited_text\n\nstring\n\nrequired\n\n​\n\ncontent.citations.document_index\n\ninteger\n\nrequired\n\nRequired range: `x > 0`\n\n​\n\ncontent.citations.document_title\n\nstring | null\n\nrequired\n\n​\n\ncontent.citations.end_char_index\n\ninteger\n\nrequired\n\n​\n\ncontent.citations.start_char_index\n\ninteger\n\nrequired\n\nRequired range: `x > 0`\n\n​\n\ncontent.citations.type\n\nenum<string>\n\ndefault:\n\nchar_location\n\nrequired\n\nAvailable options:\n\n`char_location`\n\n​\n\ncontent.text\n\nstring\n\nrequired\n\nMaximum length: `5000000`\n\n​\n\ncontent.type\n\nenum<string>\n\ndefault:\n\ntext\n\nrequired\n\nAvailable options:\n\n`text`\n\n​\n\nid\n\nstring\n\nrequired\n\nUnique object identifier.\n\nThe format and length of IDs may change over time.\n\n​\n\nmodel\n\nstring\n\nrequired\n\nThe model that handled the request.\n\nRequired string length: `1 - 256`\n\n​\n\nrole\n\nenum<string>\n\ndefault:\n\nassistant\n\nrequired\n\nConversational role of the generated message.\n\nThis will always be `\"assistant\"`.\n\nAvailable options:\n\n`assistant`\n\n​\n\nstop_reason\n\nenum<string> | null\n\nrequired\n\nThe reason that we stopped.\n\nThis may be one the following values:\n\n  * `\"end_turn\"`: the model reached a natural stopping point\n  * `\"max_tokens\"`: we exceeded the requested `max_tokens` or the model's maximum\n  * `\"stop_sequence\"`: one of your provided custom `stop_sequences` was generated\n  * `\"tool_use\"`: the model invoked one or more tools\n\n\n\nIn non-streaming mode this value is always non-null. In streaming mode, it is null in the `message_start` event and non-null otherwise.\n\nAvailable options:\n\n`end_turn`,\n\n`max_tokens`,\n\n`stop_sequence`,\n\n`tool_use`\n\n​\n\nstop_sequence\n\nstring | null\n\nrequired\n\nWhich custom stop sequence was generated, if any.\n\nThis value will be a non-null string if one of your custom stop sequences was generated.\n\n​\n\ntype\n\nenum<string>\n\ndefault:\n\nmessage\n\nrequired\n\nObject type.\n\nFor Messages, this is always `\"message\"`.\n\nAvailable options:\n\n`message`\n\n​\n\nusage\n\nobject\n\nrequired\n\nBilling and rate-limit usage.\n\nAnthropic's API bills and rate-limits by token counts, as tokens represent the underlying cost to our systems.\n\nUnder the hood, the API transforms requests into a format suitable for the model. The model's output then goes through a parsing stage before becoming an API response. As a result, the token counts in `usage` will not match one-to-one with the exact visible content of an API request or response.\n\nFor example, `output_tokens` will be non-zero, even for an empty string response from Claude.\n\nTotal input tokens in a request is the summation of `input_tokens`, `cache_creation_input_tokens`, and `cache_read_input_tokens`.\n\nShow child attributes\n\n​\n\nusage.cache_creation_input_tokens\n\ninteger | null\n\nrequired\n\nThe number of input tokens used to create the cache entry.\n\nRequired range: `x > 0`\n\n​\n\nusage.cache_read_input_tokens\n\ninteger | null\n\nrequired\n\nThe number of input tokens read from the cache.\n\nRequired range: `x > 0`\n\n​\n\nusage.input_tokens\n\ninteger\n\nrequired\n\nThe number of input tokens which were used.\n\nRequired range: `x > 0`\n\n​\n\nusage.output_tokens\n\ninteger\n\nrequired\n\nThe number of output tokens which were used.\n\nRequired range: `x > 0`\n\nWas this page helpful?\n\nYesNo\n\n[Getting help](/en/api/getting-help)[Count Message tokens](/en/api/messages-count-tokens)\n\n[x](https://x.com/AnthropicAI)[linkedin](https://www.linkedin.com/company/anthropicresearch)\n"
  },
  {
    "url": "https://docs.anthropic.com/en/api/system-prompts",
    "title": "Home - Anthropic",
    "description": "",
    "content": "[Anthropic home page](/)\n\nEnglish\n\nSearch...\n\n  * [Research](https://www.anthropic.com/research)\n  * [News](https://www.anthropic.com/news)\n  * [Go to claude.ai](https://claude.ai/)\n  * [Go to claude.ai](https://claude.ai/)\n\n\n\nSearch...\n\nNavigation\n\n[Welcome](/en/home)[User Guides](/en/docs/welcome)[API Reference](/en/api/getting-started)[Prompt Library](/en/prompt-library/library)[Release Notes](/en/release-notes/overview)\n\nBuild with\n\nLearn how to get started with the Anthropic API and Claude.\n\nHelp me get started with prompt caching…\n\n[Explore the docs](/en/docs/welcome)\n\nGet started with tools and guides\n\n## [Get startedMake your first API call in minutes.](/en/docs/initial-setup)## [API ReferenceIntegrate and scale using our API and SDKs.](/en/api/getting-started)## [Anthropic ConsoleCraft and test powerful prompts directly in your browser.](https://console.anthropic.com)## [Anthropic CoursesExplore Anthropic’s educational courses and projects.](https://github.com/anthropics/courses)## [Anthropic CookbookSee replicable code samples and implementations.](https://github.com/anthropics/anthropic-cookbook)## [Anthropic QuickstartsDeployable applications built with our API.](https://github.com/anthropics/anthropic-quickstarts)\n"
  },
  {
    "url": "https://docs.anthropic.com/en/api/human-in-the-loop",
    "title": "Home - Anthropic",
    "description": "",
    "content": "[Anthropic home page](/)\n\nEnglish\n\nSearch...\n\n  * [Research](https://www.anthropic.com/research)\n  * [News](https://www.anthropic.com/news)\n  * [Go to claude.ai](https://claude.ai/)\n  * [Go to claude.ai](https://claude.ai/)\n\n\n\nSearch...\n\nNavigation\n\n[Welcome](/en/home)[User Guides](/en/docs/welcome)[API Reference](/en/api/getting-started)[Prompt Library](/en/prompt-library/library)[Release Notes](/en/release-notes/overview)\n\nBuild with\n\nLearn how to get started with the Anthropic API and Claude.\n\nHelp me get started with prompt caching…\n\n[Explore the docs](/en/docs/welcome)\n\nGet started with tools and guides\n\n## [Get startedMake your first API call in minutes.](/en/docs/initial-setup)## [API ReferenceIntegrate and scale using our API and SDKs.](/en/api/getting-started)## [Anthropic ConsoleCraft and test powerful prompts directly in your browser.](https://console.anthropic.com)## [Anthropic CoursesExplore Anthropic’s educational courses and projects.](https://github.com/anthropics/courses)## [Anthropic CookbookSee replicable code samples and implementations.](https://github.com/anthropics/anthropic-cookbook)## [Anthropic QuickstartsDeployable applications built with our API.](https://github.com/anthropics/anthropic-quickstarts)\n"
  }
]