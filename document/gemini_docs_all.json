[
  {
    "url": "https://ai.google.dev/gemini-api/docs/",
    "title": "Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\n# Gemini Developer API\n\n[Get a Gemini API Key](https://aistudio.google.com/apikey)\n\nGet a Gemini API key and make your first API request in minutes.\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"YOUR_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=\"Explain how AI works\",\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"YOUR_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Explain how AI works\",\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=YOUR_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Explain how AI works in a few words\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n## Meet the models\n\n[Use Gemini in Google AI Studio](https://aistudio.google.com)\n\n2.5 Pro experiment\n\nOur most powerful thinking model with features for complex reasoning and much more \n\n[](/gemini-api/docs/models#gemini-2.5-pro-preview-03-25)\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n[](/gemini-api/docs/models/gemini#gemini-2.0-flash)\n\n2.0 Flash-Lite bolt\n\nOur fastest and most cost-efficient multimodal model with great performance for high-frequency tasks \n\n[](/gemini-api/docs/models/gemini#gemini-2.0-flash-lite)\n\n## Explore the API\n\n### Native Image Generation\n\nGenerate and edit highly contextual images natively with Gemini 2.0 Flash.\n\n[](/gemini-api/docs/image-generation)\n\n### Explore long context\n\nInput millions of tokens to Gemini models and derive understanding from unstructured images, videos, and documents.\n\n[](/gemini-api/docs/long-context)\n\n### Generate structured outputs\n\nConstrain Gemini to respond with JSON, a structured data format suitable for automated processing.\n\n[](/gemini-api/docs/structured-output)\n\n### Start building with the Gemini API\n\n[Get started](/gemini-api/docs/quickstart)\n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\n[[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/image-generation#imagen",
    "title": "Generate images  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Generate images \n\nThe Gemini API supports image generation using Gemini 2.0 Flash Experimental and using Imagen 3. This guide helps you get started with both models.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Generate images using Gemini\n\nGemini 2.0 Flash Experimental supports the ability to output text and inline images. This lets you use Gemini to conversationally edit images or generate outputs with interwoven text (for example, generating a blog post with text and images in a single turn). All generated images include a [SynthID watermark](/responsible/docs/safeguards/synthid), and images in Google AI Studio include a visible watermark as well.\n\n**Note:** Make sure to include `responseModalities`: [\"Text\", \"Image\"] in your generation configuration for text and image output with `gemini-2.0-flash-exp-image-generation`. Image only is not allowed.\n\nThe following example shows how to use Gemini 2.0 to generate text-and-image output:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    from PIL import Image\n    from io import BytesIO\n    import base64\n    \n    client = genai.Client()\n    \n    contents = ('Hi, can you create a 3d rendered image of a pig '\n                'with wings and a top hat flying over a happy '\n                'futuristic scifi city with lots of greenery?')\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash-exp-image-generation\",\n        contents=contents,\n        config=types.GenerateContentConfig(\n          response_modalities=['Text', 'Image']\n        )\n    )\n    \n    for part in response.candidates[0].content.parts:\n      if part.text is not None:\n        print(part.text)\n      elif part.inline_data is not None:\n        image = Image.open(BytesIO((part.inline_data.data)))\n        image.save('gemini-native-image.png')\n        image.show()\n    \n\n### JavaScript\n\n**Note:** We've released the [Google SDK for TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai) in [preview launch stage](https://github.com/googleapis/js-genai?tab=readme-ov-file#preview-launch). Use this SDK for image generation features.\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    import * as fs from \"node:fs\";\n    \n    async function main() {\n    \n      const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n      const contents =\n        \"Hi, can you create a 3d rendered image of a pig \" +\n        \"with wings and a top hat flying over a happy \" +\n        \"futuristic scifi city with lots of greenery?\";\n    \n      // Set responseModalities to include \"Image\" so the model can generate  an image\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash-exp-image-generation\",\n        contents: contents,\n        config: {\n          responseModalities: [\"Text\", \"Image\"],\n        },\n      });\n      for (const part of response.candidates[0].content.parts) {\n        // Based on the part type, either show the text or save the image\n        if (part.text) {\n          console.log(part.text);\n        } else if (part.inlineData) {\n          const imageData = part.inlineData.data;\n          const buffer = Buffer.from(imageData, \"base64\");\n          fs.writeFileSync(\"gemini-native-image.png\", buffer);\n          console.log(\"Image saved as gemini-native-image.png\");\n        }\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl -s -X POST \\\n      \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\n        \"contents\": [{\n          \"parts\": [\n            {\"text\": \"Hi, can you create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?\"}\n          ]\n        }],\n        \"generationConfig\":{\"responseModalities\":[\"Text\",\"Image\"]}\n      }' \\\n      | grep -o '\"data\": \"[^\"]*\"' \\\n      | cut -d'\"' -f4 \\\n      | base64 --decode > gemini-native-image.png\n    \n\nAI-generated image of a fantastical flying pig\n\nDepending on the prompt and context, Gemini will generate content in different modes (text to image, text to image and text, etc.). Here are some examples:\n\n  * Text to image \n    * **Example prompt:** \"Generate an image of the Eiffel tower with fireworks in the background.\"\n  * Text to image(s) and text (interleaved) \n    * **Example prompt:** \"Generate an illustrated recipe for a paella.\"\n  * Image(s) and text to image(s) and text (interleaved) \n    * **Example prompt:** (With an image of a furnished room) \"What other color sofas would work in my space? can you update the image?\"\n  * Image editing (text and image to image) \n    * **Example prompt:** \"Edit this image to make it look like a cartoon\"\n    * **Example prompt:** [image of a cat] + [image of a pillow] + \"Create a cross stitch of my cat on this pillow.\"\n  * Multi-turn image editing (chat) \n    * **Example prompts:** [upload an image of a blue car.] \"Turn this car into a convertible.\" \"Now change the color to yellow.\"\n\n\n\n### Image editing with Gemini\n\nTo perform image editing, add an image as input. The following example demonstrats uploading base64 encoded images. For multiple images and larger payloads, check the [image input](/gemini-api/docs/vision#image-input) section.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    from PIL import Image\n    from io import BytesIO\n    \n    import PIL.Image\n    \n    image = PIL.Image.open('/path/to/image.png')\n    \n    client = genai.Client()\n    \n    text_input = ('Hi, This is a picture of me.'\n                'Can you add a llama next to me?',)\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash-exp-image-generation\",\n        contents=[text_input, image],\n        config=types.GenerateContentConfig(\n          response_modalities=['Text', 'Image']\n        )\n    )\n    \n    for part in response.candidates[0].content.parts:\n      if part.text is not None:\n        print(part.text)\n      elif part.inline_data is not None:\n        image = Image.open(BytesIO(part.inline_data.data))\n        image.show()\n    \n\n### JavaScript\n\n**Note:** We've released the [Google SDK for TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai) in [preview launch stage](https://github.com/googleapis/js-genai?tab=readme-ov-file#preview-launch). Use this SDK for image generation features.\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    import * as fs from \"node:fs\";\n    \n    async function main() {\n    \n      const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n      // Load the image from the local file system\n      const imagePath = \"path/to/image.png\";\n      const imageData = fs.readFileSync(imagePath);\n      const base64Image = imageData.toString(\"base64\");\n    \n      // Prepare the content parts\n      const contents = [\n        { text: \"Can you add a llama next to the image?\" },\n        {\n          inlineData: {\n            mimeType: \"image/png\",\n            data: base64Image,\n          },\n        },\n      ];\n    \n      // Set responseModalities to include \"Image\" so the model can generate an image\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash-exp-image-generation\",\n        contents: contents,\n        config: {\n          responseModalities: [\"Text\", \"Image\"],\n        },\n      });\n      for (const part of response.candidates[0].content.parts) {\n        // Based on the part type, either show the text or save the image\n        if (part.text) {\n          console.log(part.text);\n        } else if (part.inlineData) {\n          const imageData = part.inlineData.data;\n          const buffer = Buffer.from(imageData, \"base64\");\n          fs.writeFileSync(\"gemini-native-image.png\", buffer);\n          console.log(\"Image saved as gemini-native-image.png\");\n        }\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    IMG_PATH=/path/to/your/image1.jpeg\n    \n    if [[ \"$(base64 --version 2>&1)\" = *\"FreeBSD\"* ]]; then\n      B64FLAGS=\"--input\"\n    else\n      B64FLAGS=\"-w0\"\n    fi\n    \n    IMG_BASE64=$(base64 \"$B64FLAGS\" \"$IMG_PATH\" 2>&1)\n    \n    curl -X POST \\\n      \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY\" \\\n        -H 'Content-Type: application/json' \\\n        -d \"{\n          \\\"contents\\\": [{\n            \\\"parts\\\":[\n                {\\\"text\\\": \\\"'Hi, This is a picture of me. Can you add a llama next to me\\\"},\n                {\n                  \\\"inline_data\\\": {\n                    \\\"mime_type\\\":\\\"image/jpeg\\\",\n                    \\\"data\\\": \\\"$IMG_BASE64\\\"\n                  }\n                }\n            ]\n          }],\n          \\\"generationConfig\\\": {\\\"responseModalities\\\": [\\\"Text\\\", \\\"Image\\\"]}\n        }\"  \\\n      | grep -o '\"data\": \"[^\"]*\"' \\\n      | cut -d'\"' -f4 \\\n      | base64 --decode > gemini-edited-image.png\n    \n\n### Limitations\n\n  * For best performance, use the following languages: EN, es-MX, ja-JP, zh-CN, hi-IN.\n  * Image generation does not support audio or video inputs.\n  * Image generation may not always trigger: \n    * The model may output text only. Try asking for image outputs explicitly (e.g. \"generate an image\", \"provide images as you go along\", \"update the image\").\n    * The model may stop generating partway through. Try again or try a different prompt.\n  * When generating text for an image, Gemini works best if you first generate the text and then ask for an image with the text.\n\n\n\n## Choose a model\n\nWhich model should you use to generate images? It depends on your use case.\n\nGemini 2.0 is best for producing contextually relevant images, blending text + images, incorporating world knowledge, and reasoning about images. You can use it to create accurate, contextually relevant visuals embedded in long text sequences. You can also edit images conversationally, using natural language, while maintaining context throughout the conversation.\n\nIf image quality is your top priority, then Imagen 3 is a better choice. Imagen 3 excels at photorealism, artistic detail, and specific artistic styles like impressionism or anime. Imagen 3 is also a good choice for specialized image editing tasks like updating product backgrounds, upscaling images, and infusing branding and style into visuals. You can use Imagen 3 to create logos or other branded product designs.\n\n## Generate images using Imagen 3\n\nThe Gemini API provides access to [Imagen 3](https://deepmind.google/technologies/imagen-3/), Google's highest quality text-to-image model, featuring a number of new and improved capabilities. Imagen 3 can do the following:\n\n  * Generate images with better detail, richer lighting, and fewer distracting artifacts than previous models\n  * Understand prompts written in natural language\n  * Generate images in a wide range of formats and styles\n  * Render text more effectively than previous models\n\n**Note:** Imagen 3 is only available on the [Paid Tier](/gemini-api/docs/pricing) and always includes a [SynthID](https://deepmind.google/technologies/synthid/) watermark.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    from PIL import Image\n    from io import BytesIO\n    \n    client = genai.Client(api_key='GEMINI_API_KEY')\n    \n    response = client.models.generate_images(\n        model='imagen-3.0-generate-002',\n        prompt='Robot holding a red skateboard',\n        config=types.GenerateImagesConfig(\n            number_of_images= 4,\n        )\n    )\n    for generated_image in response.generated_images:\n      image = Image.open(BytesIO(generated_image.image.image_bytes))\n      image.show()\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    import * as fs from \"node:fs\";\n    \n    async function main() {\n    \n      const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n      const response = await ai.models.generateImages({\n        model: 'imagen-3.0-generate-002',\n        prompt: 'Robot holding a red skateboard',\n        config: {\n          numberOfImages: 4,\n        },\n      });\n    \n      let idx = 1;\n      for (const generatedImage of response.generatedImages) {\n        let imgBytes = generatedImage.image.imageBytes;\n        const buffer = Buffer.from(imgBytes, \"base64\");\n        fs.writeFileSync(`imagen-${idx}.png`, buffer);\n        idx++;\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl -X POST \\\n        \"https://generativelanguage.googleapis.com/v1beta/models/imagen-3.0-generate-002:predict?key=GEMINI_API_KEY\" \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\n            \"instances\": [\n              {\n                \"prompt\": \"Robot holding a red skateboard\"\n              }\n            ],\n            \"parameters\": {\n              \"sampleCount\": 4\n            }\n          }'\n    \n\nAI-generated image of two fuzzy bunnies in the kitchen\n\nImagen supports English only prompts at this time and the following parameters:\n\n### Imagen model parameters\n\n(Naming conventions vary by programming language.)\n\n  * `numberOfImages`: The number of images to generate, from 1 to 4 (inclusive). The default is 4.\n  * `aspectRatio`: Changes the aspect ratio of the generated image. Supported values are `\"1:1\"`, `\"3:4\"`, `\"4:3\"`, `\"9:16\"`, and `\"16:9\"`. The default is `\"1:1\"`.\n  * `personGeneration`: Allow the model to generate images of people. The following values are supported: \n    * `\"DONT_ALLOW\"`: Block generation of images of people.\n    * `\"ALLOW_ADULT\"`: Generate images of adults, but not children. This is the default.\n\n\n\n## What's next\n\n  * To learn more about prompt writing for Imagen, see the [Imagen prompt guide](/gemini-api/docs/imagen-prompt-guide).\n  * To learn more about Gemini 2.0 models, see [Gemini models](/gemini-api/docs/models/gemini) and [Experimental models](/gemini-api/docs/models/experimental-models).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/video",
    "title": "Generate video using Veo  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Generate video using Veo \n\nThe Gemini API provides access to [Veo 2](https://deepmind.google/technologies/veo/), Google's state-of-the-art video generation model. Veo is designed to help you build next-generation AI applications that transform user prompts and images into high quality video assets.\n\nThis guide will help you get started with Veo using the Gemini API.\n\n## About Veo\n\n**Note:** Veo is a **paid feature** and will not run in the Free tier. Visit the [Pricing](/gemini-api/docs/pricing#veo-2) page for more details.\n\nVeo is Google's most capable video generation model to date. It generates videos in a wide range of cinematic and visual styles, capturing prompt nuance to render intricate details consistently across frames.\n\nTo learn more and see example output, check out the [Google DeepMind Veo overview](https://deepmind.google/technologies/veo/).\n\n### Specifications\n\n**Modalities** | \n\n  * Text-to-video generation\n  * Image-to-video generation\n\n  \n---|---  \n**Request latency** | \n\n  * Min: 11 seconds\n  * Max: 6 minutes (during peak hours)\n\n  \n**Variable length generation** | 5-8 seconds  \n**Resolution** | 720p  \n**Frame rate** | 24fps  \n**Aspect ratio** | \n\n  * 16:9 - landscape\n  * 9:16 - portrait\n\n  \n**Input languages (text-to-video)** | English  \n**Note:** Check out the [Models](/gemini-api/docs/models#veo-2), [Pricing](/gemini-api/docs/pricing#veo-2), and [Rate limits](/gemini-api/docs/rate-limits) pages for more usage limitations for Veo.\n\nVideos created by Veo are watermarked using [SynthID](https://deepmind.google/technologies/synthid/), our tool for watermarking and identifying AI-generated content, and are passed through safety filters and memorization checking processes that help mitigate privacy, copyright and bias risks.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\nTo use Veo with the Google Gen AI SDKs, ensure that you have one of the following versions installed:\n\n  * [Python](https://pypi.org/project/google-genai/) v1.10.0 or later\n  * [TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai) v0.8.0 or later\n  * [Go](https://pkg.go.dev/google.golang.org/genai) v1.0.0 or later\n\n\n\n## Generate videos\n\nThis section provides code examples for generating videos using text prompts and using images.\n\n### Generate from text\n\nYou can use the following code to generate videos with Veo:\n\n### Python\n    \n    \n    import time\n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client()  # read API key from GOOGLE_API_KEY\n    \n    operation = client.models.generate_videos(\n        model=\"veo-2.0-generate-001\",\n        prompt=\"Panning wide shot of a calico kitten sleeping in the sunshine\",\n        config=types.GenerateVideosConfig(\n            person_generation=\"dont_allow\",  # \"dont_allow\" or \"allow_adult\"\n            aspect_ratio=\"16:9\",  # \"16:9\" or \"9:16\"\n        ),\n    )\n    \n    while not operation.done:\n        time.sleep(20)\n        operation = client.operations.get(operation)\n    \n    for n, generated_video in enumerate(operation.response.generated_videos):\n        client.files.download(file=generated_video.video)\n        generated_video.video.save(f\"video{n}.mp4\")  # save the video\n    \n    \n\n### REST\n    \n    \n    # Use curl to send a POST request to the predictLongRunning endpoint\n    # The request body includes the prompt for video generation\n    curl \"${BASE_URL}/models/veo-2.0-generate-001:predictLongRunning?key=${GOOGLE_API_KEY}\" \\\n      -H \"Content-Type: application/json\" \\\n      -X \"POST\" \\\n      -d '{\n        \"instances\": [{\n            \"prompt\": \"Panning wide shot of a calico kitten sleeping in the sunshine\"\n          }\n        ],\n        \"parameters\": {\n          \"aspectRatio\": \"16:9\",\n          \"personGeneration\": \"dont_allow\",\n        }\n      }' | tee result.json | jq .name | sed 's/\"//g' > op_name\n    \n\nThis code takes about 2-3 minutes to run, though it may take longer if resources are constrained. Once it's done running, you should see a video that looks something like this:\n\nIf you see an error message instead of a video, this means that resources are constrained and your request couldn't be completed. In this case, run the code again.\n\nGenerated videos are stored on the server for 2 days, after which they are removed. If you want to save a local copy of your generated video, you must run `result()` and `save()` within 2 days of generation.\n\n### Generate from images\n\nYou can also generate videos using images. The following code generates an image using Imagen, then uses the generated image as the starting frame for the generated video.\n\nFirst, generate an image using [Imagen](/gemini-api/docs/image-generation#imagen):\n\n### Python\n    \n    \n    prompt=\"Panning wide shot of a calico kitten sleeping in the sunshine\",\n    \n    imagen = client.models.generate_images(\n        model=\"imagen-3.0-generate-002\",\n        prompt=prompt,\n        config=types.GenerateImagesConfig(\n          aspect_ratio=\"16:9\",\n          number_of_images=1\n        )\n    )\n    \n    images[0]\n    \n\nThen, generate a video using the resulting image as the first frame:\n\n### Python\n    \n    \n    operation = client.models.generate_videos(\n        model=\"veo-2.0-generate-001\",\n        prompt=prompt,\n        image = images[0],\n        config=types.GenerateVideosConfig(\n          # person_generation only accepts \"dont_allow\" for image-to-video\n          aspect_ratio=\"16:9\",  # \"16:9\" or \"9:16\"\n          number_of_videos=2\n        ),\n    )\n    \n    # Wait for videos to generate\n     while not operation.done:\n      time.sleep(20)\n      operation = client.operations.get(operation)\n    \n    for n, video in enumerate(operation.response.generated_videos):\n        fname = f'with_image_input{n}.mp4'\n        print(fname)\n        video.save(fname)\n    \n\n## Veo model parameters\n\n(Naming conventions vary by programming language.)\n\n  * `prompt`: The text prompt for the video. When present, the `image` parameter is optional.\n  * `image`: The image to use as the first frame for the video. When present, the `prompt` parameter is optional.\n  * `negativePrompt`: Text string that describes anything you want to _discourage_ the model from generating\n  * `aspectRatio`: Changes the aspect ratio of the generated video. Supported values are `\"16:9\"` and `\"9:16\"`. The default is `\"16:9\"`.\n  * `personGeneration`: Allow the model to generate videos of people. The following values are supported: \n    * Text-to-video generation: \n      * `\"dont_allow\"`: Don't allow the inclusion of people or faces.\n      * `\"allow_adult\"`: Generate videos that include adults, but not children.\n    * Image-to-video generation: \n      * `\"dont_allow\"`: Default and only value for image-to-video generation.\n  * `numberOfVideos`: Output videos requested, either `1` or `2`.\n  * `durationSeconds`: Length of each output video in seconds, between `5` and `8`.\n  * `enhance_prompt`: Enable or disable the prompt rewriter. Enabled by default.\n\n\n\n## Things to try\n\nTo get the most out of Veo, incorporate video-specific terminology into your prompts. Veo understands a wide range of terms related to:\n\n  * **Shot composition:** Specify the framing and number of subjects in the shot (e.g., \"single shot,\" \"two shot,\" \"over-the-shoulder shot\").\n  * **Camera positioning and movement:** Control the camera's location and movement using terms like \"eye level,\" \"high angle,\" \"worms eye,\" \"dolly shot,\" \"zoom shot,\" \"pan shot,\" and \"tracking shot.\"\n  * **Focus and lens effects:** Use terms like \"shallow focus,\" \"deep focus,\" \"soft focus,\" \"macro lens,\" and \"wide-angle lens\" to achieve specific visual effects.\n  * **Overall style and subject:** Guide Veo's creative direction by specifying styles like \"sci-fi,\" \"romantic comedy,\" \"action movie,\" or \"animation.\" You can also describe the subjects and backgrounds you want, such as \"cityscape,\" \"nature,\" \"vehicles,\" or \"animals.\"\n\n\n\n## Veo prompt guide\n\nThis section of the Veo guide contains examples of videos you can create using Veo, and shows you how to modify prompts to produce distinct results.\n\n### Safety filters\n\nVeo applies safety filters across Gemini to help ensure that generated videos and uploaded photos don't contain offensive content. Prompts that violate our [terms and guidelines](/gemini-api/docs/usage-policies#abuse-monitoring) are blocked.\n\n### Prompt writing basics\n\nGood prompts are descriptive and clear. To get your generated video as close as possible to what you want, start with identifying your core idea, and then refine your idea by adding keywords and modifiers.\n\nThe following elements should be included in your prompt:\n\n  * **Subject** : The object, person, animal, or scenery that you want in your video.\n  * **Context** : The background or context in which the subject is placed.\n  * **Action** : What the subject is doing (for example, _walking_ , _running_ , or _turning their head_).\n  * **Style** : This can be general or very specific. Consider using specific film style keywords, such as _horror film_ , _film noir_ , or animated styles like _cartoon_ style.\n  * **Camera motion** : [Optional] What the camera is doing, such as _aerial view_ , _eye-level_ , _top-down shot_ , or _low-angle shot_.\n  * **Composition** : [Optional] How the shot is framed, such as _wide shot_ , _close-up_ , or _extreme close-up_.\n  * **Ambiance** : [Optional] How the color and light contribute to the scene, such as _blue tones_ , _night_ , or _warm tones_.\n\n\n\n#### More tips for writing prompts\n\nThe following tips help you write prompts that generate your videos:\n\n  * **Use descriptive language** : Use adjectives and adverbs to paint a clear picture for Veo.\n  * **Provide context** : If necessary, include background information to help your model understand what you want.\n  * **Reference specific artistic styles** : If you have a particular aesthetic in mind, reference specific artistic styles or art movements.\n  * **Utilize prompt engineering tools** : Consider exploring prompt engineering tools or resources to help you refine your prompts and achieve optimal results. For more information, visit [Introduction to prompt design](/gemini-api/docs/prompting-intro).\n  * **Enhance the facial details in your personal and group images** : Specify facial details as a focus of the photo like using the word _portrait_ in the prompt.\n\n\n\n### Example prompts and output\n\nThis section presents several prompts, highlighting how descriptive details can elevate the outcome of each video.\n\n#### Icicles\n\nThis video demonstrates how you can use the elements of prompt writing basics in your prompt.\n\n**Prompt** | **Generated output**  \n---|---  \nClose up shot (composition) of melting icicles (subject) on a frozen rock wall (context) with cool blue tones (ambiance), zoomed in (camera motion) maintaining close-up detail of water drips (action).  |   \n  \n#### Man on the phone\n\nThese videos demonstrate how you can revise your prompt with increasingly specific details to get Veo to refine the output to your liking.\n\n**Prompt** | **Generated output** | **Analysis**  \n---|---|---  \nThe camera dollies to show a close up of a desperate man in a green trench coat. He's making a call on a rotary-style wall phone with a green neon light. It looks like a movie scene. |  | This is the first generated video based on the prompt.  \nA close-up cinematic shot follows a desperate man in a weathered green trench coat as he dials a rotary phone mounted on a gritty brick wall, bathed in the eerie glow of a green neon sign. The camera dollies in, revealing the tension in his jaw and the desperation etched on his face as he struggles to make the call. The shallow depth of field focuses on his furrowed brow and the black rotary phone, blurring the background into a sea of neon colors and indistinct shadows, creating a sense of urgency and isolation. |  | A more detailed prompt results in a video that is more focused with a richer environment.  \nA video with smooth motion that dollies in on a desperate man in a green trench coat, using a vintage rotary phone against a wall bathed in an eerie green neon glow. The camera starts from a medium distance, slowly moving closer to the man's face, revealing his frantic expression and the sweat on his brow as he urgently dials the phone. The focus is on the man's hands, his fingers fumbling with the dial as he desperately tries to connect. The green neon light casts long shadows on the wall, adding to the tense atmosphere. The scene is framed to emphasize the isolation and desperation of the man, highlighting the stark contrast between the vibrant glow of the neon and the man's grim determination. |  | Adding more detail gives the subject a realistic expression and creates an intense and vibrant scene.  \n  \n#### Snow leopard\n\nThis example demonstrates the output Veo might generate for a simple prompt.\n\n**Prompt** | **Generated output**  \n---|---  \nA cute creature with snow leopard-like fur is walking in winter forest, 3D cartoon style render. |   \n  \n#### Running snow leopard\n\nThis prompt has more detail and demonstrates generated output that might be closer to what you want in your video.\n\n**Prompt** | **Generated output**  \n---|---  \nCreate a short 3D animated scene in a joyful cartoon style. A cute creature with snow leopard-like fur, large expressive eyes, and a friendly, rounded form happily prances through a whimsical winter forest. The scene should feature rounded, snow-covered trees, gentle falling snowflakes, and warm sunlight filtering through the branches. The creature's bouncy movements and wide smile should convey pure delight. Aim for an upbeat, heartwarming tone with bright, cheerful colors and playful animation. |   \n  \n### Examples by writing elements\n\nThese examples show you how to refine your prompts by each basic element.\n\n#### Subject\n\nThis example shows you how to specify a subject description.\n\n**Subject description** | **Prompt** | **Generated output**  \n---|---|---  \nThe description can include a subject, or multiple subjects and actions. Here, our subject is \"white concrete apartment building.\" | An architectural rendering of a white concrete apartment building with flowing organic shapes, seamlessly blending with lush greenery and futuristic elements |   \n  \n#### Context\n\nThis example shows you how to specify context.\n\n**Context** | **Prompt** | **Generated output**  \n---|---|---  \nThe background or context in which the subject will be placed is very important. Try placing your subject in a variety of backgrounds like on a busy street, or in outer space. | A satellite floating through outer space with the moon and some stars in the background. |   \n  \n#### Action\n\nThis example shows you how to specify action.\n\n**Action** | **Prompt** | **Generated output**  \n---|---|---  \nWhat is the subject doing like walking, running, or turning their head. | A wide shot of a woman walking along the beach, looking content and relaxed towards the horizon at sunset. |   \n  \n#### Style\n\nThis example shows you how to specify style.\n\n**Style** | **Prompt** | **Generated output**  \n---|---|---  \nYou can add keywords to improve generation quality and steer it closer to intended style, such as shallow depth of field, movie still, minimalistic, surreal, vintage, futuristic, or double-exposure. | Film noir style, man and woman walk on the street, mystery, cinematic, black and white. |   \n  \n#### Camera motion\n\nThis example shows you how to specify camera motion.\n\n**Camera motion** | **Prompt** | **Generated output**  \n---|---|---  \nOptions for camera motion include POV shot, aerial view, tracking drone view, or tracking shot. | A POV shot from a vintage car driving in the rain, Canada at night, cinematic. |   \n  \n#### Composition\n\nThis example shows you how to specify composition.\n\n**Composition** | **Prompt** | **Generated output**  \n---|---|---  \nHow the shot is framed (wide shot, close-up, low angle). | Extreme close-up of a an eye with city reflected in it. |   \n| Create a video of a wide shot of surfer walking on a beach with a surfboard, beautiful sunset, cinematic. |   \n  \n#### Ambiance\n\nThis example shows you how to specify ambiance.\n\n**Ambiance** | **Prompt** | **Generated output**  \n---|---|---  \nColor palettes play a vital role in photography, influencing the mood and conveying intended emotions. Try things like \"muted orange warm tones,\" \"natural light,\" \"sunrise\" or \"sunset\". For example, a warm, golden palette can infuse a romantic and atmospheric feel into a photograph. | A close-up of a girl holding adorable golden retriever puppy in the park, sunlight. |   \n| Cinematic close-up shot of a sad woman riding a bus in the rain, cool blue tones, sad mood. |   \n  \n### Use reference images to generate videos\n\nYou can bring images to life by using Veo's [image-to-video](/gemini-api/docs/video#generate-from-images) capability. You can use existing assets, or try [Imagen](/gemini-api/docs/image-generation) to generate something new.\n\n**Prompt** | **Generated output**  \n---|---  \nBunny with a chocolate candy bar. |   \nBunny runs away. |   \n  \n### Negative prompts\n\nNegative prompts can be a powerful tool to help specify elements you _don't_ want in the video. Describe what you want to discourage the model from generating after the phrase \"Negative prompt\". Follow these tips:\n\n  * ❌ Don't use instructive language or words like _no_ or _don't_. For example, \"No walls\" or \"don't show walls\".\n\n  * ✅ Do describe what you don't want to see. For example, \"wall, frame\", which means that you don't want a wall or a frame in the video.\n\n\n**Prompt** | **Generated output**  \n---|---  \nGenerate a short, stylized animation of a large, solitary oak tree with leaves blowing vigorously in a strong wind. The tree should have a slightly exaggerated, whimsical form, with dynamic, flowing branches. The leaves should display a variety of autumn colors, swirling and dancing in the wind. The animation should use a warm, inviting color palette. |   \nGenerate a short, stylized animation of a large, solitary oak tree with leaves blowing vigorously in a strong wind. The tree should have a slightly exaggerated, whimsical form, with dynamic, flowing branches. The leaves should display a variety of autumn colors, swirling and dancing in the wind. The animation should use a warm, inviting color palette.   \n  \nWith negative prompt - urban background, man-made structures, dark, stormy, or threatening atmosphere. |   \n  \n### Aspect ratios\n\nGemini Veo video generation supports the following two aspect ratios:\n\n**Aspect ratio** | **Description**  \n---|---  \nWidescreen or 16:9 | The most common aspect ratio for televisions, monitors, and mobile phone screens (landscape). Use this when you want to capture more of the background, like in scenic landscapes.  \nPortrait or 9:16 | Rotated widescreen. This aspect ratio has been popularized by short form video applications, such as Youtube shorts. Use this for portraits or tall objects with strong vertical orientations, such as buildings, trees, waterfall, or buildings.  \n  \n  \n#### Widescreen\n\nThis prompt is an example of the widescreen aspect ratio of 16:9.\n\n**Prompt** | **Generated output**  \n---|---  \nCreate a video with a tracking drone view of a man driving a red convertible car in Palm Springs, 1970s, warm sunlight, long shadows. |   \n  \n#### Portrait\n\nThis prompt is an example of the portrait aspect ratio of 9:16.\n\n**Prompt** | **Generated output**  \n---|---  \nCreate a video highlighting the smooth motion of a majestic Hawaiian waterfall within a lush rainforest. Focus on realistic water flow, detailed foliage, and natural lighting to convey tranquility. Capture the rushing water, misty atmosphere, and dappled sunlight filtering through the dense canopy. Use smooth, cinematic camera movements to showcase the waterfall and its surroundings. Aim for a peaceful, realistic tone, transporting the viewer to the serene beauty of the Hawaiian rainforest. |   \n  \n## What's next\n\n  * Gain more experience generating AI videos with the [Veo Colab](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_Veo.ipynb).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-09 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-09 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/gemini-for-research",
    "title": "Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\n# Accelerate discovery with Gemini for Research\n\n[Get a Gemini API Key](https://aistudio.google.com/apikey)\n\nGemini models can be used to advance foundational research across disciplines. Here are ways that you can explore Gemini for your research:\n\n  * **Fine-tuning** : You can fine-tune Gemini models for a variety of modalities to advance your research. [Learn more](/gemini-api/docs/model-tuning/tutorial).\n  * **Analyze and control model outputs** : For further analysis, you can examine a response candidate generated by the model using tools like `Logprobs` and `CitationMetadata`. You can also configure options for model generation and outputs, such as `responseSchema`, `topP`, and `topK`. [Learn more](/api/generate-content).\n  * **Multimodal inputs** : Gemini can process images, audio, and videos, enabling a multitude of exciting research directions. [Learn more](/gemini-api/docs/vision).\n  * **Long-context capabilities** : Gemini 1.5 Flash comes with a 1-million-token context window, and Gemini 1.5 Pro comes with a 2-million-token context window. [Learn more](/gemini-api/docs/long-context).\n  * **Grow with Google** : Quickly access Gemini models through the API and Google AI Studio for production use cases. If you're looking for a Google Cloud-based platform, Vertex AI can provide additional supporting infrastructure.\n\n\n\nTo support academic research and drive cutting-edge research, Google provides access to Gemini API credits for scientists and academic researchers through the [Gemini Academic Program](/gemini-api/docs/gemini-for-research#gemini-academic-program).\n\n## Get started with Gemini\n\nThe Gemini API and Google AI Studio help you start working with Google's latest models and turn your ideas into applications that scale.\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"YOUR_API_KEY\")\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=\"How large is the universe?\",\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"YOUR_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"How large is the universe?\",\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=YOUR_API_KEY\" \\\n    -H 'Content-Type: application/json' \\\n    -X POST \\\n    -d '{\n      \"contents\": [{\n        \"parts\":[{\"text\": \"How large is the universe?\"}]\n        }]\n       }'\n    \n\n## Featured academics\n\n\"Our research investigates Gemini as a visual language model (VLM) and its agentic behaviors in diverse environments from robustness and safety perspectives. So far, we have evaluated Gemini's robustness against distractions such as pop-up windows when VLM agents perform computer tasks, and have leveraged Gemini to analyze social interaction, temporal events as well as risk factors based on video input.\"\n\n[](https://cs.stanford.edu/~diyiy/)\n\n\"Gemini Pro and Flash, with their long context window, have been helping us in OK-Robot, our open-vocabulary mobile manipulation project. Gemini enables complex natural language queries and commands over the robot's \"memory\": in this case, previous observations made by the robot over a long operation duration. Mahi Shafiullah and I are also using Gemini to decompose tasks into code that the robot can execute in the real world.\"\n\n[](https://www.lerrelpinto.com/)\n\n## Gemini Academic Program\n\nQualified academic researchers (such as faculty, staff, and PhD students) in [supported countries](/gemini-api/docs/available-regions) can apply to receive Gemini API credits and higher rate limits for research projects. This support enables higher throughput for scientific experiments and advances research.\n\nWe are particularly interested in the research areas in the following section, but we welcome applications from diverse scientific disciplines:\n\n  * **Evaluations and benchmarks** : Community-endorsed evaluation methods that can provide a strong performance signal in areas such as factuality, safety, instruction following, reasoning, and planning.\n\n  * **Accelerating scientific discovery to benefit humanity** : Potential applications of AI in interdisciplinary scientific research, including areas such as rare and neglected diseases, experimental biology, materials science, and sustainability.\n\n  * **Embodiment and interactions** : Utilizing large language models to investigate novel interactions within the fields of embodied AI, ambient interactions, robotics, and human-computer interaction.\n\n  * **Emergent capabilities** : Exploring new agentic capabilities required to enhance reasoning and planning, and how capabilities can be expanded during inference (e.g., by utilizing Gemini Flash).\n\n  * **Multimodal interaction and understanding** : Identifying gaps and opportunities for multimodal foundational models for analysis, reasoning, and planning across a variety of tasks.\n\n\n\n\nEligibility: Only individuals (faculty members, researchers or equivalent) affiliated with a valid academic institution, or academic research organization can apply. Note that API access and credits will be granted and removed at Google's discretion. We review applications on a monthly basis.\n\n### Start researching with the Gemini API\n\n[Apply now](https://forms.gle/HMviQstU8PxC5iCt5)\n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-01 UTC.\n\n[[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-01 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/quickstart",
    "title": "Gemini API quickstart  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini API quickstart \n\nThis quickstart shows you how to install your SDK of choice and then make your first Gemini API request.\n\nPython JavaScript REST Go Apps Script\n\n## Install the Gemini API library\n\n**Note:** We're rolling out a new set of Gemini API libraries, the [Google Gen AI SDK](/gemini-api/docs/sdks).\n\n## Make your first request\n\n[Get a Gemini API key in Google AI Studio](https://aistudio.google.com/app/apikey)\n\nUse the [`generateContent`](/api/generate-content#method:-models.generatecontent) method to send a request to the Gemini API.\n\n## What's next\n\nNow that you made your first API request, you might want to explore the following guides that show Gemini in action:\n\n  * [Text generation](/gemini-api/docs/text-generation)\n  * [Vision](/gemini-api/docs/vision)\n  * [Long context](/gemini-api/docs/long-context)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/api-key",
    "title": "Get a Gemini API key  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Get a Gemini API key \n\nTo use the Gemini API, you need an API key. You can create a key with a few clicks in Google AI Studio.\n\n[Get a Gemini API key in Google AI Studio](https://aistudio.google.com/app/apikey)\n\n## Set up your API key\n\nFor initial testing, you can hard code an API key, but this should only be temporary since it is not secure. The rest of this section goes through how to set up your API key locally as an environment variable with different operating systems.\n\n### Linux/macOS - Bash\n\nBash is a common Linux and macOS terminal configuration. You can check if you have a configuration file for it by running the following command:\n    \n    \n    ~/.bashrc\n\nIf the response is \"No such file or directory\", you will need to create this file and open it by running the following commands, or use `zsh`:\n    \n    \n    touch ~/.bashrc\n    open ~/.bashrc\n\nNext, you need to set your API key by adding the following export command:\n    \n    \n    export GEMINI_API_KEY=<YOUR_API_KEY_HERE>\n\nAfter saving the file, apply the changes by running:\n    \n    \n    source ~/.bashrc\n\n### macOS - Zsh\n\nZsh is a common Linux and macOS terminal configuration. You can check if you have a configuration file for it by running the following command:\n    \n    \n    ~/.zshrc\n\nIf the response is \"No such file or directory\", you will need to create this file and open it by running the following commands, or use `bash`:\n    \n    \n    touch ~/.zshrc\n    open ~/.zshrc\n\nNext, you need to set your API key by adding the following export command:\n    \n    \n    export GEMINI_API_KEY=<YOUR_API_KEY_HERE>\n\nAfter saving the file, apply the changes by running:\n    \n    \n    source ~/.zshrc\n\n### Windows\n\n  1. Search for \"Environment Variables\" in the system settings\n  2. Edit either \"User variables\" (for current user) or \"System variables\" (for all users - use with caution).\n  3. Create the variable and add `export GEMINI_API_KEY=your_key_here`\n  4. Apply the changes\n\n\n\n## Send your first Gemini API request\n\nYou can use a curl command to verify your setup:\n    \n    \n      curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${GEMINI_API_KEY}\" \\\n        -H 'Content-Type: application/json' \\\n        -X POST \\\n        -d '{\n          \"contents\": [{\n            \"parts\":[{\"text\": \"Write a story about a magic backpack.\"}]\n            }]\n           }'\n\n## Keep your API key secure\n\nIt's important to keep your Gemini API key secure. Here are a few things to keep in mind when using your Gemini API key:\n\n  * The Google AI Gemini API uses API keys for authorization. If others get access to your Gemini API key, they can make calls using your project's quota, which could result in lost quota or additional charges for billed projects, in addition to accessing tuned models and files.\n\n  * Adding [API key restrictions](https://cloud.google.com/api-keys/docs/add-restrictions-api-keys#add-api-restrictions) can help limit the surface area usable through each API key.\n\n  * You're responsible for keeping your Gemini API key secure.\n\n    * Do NOT check Gemini API keys into source control.\n    * Client-side applications (Android, Swift, web, and Dart/Flutter) risk exposing API keys. We don't recommend using the Google AI client SDKs in production apps to call the Google AI Gemini API directly from your mobile and web apps.\n\n\n\nFor some general best practices, you can also review this [support article](https://support.google.com/googleapi/answer/6310037).\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-03-17 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-03-17 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/libraries",
    "title": "Gemini API Libraries  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini API Libraries \n\nThis page provides information about how to download or install the libraries for accessing the Gemini API. If you're new to the Gemini API, try out the [API quickstart](/gemini-api/docs/quickstart).\n\n## Python\n\nWe provide a [Python SDK](https://pypi.org/project/google-genai) which you can install by running:\n    \n    \n    pip install google-genai\n    \n\n## JavaScript and TypeScript (Preview)\n\nWe provide a [JavaScript and TypeScript SDK](https://www.npmjs.com/package/@google/genai) which you can install by running:\n    \n    \n    npm install @google/genai\n    \n\n## Go (Preview)\n\nWe provide a [Go SDK](https://pkg.go.dev/google.golang.org/genai) which you can install by running:\n    \n    \n    go get https://github.com/googleapis/go-genai\n    \n\n## Previous SDKs\n\nThe following is a set of our previous SDK's which are no longer being actively developed:\n\n  * [Previous Dart and Flutter SDK](https://pub.dev/packages/google_generative_ai/install)\n  * [Previous Swift SDK](https://github.com/google/generative-ai-swift)\n  * [Previous Android SDK](https://github.com/google-gemini/generative-ai-android)\n  * [Previous Python SDK](https://github.com/google-gemini/deprecated-generative-ai-python)\n  * [Previous Node.js SDK](https://github.com/google-gemini/generative-ai-js)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-12 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-12 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/migrate",
    "title": "Upgrade to the Google Gen AI SDKs  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Upgrade to the Google Gen AI SDKs \n\nWhen we introduced the Gemini 2.0 model family, we also released a new set of Google Gen AI SDKs for working with the Gemini API:\n\n  * [Python](https://pypi.org/project/google-genai/)\n  * [TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai)\n  * [Go](https://pkg.go.dev/google.golang.org/genai)\n\n\n\nThese updated SDKs are will be fully compatible with all Gemini API models and features, including recent additions like the [Live API](/gemini-api/docs/live) and [Veo](/gemini-api/docs/video).\n\nWe recommend that you start migrating your projects from the old Gemini SDKs to the new Gen AI SDKs. This guide provides before-and-after examples of migrated code to help you get started. We'll continue to add examples here to help you get up and running with the new SDKs.\n\n**Note:** The new JavaScript SDK supports both Node.js and browser usage. If you want to run code in the browser, see JavaScript in the browser.\n\n## Install the SDK\n\n**Before**\n\n### Python\n    \n    \n    pip install -U -q \"google-generativeai\"\n    \n\n### JavaScript\n    \n    \n    npm install @google/generative-ai\n    \n\n**After**\n\n### Python\n    \n    \n    pip install -U -q \"google-genai\"\n    \n\n### JavaScript\n    \n    \n    npm install @google/genai\n    \n\n## Authenticate\n\nAuthenticate using an API key. You can [create](https://aistudio.google.com/app/apikey) your API key in Google AI Studio.\n\n**Before**\n\n### Python\n\nThe old SDK handled the API client object implicitly. In the new SDK you create the API client and use it to call the API. Remember, in either case the SDK will pick up your API key from the `GOOGLE_API_KEY` environment variable if you don't pass one to the client.\n    \n    \n    import google.generativeai as genai\n    \n    genai.configure(api_key=...)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenerativeAI } from \"@google/generative-ai\";\n    \n    const genAI = new GoogleGenerativeAI(\"GOOGLE_API_KEY\");\n    \n\n**After**\n\n### Python\n    \n    \n    export GOOGLE_API_KEY=\"YOUR_API_KEY\"\n    \n    \n    \n    from google import genai\n    \n    client = genai.Client() # Set the API key using the GOOGLE_API_KEY env var.\n                            # Alternatively, you could set the API key explicitly:\n                            # client = genai.Client(api_key=\"your_api_key\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({apiKey: \"GEMINI_API_KEY\"});\n    \n\n## Generate content\n\n**Before**\n\n### Python\n\nThe new SDK provides access to all the API methods through the `Client` object. Except for a few stateful special cases (`chat` and live-api `session`s), these are all stateless functions. For utility and uniformity, objects returned are `pydantic` classes.\n    \n    \n    import google.generativeai as genai\n    \n    model = genai.GenerativeModel('gemini-1.5-flash')\n    response = model.generate_content(\n        'Tell me a story in 300 words'\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenerativeAI } from \"@google/generative-ai\";\n    \n    const genAI = new GoogleGenerativeAI(process.env.API_KEY);\n    const model = genAI.getGenerativeModel({ model: \"gemini-1.5-flash\" });\n    const prompt = \"Tell me a story in 300 words\";\n    \n    const result = await model.generateContent(prompt);\n    console.log(result.response.text());\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    client = genai.Client()\n    \n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        contents='Tell me a story in 300 words.'\n    )\n    print(response.text)\n    \n    print(response.model_dump_json(\n        exclude_none=True, indent=4))\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    \n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: \"Tell me a story in 300 words.\",\n    });\n    console.log(response.text);\n    \n\n**Before**\n\n### Python\n\nMany of the same convenience features exist in the new SDK. For example, `PIL.Image` objects are automatically converted.\n    \n    \n    import google.generativeai as genai\n    \n    model = genai.GenerativeModel('gemini-1.5-flash')\n    response = model.generate_content([\n        'Tell me a story based on this image',\n        Image.open(image_path)\n    ])\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenerativeAI } from \"@google/generative-ai\";\n    \n    const genAI = new GoogleGenerativeAI(\"GOOGLE_API_KEY\");\n    const model = genAI.getGenerativeModel({ model: \"gemini-1.5-flash\" });\n    \n    function fileToGenerativePart(path, mimeType) {\n      return {\n        inlineData: {\n          data: Buffer.from(fs.readFileSync(path)).toString(\"base64\"),\n          mimeType,\n        },\n      };\n    }\n    \n    const prompt = \"Tell me a story based on this image\";\n    \n    const imagePart = fileToGenerativePart(\n      `path/to/organ.jpg`,\n      \"image/jpeg\",\n    );\n    \n    const result = await model.generateContent([prompt, imagePart]);\n    console.log(result.response.text());\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    from PIL import Image\n    \n    client = genai.Client()\n    \n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        contents=[\n            'Tell me a story based on this image',\n            Image.open(image_path)\n        ]\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import {GoogleGenAI} from '@google/genai';\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    \n    const organ = await ai.files.upload({\n      file: \"path/to/organ.jpg\",\n    });\n    \n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: [\n        createUserContent([\n          \"Tell me a story based on this image\",\n          createPartFromUri(organ.uri, organ.mimeType)\n        ]),\n      ],\n    });\n    console.log(response.text);\n    \n\n### Streaming\n\n**Before**\n\n### Python\n    \n    \n    import google.generativeai as genai\n    \n    response = model.generate_content(\n        \"Write a cute story about cats.\",\n        stream=True)\n    for chunk in response:\n        print(chunk.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenerativeAI } from \"@google/generative-ai\";\n    \n    const genAI = new GoogleGenerativeAI(\"GOOGLE_API_KEY\");\n    const model = genAI.getGenerativeModel({ model: \"gemini-1.5-flash\" });\n    \n    const prompt = \"Write a story about a magic backpack.\";\n    \n    const result = await model.generateContentStream(prompt);\n    \n    // Print text as it comes in.\n    for await (const chunk of result.stream) {\n      const chunkText = chunk.text();\n      process.stdout.write(chunkText);\n    }\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client()\n    \n    for chunk in client.models.generate_content_stream(\n      model='gemini-2.0-flash',\n      contents='Tell me a story in 300 words.'\n    ):\n        print(chunk.text)\n    \n\n### JavaScript\n    \n    \n    import {GoogleGenAI} from '@google/genai';\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    \n    const response = await ai.models.generateContentStream({\n      model: \"gemini-2.0-flash\",\n      contents: \"Write a story about a magic backpack.\",\n    });\n    let text = \"\";\n    for await (const chunk of response) {\n      console.log(chunk.text);\n      text += chunk.text;\n    }\n    \n\n## Optional arguments\n\n**Before**\n\n### Python\n\nFor all methods in the new SDK, the required arguments are provided as keyword arguments. All optional inputs are provided in the `config` argument. Config arguments can be specified as either Python dictionaries or `Config` classes in the `google.genai.types` namespace. For utility and uniformity, all definitions within the `types` module are `pydantic` classes.\n    \n    \n    import google.generativeai as genai\n    \n    model = genai.GenerativeModel(\n      'gemini-1.5-flash',\n        system_instruction='you are a story teller for kids under 5 years old',\n        generation_config=genai.GenerationConfig(\n          max_output_tokens=400,\n          top_k=2,\n          top_p=0.5,\n          temperature=0.5,\n          response_mime_type='application/json',\n          stop_sequences=['\\n'],\n        )\n    )\n    response = model.generate_content('tell me a story in 100 words')\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenerativeAI } from \"@google/generative-ai\";\n    \n    const genAI = new GoogleGenerativeAI(\"GOOGLE_API_KEY\");\n    const model = genAI.getGenerativeModel({\n      model: \"gemini-1.5-flash\",\n      generationConfig: {\n        candidateCount: 1,\n        stopSequences: [\"x\"],\n        maxOutputTokens: 20,\n        temperature: 1.0,\n      },\n    });\n    \n    const result = await model.generateContent(\n      \"Tell me a story about a magic backpack.\",\n    );\n    console.log(result.response.text())\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client()\n    \n    response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      contents='Tell me a story in 100 words.',\n      config=types.GenerateContentConfig(\n          system_instruction='you are a story teller for kids under 5 years old',\n          max_output_tokens= 400,\n          top_k= 2,\n          top_p= 0.5,\n          temperature= 0.5,\n          response_mime_type= 'application/json',\n          stop_sequences= ['\\n'],\n          seed=42,\n      ),\n    )\n    \n\n### JavaScript\n    \n    \n    import {GoogleGenAI} from '@google/genai';\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    \n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: \"Tell me a story about a magic backpack.\",\n      config: {\n        candidateCount: 1,\n        stopSequences: [\"x\"],\n        maxOutputTokens: 20,\n        temperature: 1.0,\n      },\n    });\n    \n    console.log(response.text);\n    \n\n## Safety settings\n\nGenerate a response with safety settings:\n\n**Before**\n\n### Python\n    \n    \n    import google.generativeai as genai\n    \n    model = genai.GenerativeModel('gemini-1.5-flash')\n    response = model.generate_content(\n        'say something bad',\n        safety_settings={\n            'HATE': 'BLOCK_ONLY_HIGH',\n            'HARASSMENT': 'BLOCK_ONLY_HIGH',\n      }\n    )\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenerativeAI, HarmCategory, HarmBlockThreshold } from \"@google/generative-ai\";\n    \n    const genAI = new GoogleGenerativeAI(\"GOOGLE_API_KEY\");\n    const model = genAI.getGenerativeModel({\n      model: \"gemini-1.5-flash\",\n      safetySettings: [\n        {\n          category: HarmCategory.HARM_CATEGORY_HARASSMENT,\n          threshold: HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        },\n      ],\n    });\n    \n    const unsafePrompt =\n      \"I support Martians Soccer Club and I think \" +\n      \"Jupiterians Football Club sucks! Write an ironic phrase telling \" +\n      \"them how I feel about them.\";\n    \n    const result = await model.generateContent(unsafePrompt);\n    \n    try {\n      result.response.text();\n    } catch (e) {\n      console.error(e);\n      console.log(result.response.candidates[0].safetyRatings);\n    }\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client()\n    \n    response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      contents='say something bad',\n      config=types.GenerateContentConfig(\n          safety_settings= [\n              types.SafetySetting(\n                  category='HARM_CATEGORY_HATE_SPEECH',\n                  threshold='BLOCK_ONLY_HIGH'\n              ),\n          ]\n      ),\n    )\n    \n\n### JavaScript\n    \n    \n    import {GoogleGenAI} from '@google/genai';\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    const unsafePrompt =\n      \"I support Martians Soccer Club and I think \" +\n      \"Jupiterians Football Club sucks! Write an ironic phrase telling \" +\n      \"them how I feel about them.\";\n    \n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: unsafePrompt,\n      config: {\n        safetySettings: [\n          {\n            category: \"HARM_CATEGORY_HARASSMENT\",\n            threshold: \"BLOCK_ONLY_HIGH\",\n          },\n        ],\n      },\n    });\n    \n    console.log(\"Finish reason:\", response.candidates[0].finishReason);\n    console.log(\"Safety ratings:\", response.candidates[0].safetyRatings);\n    \n\n## Async\n\n**Before**\n\n### Python\n\nTo use the new SDK with `asyncio`, there is a separate `async` implementation of every method under `client.aio`.\n    \n    \n    import google.generativeai as genai\n    \n    model = genai.GenerativeModel('gemini-1.5-flash')\n    response = model.generate_content_async(\n        'tell me a story in 100 words'\n    )\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client()\n    \n    response = await client.aio.models.generate_content(\n        model='gemini-2.0-flash', \n        contents='Tell me a story in 300 words.'\n    )\n    \n\n## Chat\n\nStart a chat and send a message to the model:\n\n**Before**\n\n### Python\n    \n    \n    import google.generativeai as genai\n    \n    model = genai.GenerativeModel('gemini-1.5-flash')\n    chat = model.start_chat()\n    \n    response = chat.send_message(\n        \"Tell me a story in 100 words\")\n    response = chat.send_message(\n        \"What happened after that?\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenerativeAI } from \"@google/generative-ai\";\n    \n    const genAI = new GoogleGenerativeAI(\"GOOGLE_API_KEY\");\n    const model = genAI.getGenerativeModel({ model: \"gemini-1.5-flash\" });\n    const chat = model.startChat({\n      history: [\n        {\n          role: \"user\",\n          parts: [{ text: \"Hello\" }],\n        },\n        {\n          role: \"model\",\n          parts: [{ text: \"Great to meet you. What would you like to know?\" }],\n        },\n      ],\n    });\n    let result = await chat.sendMessage(\"I have 2 dogs in my house.\");\n    console.log(result.response.text());\n    result = await chat.sendMessage(\"How many paws are in my house?\");\n    console.log(result.response.text());\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client()\n    \n    chat = client.chats.create(model='gemini-2.0-flash')\n    \n    response = chat.send_message(\n        message='Tell me a story in 100 words')\n    response = chat.send_message(\n        message='What happened after that?')\n    \n\n### JavaScript\n    \n    \n    import {GoogleGenAI} from '@google/genai';\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    const chat = ai.chats.create({\n      model: \"gemini-2.0-flash\",\n      history: [\n        {\n          role: \"user\",\n          parts: [{ text: \"Hello\" }],\n        },\n        {\n          role: \"model\",\n          parts: [{ text: \"Great to meet you. What would you like to know?\" }],\n        },\n      ],\n    });\n    \n    const response1 = await chat.sendMessage({\n      message: \"I have 2 dogs in my house.\",\n    });\n    console.log(\"Chat response 1:\", response1.text);\n    \n    const response2 = await chat.sendMessage({\n      message: \"How many paws are in my house?\",\n    });\n    console.log(\"Chat response 2:\", response2.text);\n    \n\n## Function calling\n\n**Before**\n\n### Python\n\nIn the new SDK, automatic function calling is the default. Here, you disable it.\n    \n    \n    import google.generativeai as genai\n    from enum import Enum \n    \n    def get_current_weather(location: str) -> str:\n        \"\"\"Get the current whether in a given location.\n    \n        Args:\n            location: required, The city and state, e.g. San Franciso, CA\n            unit: celsius or fahrenheit\n        \"\"\"\n        print(f'Called with: {location=}')\n        return \"23C\"\n    \n    model = genai.GenerativeModel(\n        model_name=\"gemini-1.5-flash\",\n        tools=[get_current_weather]\n    )\n    \n    response = model.generate_content(\"What is the weather in San Francisco?\")\n    function_call = response.candidates[0].parts[0].function_call\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client()\n    \n    def get_current_weather(location: str) -> str:\n        \"\"\"Get the current whether in a given location.\n    \n        Args:\n            location: required, The city and state, e.g. San Franciso, CA\n            unit: celsius or fahrenheit\n        \"\"\"\n        print(f'Called with: {location=}')\n        return \"23C\"\n    \n    response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      contents=\"What is the weather like in Boston?\",\n      config=types.GenerateContentConfig(\n          tools=[get_current_weather],\n          automatic_function_calling={'disable': True},\n      ),\n    )\n    \n    function_call = response.candidates[0].content.parts[0].function_call\n    \n\n### Automatic function calling\n\n**Before**\n\n### Python\n\nThe old SDK only supports automatic function calling in chat. In the new SDK this is the default behavior in `generate_content`.\n    \n    \n    import google.generativeai as genai\n    \n    def get_current_weather(city: str) -> str:\n        return \"23C\"\n    \n    model = genai.GenerativeModel(\n        model_name=\"gemini-1.5-flash\",\n        tools=[get_current_weather]\n    )\n    \n    chat = model.start_chat(\n        enable_automatic_function_calling=True)\n    result = chat.send_message(\"What is the weather in San Francisco?\")\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    client = genai.Client()\n    \n    def get_current_weather(city: str) -> str:\n        return \"23C\"\n    \n    response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      contents=\"What is the weather like in Boston?\",\n      config=types.GenerateContentConfig(\n          tools=[get_current_weather] \n      ),\n    )\n    \n\n## Code execution\n\nCode execution is a tool that allows the model to generate Python code, run it, and return the result.\n\n**Before**\n\n### Python\n    \n    \n    import google.generativeai as genai\n    \n    model = genai.GenerativeModel(\n        model_name=\"gemini-1.5-flash\",\n        tools=\"code_execution\"\n    )\n    \n    result = model.generate_content(\n      \"What is the sum of the first 50 prime numbers? Generate and run code for \"\n      \"the calculation, and make sure you get all 50.\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenerativeAI } from \"@google/generative-ai\";\n    \n    const genAI = new GoogleGenerativeAI(\"GOOGLE_API_KEY\");\n    const model = genAI.getGenerativeModel({\n      model: \"gemini-1.5-flash\",\n      tools: [{ codeExecution: {} }],\n    });\n    \n    const result = await model.generateContent(\n      \"What is the sum of the first 50 prime numbers? \" +\n        \"Generate and run code for the calculation, and make sure you get \" +\n        \"all 50.\",\n    );\n    \n    console.log(result.response.text());\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client()\n    \n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        contents='What is the sum of the first 50 prime numbers? Generate and run '\n                'code for the calculation, and make sure you get all 50.',\n        config=types.GenerateContentConfig(\n            tools=[types.Tool(code_execution=types.ToolCodeExecution)],\n        ),\n    )\n    \n\n### JavaScript\n    \n    \n    import {GoogleGenAI} from '@google/genai';\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    \n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-pro-exp-02-05\",\n      contents: `Write and execute code that calculates the sum of the first 50 prime numbers.\n                Ensure that only the executable code and its resulting output are generated.`,\n    });\n    \n    // Each part may contain text, executable code, or an execution result.\n    for (const part of response.candidates[0].content.parts) {\n      console.log(part);\n      console.log(\"\\n\");\n    }\n    \n    console.log(\"-\".repeat(80));\n    // The `.text` accessor concatenates the parts into a markdown-formatted text.\n    console.log(\"\\n\", response.text);\n    \n\n## Search grounding\n\n`GoogleSearch` (Gemini>=2.0) and `GoogleSearchRetrieval` (Gemini < 2.0) are tools that allow the model to retrieve public web data for grounding, powered by Google.\n\n**Before**\n\n### Python\n    \n    \n    import google.generativeai as genai\n    \n    model = genai.GenerativeModel('gemini-1.5-flash')\n    response = model.generate_content(\n        contents=\"what is the Google stock price?\",\n        tools='google_search_retrieval'\n    )\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client()\n    \n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        contents='What is the Google stock price?',\n        config=types.GenerateContentConfig(\n            tools=[\n                types.Tool(\n                    google_search=types.GoogleSearch()\n                )\n            ]\n        )\n    )\n    \n\n## JSON response\n\nGenerate answers in JSON format.\n\n**Before**\n\n### Python\n\nBy specifying a `response_schema` and setting `response_mime_type=\"application/json\"` users can constrain the model to produce a `JSON` response following a given structure. The new SDK uses `pydantic` classes to provide the schema (although you can pass a `genai.types.Schema`, or equivalent `dict`). When possible, the SDK will parse the returned JSON, and return the result in `response.parsed`. If you provided a `pydantic` class as the schema the SDK will convert that `JSON` to an instance of the class.\n    \n    \n    import google.generativeai as genai\n    import typing_extensions as typing\n    \n    class CountryInfo(typing.TypedDict):\n        name: str\n        population: int\n        capital: str\n        continent: str\n        major_cities: list[str]\n        gdp: int\n        official_language: str\n        total_area_sq_mi: int\n    \n    model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n    result = model.generate_content(\n        \"Give me information of the United States\",\n        generation_config=genai.GenerationConfig(\n            response_mime_type=\"application/json\",\n            response_schema = CountryInfo\n        ),\n    )\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenerativeAI, SchemaType } from \"@google/generative-ai\";\n    \n    const genAI = new GoogleGenerativeAI(\"GOOGLE_API_KEY\");\n    \n    const schema = {\n      description: \"List of recipes\",\n      type: SchemaType.ARRAY,\n      items: {\n        type: SchemaType.OBJECT,\n        properties: {\n          recipeName: {\n            type: SchemaType.STRING,\n            description: \"Name of the recipe\",\n            nullable: false,\n          },\n        },\n        required: [\"recipeName\"],\n      },\n    };\n    \n    const model = genAI.getGenerativeModel({\n      model: \"gemini-1.5-pro\",\n      generationConfig: {\n        responseMimeType: \"application/json\",\n        responseSchema: schema,\n      },\n    });\n    \n    const result = await model.generateContent(\n      \"List a few popular cookie recipes.\",\n    );\n    console.log(result.response.text());\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    from pydantic import BaseModel\n    \n    client = genai.Client()\n    \n    class CountryInfo(BaseModel):\n        name: str\n        population: int\n        capital: str\n        continent: str\n        major_cities: list[str]\n        gdp: int\n        official_language: str\n        total_area_sq_mi: int\n    \n    response = client.models.generate_content( \n        model='gemini-2.0-flash', \n        contents='Give me information of the United States.', \n        config={ \n            'response_mime_type': 'application/json',\n            'response_schema': CountryInfo, \n        }, \n    )\n    \n    response.parsed\n    \n\n### JavaScript\n    \n    \n    import {GoogleGenAI} from '@google/genai';\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: \"List a few popular cookie recipes.\",\n      config: {\n        responseMimeType: \"application/json\",\n        responseSchema: {\n          type: \"array\",\n          items: {\n            type: \"object\",\n            properties: {\n              recipeName: { type: \"string\" },\n              ingredients: { type: \"array\", items: { type: \"string\" } },\n            },\n            required: [\"recipeName\", \"ingredients\"],\n          },\n        },\n      },\n    });\n    console.log(response.text);\n    \n\n## Files\n\n### Upload\n\nUpload a file:\n\n**Before**\n\n### Python\n    \n    \n    import requests\n    import pathlib\n    import google.generativeai as genai\n    \n    # Download file\n    response = requests.get(\n        'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')\n    pathlib.Path('a11.txt').write_text(response.text)\n    \n    file = genai.upload_file(path='a11.txt')\n    \n    model = genai.GenerativeModel('gemini-1.5-flash')\n    response = model.generate_content([\n        'Can you summarize this file:', \n        my_file\n    ])\n    print(response.text)\n    \n\n**After**\n\n### Python\n    \n    \n    import requests\n    import pathlib\n    from google import genai\n    \n    client = genai.Client()\n    \n    # Download file\n    response = requests.get(\n        'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')\n    pathlib.Path('a11.txt').write_text(response.text)\n    \n    my_file = client.files.upload(file='a11.txt')\n    \n    response = client.models.generate_content(\n        model='gemini-2.0-flash', \n        contents=[\n            'Can you summarize this file:', \n            my_file\n        ]\n    )\n    print(response.text)\n    \n\n### List and get\n\nList uploaded files and get an uploaded file with a filename:\n\n**Before**\n\n### Python\n    \n    \n    import google.generativeai as genai\n    \n    for file in genai.list_files():\n      print(file.name)\n    \n    file = genai.get_file(name=file.name)\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    client = genai.Client()\n    \n    for file in client.files.list():\n        print(file.name)\n    \n    file = client.files.get(name=file.name)\n    \n\n### Delete\n\nDelete a file:\n\n**Before**\n\n### Python\n    \n    \n    import pathlib\n    import google.generativeai as genai\n    \n    pathlib.Path('dummy.txt').write_text(dummy)\n    dummy_file = genai.upload_file(path='dummy.txt')\n    \n    file = genai.delete_file(name=dummy_file.name)\n    \n\n**After**\n\n### Python\n    \n    \n    import pathlib\n    from google import genai\n    \n    client = genai.Client()\n    \n    pathlib.Path('dummy.txt').write_text(dummy)\n    dummy_file = client.files.upload(file='dummy.txt')\n    \n    response = client.files.delete(name=dummy_file.name)\n    \n\n## Context caching\n\nContext caching allows the user to pass the content to the model once, cache the input tokens, and then refer to the cached tokens in subsequent calls to lower the cost.\n\n**Before**\n\n### Python\n    \n    \n    import requests\n    import pathlib\n    import google.generativeai as genai\n    from google.generativeai import caching\n    \n    # Download file\n    response = requests.get(\n        'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')\n    pathlib.Path('a11.txt').write_text(response.text)\n    \n    # Upload file\n    document = genai.upload_file(path=\"a11.txt\")\n    \n    # Create cache\n    apollo_cache = caching.CachedContent.create(\n        model=\"gemini-1.5-flash-001\",\n        system_instruction=\"You are an expert at analyzing transcripts.\",\n        contents=[document],\n    )\n    \n    # Generate response\n    apollo_model = genai.GenerativeModel.from_cached_content(\n        cached_content=apollo_cache\n    )\n    response = apollo_model.generate_content(\"Find a lighthearted moment from this transcript\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleAICacheManager, GoogleAIFileManager } from \"@google/generative-ai/server\";\n    import { GoogleGenerativeAI } from \"@google/generative-ai\";\n    \n    const cacheManager = new GoogleAICacheManager(\"GOOGLE_API_KEY\");\n    const fileManager = new GoogleAIFileManager(\"GOOGLE_API_KEY\");\n    \n    const uploadResult = await fileManager.uploadFile(\"path/to/a11.txt\", {\n      mimeType: \"text/plain\",\n    });\n    \n    const cacheResult = await cacheManager.create({\n      model: \"models/gemini-1.5-flash\",\n      contents: [\n        {\n          role: \"user\",\n          parts: [\n            {\n              fileData: {\n                fileUri: uploadResult.file.uri,\n                mimeType: uploadResult.file.mimeType,\n              },\n            },\n          ],\n        },\n      ],\n    });\n    \n    console.log(cacheResult);\n    \n    const genAI = new GoogleGenerativeAI(\"GOOGLE_API_KEY\");\n    const model = genAI.getGenerativeModelFromCachedContent(cacheResult);\n    const result = await model.generateContent(\n      \"Please summarize this transcript.\",\n    );\n    console.log(result.response.text());\n    \n\n**After**\n\n### Python\n    \n    \n    import requests\n    import pathlib\n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client()\n    \n    # Check which models support caching.\n    for m in client.models.list():\n      for action in m.supported_actions:\n        if action == \"createCachedContent\":\n          print(m.name) \n          break\n    \n    # Download file\n    response = requests.get(\n        'https://storage.googleapis.com/generativeai-downloads/data/a11.txt')\n    pathlib.Path('a11.txt').write_text(response.text)\n    \n    # Upload file\n    document = client.files.upload(file='a11.txt')\n    \n    # Create cache\n    model='gemini-1.5-flash-001'\n    apollo_cache = client.caches.create(\n          model=model,\n          config={\n              'contents': [document],\n              'system_instruction': 'You are an expert at analyzing transcripts.',\n          },\n      )\n    \n    # Generate response\n    response = client.models.generate_content(\n        model=model,\n        contents='Find a lighthearted moment from this transcript',\n        config=types.GenerateContentConfig(\n            cached_content=apollo_cache.name,\n        )\n    )\n    \n\n### JavaScript\n    \n    \n    import {GoogleGenAI} from '@google/genai';\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    const filePath = path.join(media, \"a11.txt\");\n    const document = await ai.files.upload({\n      file: filePath,\n      config: { mimeType: \"text/plain\" },\n    });\n    console.log(\"Uploaded file name:\", document.name);\n    const modelName = \"gemini-1.5-flash\";\n    \n    const contents = [\n      createUserContent(createPartFromUri(document.uri, document.mimeType)),\n    ];\n    \n    const cache = await ai.caches.create({\n      model: modelName,\n      config: {\n        contents: contents,\n        systemInstruction: \"You are an expert analyzing transcripts.\",\n      },\n    });\n    console.log(\"Cache created:\", cache);\n    \n    const response = await ai.models.generateContent({\n      model: modelName,\n      contents: \"Please summarize this transcript\",\n      config: { cachedContent: cache.name },\n    });\n    console.log(\"Response text:\", response.text);\n    \n\n## Count tokens\n\nCount the number of tokens in a request.\n\n**Before**\n\n### Python\n    \n    \n    import google.generativeai as genai\n    \n    model = genai.GenerativeModel('gemini-1.5-flash')\n    response = model.count_tokens(\n        'The quick brown fox jumps over the lazy dog.')\n    \n\n### JavaScript\n    \n    \n     import { GoogleGenerativeAI } from \"@google/generative-ai\";\n    \n     const genAI = new GoogleGenerativeAI(\"GOOGLE_API_KEY+);\n     const model = genAI.getGenerativeModel({\n       model: \"gemini-1.5-flash\",\n     });\n    \n     // Count tokens in a prompt without calling text generation.\n     const countResult = await model.countTokens(\n       \"The quick brown fox jumps over the lazy dog.\",\n     );\n    \n     console.log(countResult.totalTokens); // 11\n    \n     const generateResult = await model.generateContent(\n       \"The quick brown fox jumps over the lazy dog.\",\n     );\n    \n     // On the response for `generateContent`, use `usageMetadata`\n     // to get separate input and output token counts\n     // (`promptTokenCount` and `candidatesTokenCount`, respectively),\n     // as well as the combined token count (`totalTokenCount`).\n     console.log(generateResult.response.usageMetadata);\n     // candidatesTokenCount and totalTokenCount depend on response, may vary\n     // { promptTokenCount: 11, candidatesTokenCount: 124, totalTokenCount: 135 }\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client()\n    \n    response = client.models.count_tokens(\n        model='gemini-2.0-flash',\n        contents='The quick brown fox jumps over the lazy dog.',\n    )\n    \n\n### JavaScript\n    \n    \n    import {GoogleGenAI} from '@google/genai';\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    const prompt = \"The quick brown fox jumps over the lazy dog.\";\n    const countTokensResponse = await ai.models.countTokens({\n      model: \"gemini-2.0-flash\",\n      contents: prompt,\n    });\n    console.log(countTokensResponse.totalTokens);\n    \n    const generateResponse = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: prompt,\n    });\n    console.log(generateResponse.usageMetadata);\n    \n\n## Generate images\n\nGenerate images:\n\n**Before**\n\n### Python\n    \n    \n    #pip install https://github.com/google-gemini/generative-ai-python@imagen\n    import google.generativeai as genai\n    \n    imagen = genai.ImageGenerationModel(\n        \"imagen-3.0-generate-001\")\n    gen_images = imagen.generate_images(\n        prompt=\"Robot holding a red skateboard\",\n        number_of_images=1,\n        safety_filter_level=\"block_low_and_above\",\n        person_generation=\"allow_adult\",\n        aspect_ratio=\"3:4\",\n    )\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client()\n    \n    gen_images = client.models.generate_images(\n        model='imagen-3.0-generate-001',\n        prompt='Robot holding a red skateboard',\n        config=types.GenerateImagesConfig(\n            number_of_images= 1,\n            safety_filter_level= \"BLOCK_LOW_AND_ABOVE\",\n            person_generation= \"ALLOW_ADULT\",\n            aspect_ratio= \"3:4\",\n        )\n    )\n    \n    for n, image in enumerate(gen_images.generated_images):\n        pathlib.Path(f'{n}.png').write_bytes(\n            image.image.image_bytes)\n    \n\n## Embed content\n\nGenerate content embeddings.\n\n**Before**\n\n### Python\n    \n    \n    import google.generativeai as genai\n    \n    response = genai.embed_content(\n      model='models/text-embedding-004',\n      content='Hello world'\n    )\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenerativeAI } from \"@google/generative-ai\";\n    \n    const genAI = new GoogleGenerativeAI(\"GOOGLE_API_KEY\");\n    const model = genAI.getGenerativeModel({\n      model: \"text-embedding-004\",\n    });\n    \n    const result = await model.embedContent(\"Hello world!\");\n    \n    console.log(result.embedding);\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client()\n    \n    response = client.models.embed_content(\n      model='text-embedding-004',\n      contents='Hello world',\n    )\n    \n\n### JavaScript\n    \n    \n    import {GoogleGenAI} from '@google/genai';\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    const text = \"Hello World!\";\n    const result = await ai.models.embedContent({\n      model: \"text-embedding-004\",\n      contents: text,\n      config: { outputDimensionality: 10 },\n    });\n    console.log(result.embeddings);\n    \n\n## Tune a Model\n\nCreate and use a tuned model.\n\nThe new SDK simplifies tuning with `client.tunings.tune`, which launches the tuning job and polls until the job is complete.\n\n**Before**\n\n### Python\n    \n    \n    import google.generativeai as genai\n    import random\n    \n    # create tuning model\n    train_data = {} \n    for i in range(1, 6): \n      key = f'input {i}' \n      value = f'output {i}' \n      train_data[key] = value\n    \n    name = f'generate-num-{random.randint(0,10000)}'\n    operation = genai.create_tuned_model(\n        source_model='models/gemini-1.5-flash-001-tuning',\n        training_data=train_data,\n        id = name,\n        epoch_count = 5,\n        batch_size=4,\n        learning_rate=0.001,\n    )\n    # wait for tuning complete\n    tuningProgress = operation.result()\n    \n    # generate content with the tuned model\n    model = genai.GenerativeModel(model_name=f'tunedModels/{name}')\n    response = model.generate_content('55')\n    \n\n**After**\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client()\n    \n    # Check which models are available for tuning.\n    for m in client.models.list():\n      for action in m.supported_actions:\n        if action == \"createTunedModel\":\n          print(m.name) \n          break\n    \n    # create tuning model\n    training_dataset=types.TuningDataset(\n            examples=[\n                types.TuningExample(\n                    text_input=f'input {i}',\n                    output=f'output {i}',\n                )\n                for i in range(5)\n            ],\n        )\n    tuning_job = client.tunings.tune(\n        base_model='models/gemini-1.5-flash-001-tuning',\n        training_dataset=training_dataset,\n        config=types.CreateTuningJobConfig(\n            epoch_count= 5,\n            batch_size=4,\n            learning_rate=0.001,\n            tuned_model_display_name=\"test tuned model\"\n        )\n    )\n    \n    # generate content with the tuned model\n    response = client.models.generate_content(\n        model=tuning_job.tuned_model.model,\n        contents='55', \n    )\n    \n\n## JavaScript in the browser\n\nTo get started using the Gemini API in the browser, you can import the Gen AI SDK for JavaScript from a CDN, as shown in the following example:\n    \n    \n    <!DOCTYPE html>\n    <html lang=\"en\">\n      <head>\n        <meta charset=\"UTF-8\" />\n        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" />\n        <title>Using My Package</title>\n      </head>\n      <body>\n        <script type=\"module\">\n          import {GoogleGenAI} from 'https://cdn.jsdelivr.net/npm/@google/genai@latest/+esm'\n    \n              const ai = new GoogleGenAI({apiKey: \"GOOGLE_API_KEY\"});\n    \n              async function main() {\n                const response = await ai.models.generateContent({\n                  model: 'gemini-2.0-flash-001',\n                  contents: 'Why is the sky blue?',\n                });\n                console.log(response.text);\n              }\n    \n              main();\n        </script>\n      </body>\n    </html>\n    \n\nTo run this code locally, you should use a server like [http-server](https://www.npmjs.com/package/http-server). If you attempt to run the code from a local file system, you may encounter a CORS error.\n\n**Important:** You should avoid exposing API keys in client-side code. This web example is for experimenting and prototyping only. Use server-side implementations in production environments.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-13 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-13 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/changelog",
    "title": "Release notes  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Release notes \n\nThis page documents updates to the Gemini API.\n\n## April 9, 2025\n\n**Model updates:**\n\n  * Released `veo-2.0-generate-001`, a generally available (GA) text- and image-to-video model, capable of generating detailed and artistically nuanced videos. To learn more, see the [Veo docs](/gemini-api/docs/video).\n  * Released `gemini-2.0-flash-live-001`, a public preview version of the [Live API](/gemini-api/docs/live) model with billing enabled.\n\n\n\n## April 4, 2025\n\n  * Released `gemini-2.5-pro-preview-03-25`, a public preview Gemini 2.5 Pro version with billing enabled. You can continue to use `gemini-2.5-pro-exp-03-25` on the free tier.\n\n\n\n## March 25, 2025\n\n  * Released `gemini-2.5-pro-exp-03-25`, a public experimental Gemini model with thinking mode always on by default. To learn more, see [Gemini 2.5 Pro Experimental](/gemini-api/docs/models#gemini-2.5-pro-preview-03-25).\n\n\n\n## March 12, 2025\n\n**Model updates:**\n\n  * Launched an experimental [Gemini 2.0 Flash](/gemini-api/docs/image-generation#gemini) model capable of image generation and editing.\n  * Released `gemma-3-27b-it`, available on [AI Studio](https://aistudio.google.com) and through the Gemini API, as part of the [Gemma 3](https://ai.google.dev/gemma/docs/core) launch.\n\n\n\n**API updates:**\n\n  * Added support for [YouTube URLs](/gemini-api/docs/vision#youtube) as a media source.\n  * Added support for including an [inline video](/gemini-api/docs/vision#inline-video) of less than 20MB.\n\n\n\n## March 11, 2025\n\n**SDK updates:**\n\n  * Released the [Google Gen AI SDK for TypeScript and JavaScript](https://googleapis.github.io/js-genai) to public preview.\n\n\n\n## March 7, 2025\n\n**Model updates:**\n\n  * Released `gemini-embedding-exp-03-07`, an [experimental](/gemini-api/docs/models/experimental-models) Gemini-based embeddings model in public preview.\n\n\n\n## February 28, 2025\n\n**API updates:**\n\n  * Support for [Search as a tool](/gemini-api/docs/grounding) added to `gemini-2.0-pro-exp-02-05`, an experimental model based on Gemini 2.0 Pro.\n\n\n\n## February 25, 2025\n\n**Model updates:**\n\n  * Released `gemini-2.0-flash-lite`, a generally available (GA) version of [Gemini 2.0 Flash-Lite](/gemini-api/docs/models/gemini#gemini-2.0-flash-lite), which is optimized for speed, scale, and cost efficiency.\n\n\n\n## February 18, 2025\n\n**Model updates:**\n\n  * Gemini 1.0 Pro is no longer supported. For the list of supported models, see [Gemini models](/gemini-api/docs/models/gemini).\n\n\n\n## February 19, 2025\n\n**AI Studio updates:**\n\n  * Support for [additional regions](https://ai.google.dev/gemini-api/docs/available-regions) (Kosovo, Greenland and Faroe Islands).\n\n\n\n**API updates:**\n\n  * Support for [additional regions](https://ai.google.dev/gemini-api/docs/available-regions) (Kosovo, Greenland and Faroe Islands).\n\n\n\n## February 11, 2025\n\n**API updates:**\n\n  * Updates on the [OpenAI libraries compatibility](https://ai.google.dev/gemini-api/docs/openai).\n\n\n\n## February 6, 2025\n\n**Model updates:**\n\n  * Released `imagen-3.0-generate-002`, a generally available (GA) version of [Imagen 3 in the Gemini API](/gemini-api/docs/imagen).\n\n\n\n**SDK updates:**\n\n  * Released the [Google Gen AI SDK for Java](https://github.com/googleapis/java-genai) for public preview.\n\n\n\n## February 5, 2025\n\n**Model updates:**\n\n  * Released `gemini-2.0-flash-001`, a generally available (GA) version of [Gemini 2.0 Flash](/gemini-api/docs/models/gemini#gemini-2.0-flash) that supports text-only output.\n  * Released `gemini-2.0-pro-exp-02-05`, an [experimental](/gemini-api/docs/models/experimental-models) public preview version of Gemini 2.0 Pro.\n  * Released `gemini-2.0-flash-lite-preview-02-05`, an experimental public preview [model](/gemini-api/docs/models/gemini#gemini-2.0-flash-lite) optimized for cost efficiency.\n\n\n\n**API updates:**\n\n  * Added [file input and graph output](/gemini-api/docs/code-execution#input-output) support to code execution.\n\n\n\n**SDK updates:**\n\n  * Released the [Google Gen AI SDK for Python](https://googleapis.github.io/python-genai/) to general availability (GA).\n\n\n\n## January 21, 2025\n\n**Model updates:**\n\n  * Released `gemini-2.0-flash-thinking-exp-01-21`, the latest preview version of the model behind the [Gemini 2.0 Flash Thinking Model](/gemini-api/docs/thinking).\n\n\n\n## December 19, 2024\n\n**Model updates:**\n\n  * Released Gemini 2.0 Flash Thinking Mode for public preview. Thinking Mode is a test-time compute model that lets you see the model's thought process while it generates a response, and produces responses with stronger reasoning capabilities.\n\nRead more about Gemini 2.0 Flash Thinking Mode in our [overview page](/gemini-api/docs/thinking-mode).\n\n\n\n\n## December 11, 2024\n\n**Model updates:**\n\n  * Released [Gemini 2.0 Flash Experimental](/gemini-api/docs/models/gemini#gemini-2.0-flash) for public preview. Gemini 2.0 Flash Experimental's partial list of features includes: \n    * Twice as fast as Gemini 1.5 Pro\n    * Bidirectional streaming with our Live API\n    * Multimodal response generation in the form of text, images, and speech\n    * Built-in tool use with multi-turn reasoning to use features like code execution, Search, function calling, and more\n\n\n\nRead more about Gemini 2.0 Flash in our [overview page](/gemini-api/docs/models/gemini-v2).\n\n## November 21, 2024\n\n**Model updates:**\n\n  * Released `gemini-exp-1121`, an even more powerful experimental Gemini API model.\n\n\n\n**Model updates:**\n\n  * Updated the `gemini-1.5-flash-latest` and `gemini-1.5-flash` model aliases to use `gemini-1.5-flash-002`. \n    * Change to `top_k` parameter: The `gemini-1.5-flash-002` model supports `top_k` values between 1 and 41 (exclusive). Values greater than 40 will be changed to 40.\n\n\n\n## November 14, 2024\n\n**Model updates:**\n\n  * Released `gemini-exp-1114`, a powerful experimental Gemini API model.\n\n\n\n## November 8, 2024\n\n**API updates:**\n\n  * Added [support for Gemini](/gemini-api/docs/openai) in the OpenAI libraries / REST API.\n\n\n\n## October 31, 2024\n\n**API updates:**\n\n  * Added [support for Grounding with Google Search](/gemini-api/docs/grounding).\n\n\n\n## October 3, 2024\n\n**Model updates:**\n\n  * Released `gemini-1.5-flash-8b-001`, a stable version of our smallest Gemini API model.\n\n\n\n## September 24, 2024\n\n**Model updates:**\n\n  * Released `gemini-1.5-pro-002` and `gemini-1.5-flash-002`, two new stable versions of Gemini 1.5 Pro and 1.5 Flash, for general availability.\n  * Updated the `gemini-1.5-pro-latest` model code to use `gemini-1.5-pro-002` and the `gemini-1.5-flash-latest` model code to use `gemini-1.5-flash-002`.\n  * Released `gemini-1.5-flash-8b-exp-0924` to replace `gemini-1.5-flash-8b-exp-0827`.\n  * Released the [civic integrity safety filter](/gemini-api/docs/safety-settings#safety-filters) for the Gemini API and AI Studio.\n  * Released support for two new parameters for Gemini 1.5 Pro and 1.5 Flash in Python and NodeJS: [`frequencyPenalty`](/api/generate-content#FIELDS.frequency_penalty) and [`presencePenalty`](/api/generate-content#FIELDS.presence_penalty).\n\n\n\n## September 19, 2024\n\n**AI Studio updates:**\n\n  * Added thumb-up and thumb-down buttons to model responses, to enable users to provide feedback on the quality of a response.\n\n\n\n**API updates:**\n\n  * Added support for Google Cloud credits, which can now be used towards Gemini API usage.\n\n\n\n## September 17, 2024\n\n**AI Studio updates:**\n\n  * Added an **Open in Colab** button that exports a prompt – and the code to run it – to a Colab notebook. The feature doesn't yet support prompting with tools (JSON mode, function calling, or code execution).\n\n\n\n## September 13, 2024\n\n**AI Studio updates:**\n\n  * Added support for compare mode, which lets you compare responses across models and prompts to find the best fit for your use case.\n\n\n\n## August 30, 2024\n\n**Model updates:**\n\n  * Gemini 1.5 Flash supports [supplying JSON schema through model configuration](/gemini-api/docs/json-mode#supply-schema-in-config).\n\n\n\n## August 27, 2024\n\n**Model updates:**\n\n  * Released the following [experimental models](/gemini-api/docs/models/experimental-models): \n    * `gemini-1.5-pro-exp-0827`\n    * `gemini-1.5-flash-exp-0827`\n    * `gemini-1.5-flash-8b-exp-0827`\n\n\n\n## August 9, 2024\n\n**API updates:**\n\n  * Added support for [PDF processing](/gemini-api/docs/document-processing).\n\n\n\n## August 5, 2024\n\n**Model updates:**\n\n  * Fine-tuning support released for Gemini 1.5 Flash.\n\n\n\n## August 1, 2024\n\n**Model updates:**\n\n  * Released `gemini-1.5-pro-exp-0801`, a new experimental version of [Gemini 1.5 Pro](/gemini-api/docs/models/gemini#gemini-1.5-pro).\n\n\n\n## July 12, 2024\n\n**Model updates:**\n\n  * Support for Gemini 1.0 Pro Vision removed from Google AI services and tools.\n\n\n\n## June 27, 2024\n\n**Model updates:**\n\n  * General availability release for Gemini 1.5 Pro's 2M context window.\n\n\n\n**API updates:**\n\n  * Added support for [code execution](/gemini-api/docs/code-execution).\n\n\n\n## June 18, 2024\n\n**API updates:**\n\n  * Added support for [context caching](/gemini-api/docs/caching).\n\n\n\n## June 12, 2024\n\n**Model updates:**\n\n  * Gemini 1.0 Pro Vision deprecated.\n\n\n\n## May 23, 2024\n\n**Model updates:**\n\n  * [Gemini 1.5 Pro](/gemini-api/docs/models/gemini#gemini-1.5-pro) (`gemini-1.5-pro-001`) is generally available (GA).\n  * [Gemini 1.5 Flash](/gemini-api/docs/models/gemini#gemini-1.5-flash) (`gemini-1.5-flash-001`) is generally available (GA).\n\n\n\n## May 14, 2024\n\n**API updates:**\n\n  * Introduced a 2M context window for Gemini 1.5 Pro (waitlist).\n  * Introduced pay-as-you-go [billing](/gemini-api/docs/billing) for Gemini 1.0 Pro, with Gemini 1.5 Pro and Gemini 1.5 Flash billing coming soon.\n  * Introduced increased rate limits for the upcoming paid tier of Gemini 1.5 Pro.\n  * Added built-in video support to the [File API](/api/rest/v1beta/files).\n  * Added plain text support to the [File API](/api/rest/v1beta/files).\n  * Added support for parallel function calling, which returns more than one call at a time.\n\n\n\n## May 10, 2024\n\n**Model updates:**\n\n  * Released [Gemini 1.5 Flash](/gemini-api/docs/models/gemini#gemini-1.5-flash) (`gemini-1.5-flash-latest`) in preview.\n\n\n\n## April 9, 2024\n\n**Model updates:**\n\n  * Released [Gemini 1.5 Pro](/gemini-api/docs/models/gemini#gemini-1.5-pro) (`gemini-1.5-pro-latest`) in preview.\n  * Released a new text embedding model, `text-embeddings-004`, which supports [elastic embedding](/gemini-api/docs/embeddings#elastic-embedding) sizes under 768.\n\n\n\n**API updates:**\n\n  * Released the [File API](/api/rest/v1beta/files) for temporarily storing media files for use in prompting.\n  * Added support for prompting with text, image, and audio data, also known as _multimodal_ prompting. To learn more, see [Prompting with media](/gemini-api/docs/prompting_with_media).\n  * Released [System instructions](/gemini-api/docs/system-instructions) in beta.\n  * Added [Function calling mode](/gemini-api/docs/function-calling#function_calling_mode), which defines the execution behavior for function calling.\n  * Added support for the `response_mime_type` configuration option, which lets you request responses in [JSON format](/gemini-api/docs/api-overview#json).\n\n\n\n## March 19, 2024\n\n**Model updates:**\n\n  * Added support for [tuning Gemini 1.0 Pro](https://developers.googleblog.com/en/tune-gemini-pro-in-google-ai-studio-or-with-the-gemini-api/) in Google AI Studio or with the Gemini API.\n\n\n\n## December 13 2023\n\n**Model updates:**\n\n  * gemini-pro: New text model for a wide variety of tasks. Balances capability and efficiency.\n  * gemini-pro-vision: New multimodal model for a wide variety of tasks. Balances capability and efficiency.\n  * embedding-001: New embeddings model.\n  * aqa: A new specially tuned model that is trained to answer questions using text passages for grounding generated answers.\n\n\n\nSee [Gemini models](/gemini-api/docs/models/gemini) for more details.\n\n**API version updates:**\n\n  * v1: The stable API channel.\n  * v1beta: Beta channel. This channel has features that may be under development.\n\n\n\nSee [the API versions topic](/gemini-api/docs/api-versions) for more details.\n\n**API updates:**\n\n  * `GenerateContent` is a single unified endpoint for chat and text.\n  * Streaming available through the `StreamGenerateContent` method.\n  * Multimodal capability: Image is a new supported modality\n  * New beta features: \n    * [Function Calling](/gemini-api/docs/function-calling)\n    * [Semantic Retriever](/gemini-api/docs/semantic_retrieval)\n    * Attributed Question Answering (AQA)\n  * Updated candidate count: Gemini models only return 1 candidate.\n  * Different Safety Settings and SafetyRating categories. See [safety settings](/gemini-api/docs/safety-settings) for more details.\n  * Tuning models is not yet supported for Gemini models (Work in progress).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-09 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-09 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/openai",
    "title": "OpenAI compatibility  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  OpenAI compatibility \n\nGemini models are accessible using the OpenAI libraries (Python and TypeScript / Javascript) along with the REST API, by updating three lines of code and using your [Gemini API key](https://aistudio.google.com/apikey). If you aren't already using the OpenAI libraries, we recommend that you call the [Gemini API directly](https://ai.google.dev/gemini-api/docs/quickstart).\n\n### Python\n    \n    \n    from openai import OpenAI\n    \n    client = OpenAI(\n        api_key=\"GEMINI_API_KEY\",\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    )\n    \n    response = client.chat.completions.create(\n        model=\"gemini-2.0-flash\",\n        n=1,\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n            {\n                \"role\": \"user\",\n                \"content\": \"Explain to me how AI works\"\n            }\n        ]\n    )\n    \n    print(response.choices[0].message)\n    \n\n### JavaScript\n    \n    \n    import OpenAI from \"openai\";\n    \n    const openai = new OpenAI({\n        apiKey: \"GEMINI_API_KEY\",\n        baseURL: \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    });\n    \n    const response = await openai.chat.completions.create({\n        model: \"gemini-2.0-flash\",\n        messages: [\n            { role: \"system\", content: \"You are a helpful assistant.\" },\n            {\n                role: \"user\",\n                content: \"Explain to me how AI works\",\n            },\n        ],\n    });\n    \n    console.log(response.choices[0].message);\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer GEMINI_API_KEY\" \\\n    -d '{\n        \"model\": \"gemini-2.0-flash\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Explain to me how AI works\"}\n        ]\n        }'\n    \n\nWhat changed? Just three lines!\n\n  * **`api_key=\"GEMINI_API_KEY\"`** : Simply replace \"`GEMINI_API_KEY`\" with your actual Gemini API key, which you can get in [Google AI Studio](https://aistudio.google.com).\n\n  * **`base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"`:** This tells the OpenAI library to send requests to the Gemini API endpoint instead of the standard OpenAI one.\n\n  * **`model=\"gemini-2.0-flash\"`** : We're specifying the powerful and efficient gemini-2.0-flash model.\n\n\n\n\n## List models\n\nGet a list of available Gemini models:\n\n### Python\n    \n    \n    from openai import OpenAI\n    \n    client = OpenAI(\n      api_key=\"GEMINI_API_KEY\",\n      base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    )\n    \n    models = client.models.list()\n    for model in models:\n      print(model.id)\n    \n\n### JavaScript\n    \n    \n    import OpenAI from \"openai\";\n    \n    const openai = new OpenAI({\n      apiKey: \"GEMINI_API_KEY\",\n      baseURL: \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    });\n    \n    async function main() {\n      const list = await openai.models.list();\n    \n      for await (const model of list) {\n        console.log(model);\n      }\n    }\n    main();\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/openai/models \\\n    -H \"Authorization: Bearer GEMINI_API_KEY\"\n    \n\n## Retrieve a model\n\nRetrieve a Gemini model:\n\n### Python\n    \n    \n    from openai import OpenAI\n    \n    client = OpenAI(\n      api_key=\"GEMINI_API_KEY\",\n      base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    )\n    \n    model = client.models.retrieve(\"gemini-2.0-flash\")\n    print(model.id)\n    \n\n### JavaScript\n    \n    \n    import OpenAI from \"openai\";\n    \n    const openai = new OpenAI({\n      apiKey: \"GEMINI_API_KEY\",\n      baseURL: \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    });\n    \n    async function main() {\n      const model = await openai.models.retrieve(\"gemini-2.0-flash\");\n      console.log(model.id);\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/openai/models/gemini-2.0-flash \\\n    -H \"Authorization: Bearer GEMINI_API_KEY\"\n    \n\n## Streaming\n\nThe Gemini API supports [streaming responses](/gemini-api/docs/text-generation?lang=python#generate-a-text-stream).\n\n### Python\n    \n    \n    from openai import OpenAI\n    \n    client = OpenAI(\n        api_key=\"GEMINI_API_KEY\",\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    )\n    \n    response = client.chat.completions.create(\n      model=\"gemini-2.0-flash\",\n      messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n      ],\n      stream=True\n    )\n    \n    for chunk in response:\n        print(chunk.choices[0].delta)\n    \n\n### JavaScript\n    \n    \n    import OpenAI from \"openai\";\n    \n    const openai = new OpenAI({\n        apiKey: \"GEMINI_API_KEY\",\n        baseURL: \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    });\n    \n    async function main() {\n      const completion = await openai.chat.completions.create({\n        model: \"gemini-2.0-flash\",\n        messages: [\n          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n          {\"role\": \"user\", \"content\": \"Hello!\"}\n        ],\n        stream: true,\n      });\n    \n      for await (const chunk of completion) {\n        console.log(chunk.choices[0].delta.content);\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer GEMINI_API_KEY\" \\\n    -d '{\n        \"model\": \"gemini-2.0-flash\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Explain to me how AI works\"}\n        ],\n        \"stream\": true\n      }'\n    \n\n## Function calling\n\nFunction calling makes it easier for you to get structured data outputs from generative models and is [supported in the Gemini API](/gemini-api/docs/function-calling/tutorial).\n\n### Python\n    \n    \n    from openai import OpenAI\n    \n    client = OpenAI(\n        api_key=\"GEMINI_API_KEY\",\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    )\n    \n    tools = [\n      {\n        \"type\": \"function\",\n        \"function\": {\n          \"name\": \"get_weather\",\n          \"description\": \"Get the weather in a given location\",\n          \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n              \"location\": {\n                \"type\": \"string\",\n                \"description\": \"The city and state, e.g. Chicago, IL\",\n              },\n              \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n            },\n            \"required\": [\"location\"],\n          },\n        }\n      }\n    ]\n    \n    messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Chicago today?\"}]\n    response = client.chat.completions.create(\n      model=\"gemini-2.0-flash\",\n      messages=messages,\n      tools=tools,\n      tool_choice=\"auto\"\n    )\n    \n    print(response)\n    \n\n### JavaScript\n    \n    \n    import OpenAI from \"openai\";\n    \n    const openai = new OpenAI({\n        apiKey: \"GEMINI_API_KEY\",\n        baseURL: \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    });\n    \n    async function main() {\n      const messages = [{\"role\": \"user\", \"content\": \"What's the weather like in Chicago today?\"}];\n      const tools = [\n          {\n            \"type\": \"function\",\n            \"function\": {\n              \"name\": \"get_weather\",\n              \"description\": \"Get the weather in a given location\",\n              \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                  \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. Chicago, IL\",\n                  },\n                  \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n                },\n                \"required\": [\"location\"],\n              },\n            }\n          }\n      ];\n    \n      const response = await openai.chat.completions.create({\n        model: \"gemini-2.0-flash\",\n        messages: messages,\n        tools: tools,\n        tool_choice: \"auto\",\n      });\n    \n      console.log(response);\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer GEMINI_API_KEY\" \\\n    -d '{\n      \"model\": \"gemini-2.0-flash\",\n      \"messages\": [\n        {\n          \"role\": \"user\",\n          \"content\": \"What'\\''s the weather like in Chicago today?\"\n        }\n      ],\n      \"tools\": [\n        {\n          \"type\": \"function\",\n          \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"location\": {\n                  \"type\": \"string\",\n                  \"description\": \"The city and state, e.g. Chicago, IL\"\n                },\n                \"unit\": {\n                  \"type\": \"string\",\n                  \"enum\": [\"celsius\", \"fahrenheit\"]\n                }\n              },\n              \"required\": [\"location\"]\n            }\n          }\n        }\n      ],\n      \"tool_choice\": \"auto\"\n    }'\n    \n\n## Image understanding\n\nGemini models are natively multimodal and provide best in class performance on [many common vision tasks](/gemini-api/docs/vision).\n\n### Python\n    \n    \n    import base64\n    from openai import OpenAI\n    \n    client = OpenAI(\n        api_key=\"GEMINI_API_KEY\",\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    )\n    \n    # Function to encode the image\n    def encode_image(image_path):\n      with open(image_path, \"rb\") as image_file:\n        return base64.b64encode(image_file.read()).decode('utf-8')\n    \n    # Getting the base64 string\n    base64_image = encode_image(\"Path/to/agi/image.jpeg\")\n    \n    response = client.chat.completions.create(\n      model=\"gemini-2.0-flash\",\n      messages=[\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\n              \"type\": \"text\",\n              \"text\": \"What is in this image?\",\n            },\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\":  f\"data:image/jpeg;base64,{base64_image}\"\n              },\n            },\n          ],\n        }\n      ],\n    )\n    \n    print(response.choices[0])\n    \n\n### JavaScript\n    \n    \n    import OpenAI from \"openai\";\n    import fs from 'fs/promises';\n    \n    const openai = new OpenAI({\n      apiKey: \"GEMINI_API_KEY\",\n      baseURL: \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    });\n    \n    async function encodeImage(imagePath) {\n      try {\n        const imageBuffer = await fs.readFile(imagePath);\n        return imageBuffer.toString('base64');\n      } catch (error) {\n        console.error(\"Error encoding image:\", error);\n        return null;\n      }\n    }\n    \n    async function main() {\n      const imagePath = \"Path/to/agi/image.jpeg\";\n      const base64Image = await encodeImage(imagePath);\n    \n      const messages = [\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\n              \"type\": \"text\",\n              \"text\": \"What is in this image?\",\n            },\n            {\n              \"type\": \"image_url\",\n              \"image_url\": {\n                \"url\": `data:image/jpeg;base64,${base64Image}`\n              },\n            },\n          ],\n        }\n      ];\n    \n      try {\n        const response = await openai.chat.completions.create({\n          model: \"gemini-2.0-flash\",\n          messages: messages,\n        });\n    \n        console.log(response.choices[0]);\n      } catch (error) {\n        console.error(\"Error calling Gemini API:\", error);\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    bash -c '\n      base64_image=$(base64 -i \"Path/to/agi/image.jpeg\");\n      curl \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\" \\\n        -H \"Content-Type: application/json\" \\\n        -H \"Authorization: Bearer GEMINI_API_KEY\" \\\n        -d \"{\n          \\\"model\\\": \\\"gemini-2.0-flash\\\",\n          \\\"messages\\\": [\n            {\n              \\\"role\\\": \\\"user\\\",\n              \\\"content\\\": [\n                { \\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"What is in this image?\\\" },\n                {\n                  \\\"type\\\": \\\"image_url\\\",\n                  \\\"image_url\\\": { \\\"url\\\": \\\"data:image/jpeg;base64,${base64_image}\\\" }\n                }\n              ]\n            }\n          ]\n        }\"\n    '\n    \n\n## Generate an image\n\n**Note:** Image generation is only available in the paid tier.\n\nGenerate an image:\n\n### Python\n    \n    \n    import base64\n    from openai import OpenAI\n    from PIL import Image\n    from io import BytesIO\n    \n    client = OpenAI(\n        api_key=\"GEMINI_API_KEY\",\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    )\n    \n    response = client.images.generate(\n        model=\"imagen-3.0-generate-002\",\n        prompt=\"a portrait of a sheepadoodle wearing a cape\",\n        response_format='b64_json',\n        n=1,\n    )\n    \n    for image_data in response.data:\n      image = Image.open(BytesIO(base64.b64decode(image_data.b64_json)))\n      image.show()\n    \n\n### JavaScript\n    \n    \n    import OpenAI from \"openai\";\n    \n    const openai = new OpenAI({\n      apiKey: \"GEMINI_API_KEY\",\n      baseURL: \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    });\n    \n    async function main() {\n      const image = await openai.images.generate(\n        {\n          model: \"imagen-3.0-generate-002\",\n          prompt: \"a portrait of a sheepadoodle wearing a cape\",\n          response_format: \"b64_json\",\n          n: 1,\n        }\n      );\n    \n      console.log(image.data);\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/openai/images/generations\" \\\n      -H \"Content-Type: application/json\" \\\n      -H \"Authorization: Bearer GEMINI_API_KEY\" \\\n      -d '{\n            \"model\": \"imagen-3.0-generate-002\",\n            \"prompt\": \"a portrait of a sheepadoodle wearing a cape\",\n            \"response_format\": \"b64_json\",\n            \"n\": 1,\n          }'\n    \n\n## Audio understanding\n\nAnalyze audio input:\n\n### Python\n    \n    \n    import base64\n    from openai import OpenAI\n    \n    client = OpenAI(\n        api_key=\"GEMINI_API_KEY\",\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    )\n    \n    with open(\"/path/to/your/audio/file.wav\", \"rb\") as audio_file:\n      base64_audio = base64.b64encode(audio_file.read()).decode('utf-8')\n    \n    response = client.chat.completions.create(\n        model=\"gemini-2.0-flash\",\n        messages=[\n        {\n          \"role\": \"user\",\n          \"content\": [\n            {\n              \"type\": \"text\",\n              \"text\": \"Transcribe this audio\",\n            },\n            {\n                  \"type\": \"input_audio\",\n                  \"input_audio\": {\n                    \"data\": base64_audio,\n                    \"format\": \"wav\"\n              }\n            }\n          ],\n        }\n      ],\n    )\n    \n    print(response.choices[0].message.content)\n    \n\n### JavaScript\n    \n    \n    import fs from \"fs\";\n    import OpenAI from \"openai\";\n    \n    const client = new OpenAI({\n      apiKey: \"GEMINI_API_KEY\",\n      baseURL: \"https://generativelanguage.googleapis.com/v1beta/openai/\",\n    });\n    \n    const audioFile = fs.readFileSync(\"/path/to/your/audio/file.wav\");\n    const base64Audio = Buffer.from(audioFile).toString(\"base64\");\n    \n    async function main() {\n      const response = await client.chat.completions.create({\n        model: \"gemini-2.0-flash\",\n        messages: [\n          {\n            role: \"user\",\n            content: [\n              {\n                type: \"text\",\n                text: \"Transcribe this audio\",\n              },\n              {\n                type: \"input_audio\",\n                input_audio: {\n                  data: base64Audio,\n                  format: \"wav\",\n                },\n              },\n            ],\n          },\n        ],\n      });\n    \n      console.log(response.choices[0].message.content);\n    }\n    \n    main();\n    \n\n### REST\n\n**Note:** If you get an `Argument list too long` error, the encoding of your audio file might be too long for curl.\n    \n    \n    bash -c '\n      base64_audio=$(base64 -i \"/path/to/your/audio/file.wav\");\n      curl \"https://generativelanguage.googleapis.com/v1beta/openai/chat/completions\" \\\n        -H \"Content-Type: application/json\" \\\n        -H \"Authorization: Bearer GEMINI_API_KEY\" \\\n        -d \"{\n          \\\"model\\\": \\\"gemini-2.0-flash\\\",\n          \\\"messages\\\": [\n            {\n              \\\"role\\\": \\\"user\\\",\n              \\\"content\\\": [\n                { \\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Transcribe this audio file.\\\" },\n                {\n                  \\\"type\\\": \\\"input_audio\\\",\n                  \\\"input_audio\\\": {\n                    \\\"data\\\": \\\"${base64_audio}\\\",\n                    \\\"format\\\": \\\"wav\\\"\n                  }\n                }\n              ]\n            }\n          ]\n        }\"\n    '\n    \n\n## Structured output\n\nGemini models can output JSON objects in any [structure you define](/gemini-api/docs/structured-output).\n\n### Python\n    \n    \n    from pydantic import BaseModel\n    from openai import OpenAI\n    \n    client = OpenAI(\n        api_key=\"GEMINI_API_KEY\",\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    )\n    \n    class CalendarEvent(BaseModel):\n        name: str\n        date: str\n        participants: list[str]\n    \n    completion = client.beta.chat.completions.parse(\n        model=\"gemini-2.0-flash\",\n        messages=[\n            {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n            {\"role\": \"user\", \"content\": \"John and Susan are going to an AI conference on Friday.\"},\n        ],\n        response_format=CalendarEvent,\n    )\n    \n    print(completion.choices[0].message.parsed)\n    \n\n### JavaScript\n    \n    \n    import OpenAI from \"openai\";\n    import { zodResponseFormat } from \"openai/helpers/zod\";\n    import { z } from \"zod\";\n    \n    const openai = new OpenAI({\n        apiKey: \"GEMINI_API_KEY\",\n        baseURL: \"https://generativelanguage.googleapis.com/v1beta/openai\"\n    });\n    \n    const CalendarEvent = z.object({\n      name: z.string(),\n      date: z.string(),\n      participants: z.array(z.string()),\n    });\n    \n    const completion = await openai.beta.chat.completions.parse({\n      model: \"gemini-2.0-flash\",\n      messages: [\n        { role: \"system\", content: \"Extract the event information.\" },\n        { role: \"user\", content: \"John and Susan are going to an AI conference on Friday\" },\n      ],\n      response_format: zodResponseFormat(CalendarEvent, \"event\"),\n    });\n    \n    const event = completion.choices[0].message.parsed;\n    console.log(event);\n    \n\n## Embeddings\n\nText embeddings measure the relatedness of text strings and can be generated using the [Gemini API](/gemini-api/docs/embeddings).\n\n### Python\n    \n    \n    from openai import OpenAI\n    \n    client = OpenAI(\n        api_key=\"GEMINI_API_KEY\",\n        base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    )\n    \n    response = client.embeddings.create(\n        input=\"Your text string goes here\",\n        model=\"text-embedding-004\"\n    )\n    \n    print(response.data[0].embedding)\n    \n\n### JavaScript\n    \n    \n    import OpenAI from \"openai\";\n    \n    const openai = new OpenAI({\n        apiKey: \"GEMINI_API_KEY\",\n        baseURL: \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n    });\n    \n    async function main() {\n      const embedding = await openai.embeddings.create({\n        model: \"text-embedding-004\",\n        input: \"Your text string goes here\",\n      });\n    \n      console.log(embedding);\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/openai/embeddings\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Authorization: Bearer GEMINI_API_KEY\" \\\n    -d '{\n        \"input\": \"Your text string goes here\",\n        \"model\": \"text-embedding-004\"\n      }'\n    \n\n## Current limitations\n\nSupport for the OpenAI libraries is still in beta while we extend feature support.\n\nIf you have questions about supported parameters, upcoming features, or run into any issues getting started with Gemini, join our [Developer Forum](https://discuss.ai.google.dev/c/gemini-api/4).\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-01 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-01 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/pricing",
    "title": "Gemini Developer API Pricing  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\n#  Gemini Developer API Pricing \n\nThe Gemini API \"free tier\" is offered through the API service with lower rate limits for testing purposes. Google AI Studio usage is completely free in all available countries. The Gemini API \"paid tier\" comes with [higher rate limits](/gemini-api/docs/rate-limits), additional features, and different data handling.\n\n[Upgrade to the Paid Tier](https://aistudio.google.com/apikey)\n\n## Gemini 2.5 Pro Preview\n\n[Try it in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\nOur state-of-the-art multipurpose model, which excels at coding and complex reasoning tasks.\n\nPreview models may change before becoming stable and generally available.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge, use \"gemini-2.5-pro-exp-03-25\" | $1.25, prompts <= 200k tokens  \n$2.50, prompts > 200k tokens  \nOutput price (including thinking tokens) | Free of charge, use \"gemini-2.5-pro-exp-03-25\" | $10.00, prompts <= 200k tokens  \n$15.00, prompts > 200k  \nContext caching price | Not available | Not available  \nGrounding with Google Search | Free of charge, up to 500 RPD | 1,500 RPD (free), then $35 / 1,000 requests  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Gemini 2.0 Flash\n\n[Try it in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash)\n\nOur most balanced multimodal model with great performance across all tasks, with a 1 million token context window, and built for the era of Agents.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | $0.10 (text / image / video)  \n$0.70 (audio)  \nOutput price | Free of charge | $0.40  \nContext caching price | Free of charge | $0.025 / 1,000,000 tokens (text/image/video)  \n$0.175 / 1,000,000 tokens (audio)  \nAvailable April 15, 2025  \nContext caching (storage) | Free of charge, up to 1,000,000 tokens of storage per hour  \nAvailable April 15, 2025 | $1.00 / 1,000,000 tokens per hour  \nAvailable April 15, 2025  \nTuning price | Not available | Not available  \nGrounding with Google Search | Free of charge, up to 500 RPD | 1,500 RPD (free), then $35 / 1,000 requests  \nLive API | Free of charge | **Beginning April 11, 2025:**  \nInput: $0.35 (text), $2.10 (audio / image [video])  \nOutput: $1.50 (text), $8.50 (audio)  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Gemini 2.0 Flash-Lite\n\n[Try it in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\nOur smallest and most cost effective model, built for at scale usage.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | $0.075  \nOutput price | Free of charge | $0.30  \nContext caching price | Available April 15, 2025 | Available April 15, 2025  \nContext caching (storage) | Available April 15, 2025 | Available April 15, 2025  \nTuning price | Not available | Not available  \nGrounding with Google Search | Not available | Not available  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Imagen 3\n\n[Try it in ImageFX](https://labs.google/fx/tools/image-fx)\n\nOur state-of-the-art image generation model, available to developers on the paid tier of the Gemini API.\n\n| Free Tier | Paid Tier, per Image in USD  \n---|---|---  \nImage price | Not available | $0.03  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Veo 2\n\n[Try the API](/gemini-api/docs/video)\n\nOur state-of-the-art video generation model, available to developers on the paid tier of the Gemini API.\n\n| Free Tier | Paid Tier, per second in USD  \n---|---|---  \nVideo price | Not available | $0.35  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Gemma 3\n\n[Try Gemma 3](https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it)\n\nOur lightweight, state-of the art, open model built from the same technology that powers our Gemini models.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | Not available  \nOutput price | Free of charge | Not available  \nContext caching price | Free of charge | Not available  \nContext caching (storage) | Free of charge | Not available  \nTuning price | Not available | Not available  \nGrounding with Google Search | Not available | Not available  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Gemini 1.5 Flash\n\n[Try it in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\nOur fastest multimodal model with great performance for diverse, repetitive tasks and a 1 million token context window.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | $0.075, prompts <= 128k tokens  \n$0.15, prompts > 128k tokens  \nOutput price | Free of charge | $0.30, prompts <= 128k tokens  \n$0.60, prompts > 128k tokens  \nContext caching price | Free of charge, up to 1 million tokens of storage per hour | $0.01875, prompts <= 128k tokens  \n$0.0375, prompts > 128k tokens  \nContext caching (storage) | Free of charge | $1.00 per hour  \nTuning price | Token prices are the same for tuned models  \nTuning service is free of charge. | Token prices are the same for tuned models  \nTuning service is free of charge.  \nGrounding with Google Search | Not available | $35 / 1K grounding requests (for up to 5K requests per day).  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Gemini 1.5 Flash-8B\n\n[Try it in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash-8b)\n\nOur smallest model for lower intelligence use cases, with a 1 million token context window.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | $0.0375, prompts <= 128k tokens  \n$0.075, prompts > 128k tokens  \nOutput price | Free of charge | $0.15, prompts <= 128k tokens  \n$0.30, prompts > 128k tokens  \nContext caching price | Free of charge, up to 1 million tokens of storage per hour | $0.01, prompts <= 128k tokens  \n$0.02, prompts > 128k tokens  \nContext caching (storage) | Free of charge | $0.25 per hour  \nTuning price | Token prices are the same for tuned models  \nTuning service is free of charge. | Token prices are the same for tuned models  \nTuning service is free of charge.  \nGrounding with Google Search | Not available | $35 / 1K grounding requests (for up to 5K requests per day).  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Gemini 1.5 Pro\n\n[Try it in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\nOur highest intelligence Gemini 1.5 series model, with a breakthrough 2 million token context window.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | $1.25, prompts <= 128k tokens  \n$2.50, prompts > 128k tokens  \nOutput price | Free of charge | $5.00, prompts <= 128k tokens  \n$10.00, prompts > 128k tokens  \nContext caching price | Not available | $0.3125, prompts <= 128k tokens  \n$0.625, prompts > 128k tokens  \nContext caching (storage) | Not available | $4.50 per hour  \nTuning price | Not available | Not available  \nGrounding with Google Search | Not available | $35 / 1K grounding requests (for up to 5K requests per day).  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Text Embedding 004\n\nOur state-of-the-art text embedding model.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | Not available  \nOutput price | Free of charge | Not available  \nTuning price | Not available | Not available  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n[*] Google AI Studio usage is free of charge in all [available regions](/gemini-api/docs/available-regions). See [Billing FAQs](/gemini-api/docs/billing) for details.\n\n[**] Prices may differ from the prices listed here and the prices offered on Vertex AI. For Vertex prices, see the [Vertex AI pricing page](https://cloud.google.com/vertex-ai/generative-ai/pricing).\n\n[***] If you are using [dynamic retrieval](/gemini-api/docs/grounding) to optimize costs, only requests that contain at least one grounding support URL from the web in their response are charged for Grounding with Google Search. Costs for Gemini always apply. Rate limits are subject to change.\n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-09 UTC.\n\n[[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-09 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/rate-limits",
    "title": "Rate limits  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Rate limits \n\nRate limits regulate the number of requests you can make to the Gemini API within a given timeframe. These limits help ensure fair usage, protect against abuse, and help maintain system performance for all users.\n\n## How rate limits work\n\nRate limits are measured across three dimensions:\n\n  * Requests per minute (**RPM**)\n  * Requests per day (**RPD**)\n  * Tokens per minute (**TPM**)\n\n\n\nYour usage is evaluated against each limit, and exceeding any of them will trigger a rate limit error. For example, if your RPM limit is 20, making 21 requests within a minute will result in an error, even if you haven't exceeded your TPM or other limits.\n\nRate limits are applied per project, not per API key.\n\nLimits vary depending on the specific model being used, and some limits only apply to specific models. For example, Images per minute, or IPM, is only calculated for models capable of generating images (Imagen 3), but is conceptually similar to TPM.\n\n## Usage tiers\n\nRate limits are tied to the projects usage tier (which we will soon be rolling out). As your API usage and spending increase, you'll have an option to upgrade to a higher tier with increased rate limits.\n\nTier | Qualifications  \n---|---  \nFree | Users in [eligible countries](/gemini-api/docs/available-regions)  \nTier 1 | Billing account [linked to the project](/gemini-api/docs/billing#enable-cloud-billing)  \nTier 2 | Total spend: $250 + at least 30 days since successful payment  \nTier 3 | Total spend: $1,000 + at least 30 days since successful payment  \n  \nWhen you request an upgrade, our automated abuse protection system performs additional checks. While meeting the stated qualification criteria is generally sufficient for approval, in rare cases an upgrade request may be denied based on other factors identified during the review process.\n\nThis system helps ensure the security and integrity of the Gemini API platform for all users.\n\n## Current rate limits\n\n### Free Tier\n\nModel | RPM | TPM | RPD  \n---|---|---|---  \nGemini 2.5 Pro Experimental | 5 | 1,000,000 | 25  \nGemini 2.5 Pro Preview | \\-- | \\-- | \\--  \nGemini 2.0 Flash | 15 | 1,000,000 | 1,500  \nGemini 2.0 Flash Experimental (including image generation) | 10 | 1,000,000 | 1,500  \nGemini 2.0 Flash-Lite | 30 | 1,000,000 | 1,500  \nGemini 2.0 Flash Thinking Experimental 01-21 | 10 | 4,000,000 | 1,500  \nGemini 1.5 Flash | 15 | 1,000,000 | 1,500  \nGemini 1.5 Flash-8B | 15 | 1,000,000 | 1,500  \nGemini 1.5 Pro | 2 | 32,000 | 50  \nVeo 2 | \\-- | \\-- | \\--  \nImagen 3 | \\-- | \\-- | \\--  \nGemma 3 | 30 | 15,000 | 14,400  \nGemini Embedding Experimental 03-07 | 5 | \\-- | 100  \n  \n### Tier 1\n\nModel | RPM | TPM | RPD  \n---|---|---|---  \nGemini 2.5 Pro Experimental | \\-- | \\-- | \\--  \nGemini 2.5 Pro Preview | 150 | 2,000,000 | 1,000  \nGemini 2.0 Flash | 2,000 | 4,000,000 | \\--  \nGemini 2.0 Flash Experimental (including image generation) | 10 | 4,000,000 | \\--  \nGemini 2.0 Flash-Lite | 4,000 | 4,000,000 | \\--  \nGemini 2.0 Flash Thinking Experimental 01-21 | 10 | 4,000,000 | \\--  \nGemini 1.5 Flash | 2,000 | 4,000,000 | \\--  \nGemini 1.5 Flash-8B | 4,000 | 4,000,000 | \\--  \nGemini 1.5 Pro | 1,000 | 4,000,000 | \\--  \nImagen 3 | \\-- | 20 images per minute (IPM) | \\--  \nVeo 2 | 2 | \\-- | 50  \nGemma 3 | 30 | 15,000 | 14,400  \nGemini Embedding Experimental 03-07 | 10 | \\-- | 1,000  \n  \n### Tier 2\n\nModel | RPM | TPM | RPD  \n---|---|---|---  \nGemini 2.5 Pro Experimental | \\-- | \\-- | \\--  \nGemini 2.5 Pro Preview | 1,000 | 5,000,000 | 50,000  \nGemini 2.0 Flash | 10,000 | 10,000,000 | \\--  \nGemini 2.0 Flash Experimental (including image generation) | 10 | 4,000,000 | \\--  \nGemini 2.0 Flash-Lite | 4,000 | 4,000,000 | \\--  \nGemini 2.0 Flash Thinking Experimental 01-21 | 10 | 4,000,000 | \\--  \nGemini 1.5 Flash | 2,000 | 4,000,000 | \\--  \nGemini 1.5 Flash-8B | 4,000 | 4,000,000 | \\--  \nGemini 1.5 Pro | 1,000 | 4,000,000 | \\--  \nImagen 3 | \\-- | 20 images per minute (IPM) | \\--  \nVeo 2 | \\-- | \\-- | \\--  \nGemma 3 | 30 | 15,000 | 14,400  \nGemini Embedding Experimental 03-07 | 10 | \\-- | 1,000  \n  \n### Tier 3\n\nModel | RPM | TPM | RPD  \n---|---|---|---  \nGemini 2.5 Pro Preview | 2,000 | 8,000,000 | \\--  \n  \nSpecified rate limits are not guaranteed and actual capacity may vary.\n\n### Live API rate limits\n\n### Free Tier\n\nNumber of concurrent sessions | TPM  \n---|---  \n3 | 1,000,000  \n  \n### Tier 1\n\nNumber of concurrent sessions | TPM  \n---|---  \n50 | 4,000,000  \n  \n### Tier 2\n\nNumber of concurrent sessions | TPM  \n---|---  \n1000 | 10,000,000  \n  \n### Tier 3\n\nNumber of concurrent sessions | TPM  \n---|---  \nNot yet available | Not yet available  \n  \nSpecified rate limits are not guaranteed and actual capacity may vary.\n\n## How to upgrade to the next tier\n\nThe Gemini API uses Cloud Billing for all billing services. To transition from the Free tier to a paid tier, you must first enable Cloud Billing for your Google Cloud project.\n\nOnce your project meets the specified criteria, it becomes eligible for an upgrade to the next tier. To request an upgrade, follow these steps:\n\n  * Navigate to the [API keys page](https://aistudio.google.com/app/apikey) in AI Studio.\n  * Locate the project you want to upgrade and click \"Upgrade\". The \"Upgrade\" option will only show up for projects that meet [next tier qualifications](/gemini-api/docs/rate-limits#usage-tiers).\n\n\n\nAfter a quick validation, the project will be upgraded to the next tier.\n\n## Request a rate limit increase\n\nEach model variation has an associated rate limit (requests per minute, RPM). For details on those rate limits, see [Gemini models](/models/gemini).\n\n[Request paid tier rate limit increase](https://forms.gle/ETzX94k8jf7iSotH9)\n\nWe offer no guarantees about increasing your rate limit, but we'll do our best to review your request and reach out to you if we're able to accommodate your capacity needs.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/billing",
    "title": "Billing  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Billing \n\nThis guide provides an overview of different Gemini API billing options, explains how to enable billing and monitor usage, and provides answers to frequently asked questions (FAQs) about billing.\n\n[Upgrade to the Gemini API paid tier](https://aistudio.google.com/plan_information)\n\n## About billing\n\nBilling for the Gemini API is based on two pricing tiers: _free of charge_ (or _free_) and _pay-as-you-go_ (or _paid_). Pricing and rate limits differ between these tiers and also vary by model. You can check out the [rate limits](/gemini-api/docs/rate-limits) and [pricing](/gemini-api/docs/pricing) pages for more into. For a model-by-model breakdown of capabilities, see the [Gemini models page](/gemini-api/docs/models/gemini).\n\n#### How to request an upgrade\n\nThe Gemini API uses Cloud Billing for all billing services. To transition from the Free tier to a paid tier, you must first enable Cloud Billing for your Google Cloud project.\n\nWhen your project meets the specified criteria, it becomes eligible for an upgrade to the next tier. To request an upgrade, follow these steps:\n\n  1. Go to the [AI Studio API keys page](https://aistudio.google.com/app/apikey).\n  2. Find the project you want to upgrade and click **Upgrade**.\n  3. The system will automatically verify your project's eligibility. This process takes a few seconds.\n  4. If your project meets all the requirements, it will be instantly upgraded to the next tier.\n\n\n\n### Why use the paid tier?\n\nWhen you enable billing and use the paid tier, you benefit from [higher rate limits](/gemini-api/docs/rate-limits), and your prompts and responses aren't used to improve Google products. For more information on data use for paid services, see the [terms of service](/gemini-api/terms#data-use-paid).\n\n### Cloud Billing\n\nThe Gemini API uses [Cloud Billing](https://cloud.google.com/billing/docs/concepts) for billing services. To use the paid tier, you must set up Cloud Billing on your cloud project. After you've enabled Cloud Billing, you can use Cloud Billing tools to track spending, understand costs, make payments, and access Cloud Billing support.\n\n## Enable billing\n\nYou can enable Cloud Billing starting from Google AI Studio:\n\n  1. Open [Google AI Studio](https://aistudio.google.com/).\n\n  2. In the bottom of the left sidebar, select **Settings** > **Plan information**.\n\n  3. Click **Set up Billing** for your chosen project to enable Cloud Billing.\n\n\n\n\n## Monitor usage\n\nAfter you enable Cloud Billing, you can monitor your usage of the Gemini API in the [Google Cloud console](https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com). The service name for the API is `generativelanguage.googleapis.com`, and in the console the Gemini API is also referred to as the **Generative Language API**.\n\nTo learn more, see the Google Cloud documentation on [monitoring API usage](https://cloud.google.com/apis/docs/monitoring).\n\n## Frequently asked questions\n\nThis section provides answers to frequently asked questions.\n\n### What am I billed for?\n\nGemini API pricing is based on the following:\n\n  * Input token count\n  * Output token count\n  * Cached token count\n  * Cached token storage duration\n\n\n\nFor pricing information, see the [pricing page](/pricing).\n\n### Where can I view my quota?\n\nYou can view your quota and system limits in the [Google Cloud console](https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/quotas).\n\n### How do I request more quota?\n\nTo request more quota, follow the instructions at How to request an upgrade.\n\n### Can I use the Gemini API for free in EEA (including EU), the UK, and CH?\n\nYes, we make the free tier and paid tier available in [many regions](/gemini-api/docs/available-regions).\n\n### If I set up billing with the Gemini API, will I be charged for my Google AI Studio usage?\n\nNo, Google AI Studio usage remains free of charge regardless of if you set up billing across all supported regions.\n\n### Can I use 1M tokens in the free tier?\n\nThe free tier for Gemini API differs based on the model selected. For now, you can try the 1M token context window in the following ways:\n\n  * In Google AI Studio\n  * With pay-as-you-go plans\n  * With free-of-charge plans for select models\n\n\n\nSee the latest free-of-charge rate limits per model on [rate limits page](/gemini-api/docs/rate-limits).\n\n### How can I calculate the number of tokens I'm using?\n\nUse the [`GenerativeModel.count_tokens`](/api/python/google/generativeai/GenerativeModel#count_tokens) method to count the number of tokens. Refer to the [Tokens guide](/gemini-api/docs/tokens) to learn more about tokens.\n\n### Can I use my Google Cloud credits with the Gemini API?\n\nYes, Google Cloud credits can be used towards Gemini API usage.\n\n### How is billing handled?\n\nBilling for the Gemini API is handled by the [Cloud Billing](https://cloud.google.com/billing/docs/concepts) system.\n\n### Am I charged for failed requests?\n\nIf your request fails with a 400 or 500 error, you won't be charged for the tokens used. However, the request will still count against your quota.\n\n### Is there a charge for fine-tuning the models?\n\n[Model tuning](/gemini-api/docs/model-tuning) is free, but inference on tuned models is charged at the same rate as the base models.\n\n### Is GetTokens billed?\n\nRequests to the GetTokens API are not billed, and they don't count against inference quota.\n\n### How is my Google AI Studio data handled if I have a paid API account?\n\nRefer to the [terms](/gemini-api/terms#paid-services) for details on how data is handled when Cloud billing is enabled (see \"How Google Uses Your Data\" under \"Paid Services\"). Note that your Google AI Studio prompts are treated under the same \"Paid Services\" terms so long as at least 1 API project has billing enabled, which you can validate on the [Gemini API Key page](https://aistudio.google.com/apikey) if you see any projects marked as \"Paid\" under \"Plan\".\n\n### Where can I get help with billing?\n\nTo get help with billing, see [Get Cloud Billing support](https://cloud.google.com/support/billing).\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-03-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-03-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/text-generation",
    "title": "Text generation  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Text generation \n\nThe Gemini API can generate text output in response to various inputs, including text, images, video, and audio. This guide shows you how to generate text using text and image inputs. It also covers streaming, chat, and system instructions.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Text input\n\nThe simplest way to generate text using the Gemini API is to provide the model with a single text-only input, as shown in this example:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[\"How does AI work?\"]\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"How does AI work?\",\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    // import packages here\n    \n    func main() {\n      ctx := context.Background()\n      client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n      if err != nil {\n        log.Fatal(err)\n      }\n      defer client.Close()\n    \n      model := client.GenerativeModel(\"gemini-2.0-flash\")\n      resp, err := model.GenerateContent(ctx, genai.Text(\"How does AI work?\"))\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp) // helper function for printing content parts\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"How does AI work?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { text: 'How AI does work?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Image input\n\nThe Gemini API supports multimodal inputs that combine text and media files. The following example shows how to generate text from text and image input:\n\n### Python\n    \n    \n    from PIL import Image\n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    image = Image.open(\"/path/to/organ.png\")\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[image, \"Tell me about this instrument\"]\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import {\n      GoogleGenAI,\n      createUserContent,\n      createPartFromUri,\n    } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const image = await ai.files.upload({\n        file: \"/path/to/organ.png\",\n      });\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: [\n          createUserContent([\n            \"Tell me about this instrument\",\n            createPartFromUri(image.uri, image.mimeType),\n          ]),\n        ],\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-2.0-flash\")\n    \n    imgData, err := os.ReadFile(filepath.Join(testDataDir, \"organ.jpg\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    \n    resp, err := model.GenerateContent(ctx,\n      genai.Text(\"Tell me about this instrument\"),\n      genai.ImageData(\"jpeg\", imgData))\n    if err != nil {\n      log.Fatal(err)\n    }\n    \n    printResponse(resp)\n    \n\n### REST\n    \n    \n    # Use a temporary file to hold the base64 encoded image data\n    TEMP_B64=$(mktemp)\n    trap 'rm -f \"$TEMP_B64\"' EXIT\n    base64 $B64FLAGS $IMG_PATH > \"$TEMP_B64\"\n    \n    # Use a temporary file to hold the JSON payload\n    TEMP_JSON=$(mktemp)\n    trap 'rm -f \"$TEMP_JSON\"' EXIT\n    \n    cat > \"$TEMP_JSON\" << EOF\n    {\n      \"contents\": [\n        {\n          \"parts\": [\n            {\n              \"text\": \"Tell me about this instrument\"\n            },\n            {\n              \"inline_data\": {\n                \"mime_type\": \"image/jpeg\",\n                \"data\": \"$(cat \"$TEMP_B64\")\"\n              }\n            }\n          ]\n        }\n      ]\n    }\n    EOF\n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d \"@$TEMP_JSON\"\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const imageUrl = 'http://image/url';\n      const image = getImageData(imageUrl);\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { image },\n              { text: 'Tell me about this instrument' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n    function getImageData(url) {\n      const blob = UrlFetchApp.fetch(url).getBlob();\n    \n      return {\n        mimeType: blob.getContentType(),\n        data: Utilities.base64Encode(blob.getBytes())\n      };\n    }\n    \n\n## Streaming output\n\nBy default, the model returns a response after completing the entire text generation process. You can achieve faster interactions by using streaming to return instances of [`GenerateContentResponse`](/api/generate-content#v1beta.GenerateContentResponse) as they're generated.\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content_stream(\n        model=\"gemini-2.0-flash\",\n        contents=[\"Explain how AI works\"]\n    )\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContentStream({\n        model: \"gemini-2.0-flash\",\n        contents: \"Explain how AI works\",\n      });\n    \n      for await (const chunk of response) {\n        console.log(chunk.text);\n      }\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    iter := model.GenerateContentStream(ctx, genai.Text(\"Write a story about a magic backpack.\"))\n    for {\n      resp, err := iter.Next()\n      if err == iterator.Done {\n        break\n      }\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp)\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse&key=${GEMINI_API_KEY}\" \\\n      -H 'Content-Type: application/json' \\\n      --no-buffer \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Explain how AI works\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { text: 'Explain how AI works' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Multi-turn conversations\n\nThe Gemini SDK lets you collect multiple rounds of questions and responses into a chat. The chat format enables users to step incrementally toward answers and to get help with multipart problems. This SDK implementation of chat provides an interface to keep track of conversation history, but behind the scenes it uses the same [`generateContent`](/api/generate-content#method:-models.generatecontent) method to create the response.\n\nThe following code example shows a basic chat implementation:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    chat = client.chats.create(model=\"gemini-2.0-flash\")\n    \n    response = chat.send_message(\"I have 2 dogs in my house.\")\n    print(response.text)\n    \n    response = chat.send_message(\"How many paws are in my house?\")\n    print(response.text)\n    \n    for message in chat.get_history():\n        print(f'role - {message.role}',end=\": \")\n        print(message.parts[0].text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const chat = ai.chats.create({\n        model: \"gemini-2.0-flash\",\n        history: [\n          {\n            role: \"user\",\n            parts: [{ text: \"Hello\" }],\n          },\n          {\n            role: \"model\",\n            parts: [{ text: \"Great to meet you. What would you like to know?\" }],\n          },\n        ],\n      });\n    \n      const response1 = await chat.sendMessage({\n        message: \"I have 2 dogs in my house.\",\n      });\n      console.log(\"Chat response 1:\", response1.text);\n    \n      const response2 = await chat.sendMessage({\n        message: \"How many paws are in my house?\",\n      });\n      console.log(\"Chat response 2:\", response2.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    cs := model.StartChat()\n    \n    cs.History = []*genai.Content{\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Hello, I have 2 dogs in my house.\"),\n        },\n        Role: \"user\",\n      },\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Great to meet you. What would you like to know?\"),\n        },\n        Role: \"model\",\n      },\n    }\n    \n    res, err := cs.SendMessage(ctx, genai.Text(\"How many paws are in my house?\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    printResponse(res)\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"Hello\"\n              }\n            ]\n          },\n          {\n            \"role\": \"model\",\n            \"parts\": [\n              {\n                \"text\": \"Great to meet you. What would you like to know?\"\n              }\n            ]\n          },\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"I have two dogs in my house. How many paws are in my house?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            role: 'user',\n            parts: [\n              { text: 'Hello' },\n            ],\n          },\n          {\n            role: 'model',\n            parts: [\n              { text: 'Great to meet you. What would you like to know?' },\n            ],\n          },\n          {\n            role: 'user',\n            parts: [\n              { text: 'I have two dogs in my house. How many paws are in my house?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nYou can also use streaming with chat, as shown in the following example:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    chat = client.chats.create(model=\"gemini-2.0-flash\")\n    \n    response = chat.send_message_stream(\"I have 2 dogs in my house.\")\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n    response = chat.send_message_stream(\"How many paws are in my house?\")\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n    for message in chat.get_history():\n        print(f'role - {message.role}', end=\": \")\n        print(message.parts[0].text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const chat = ai.chats.create({\n        model: \"gemini-2.0-flash\",\n        history: [\n          {\n            role: \"user\",\n            parts: [{ text: \"Hello\" }],\n          },\n          {\n            role: \"model\",\n            parts: [{ text: \"Great to meet you. What would you like to know?\" }],\n          },\n        ],\n      });\n    \n      const stream1 = await chat.sendMessageStream({\n        message: \"I have 2 dogs in my house.\",\n      });\n      for await (const chunk of stream1) {\n        console.log(chunk.text);\n        console.log(\"_\".repeat(80));\n      }\n    \n      const stream2 = await chat.sendMessageStream({\n        message: \"How many paws are in my house?\",\n      });\n      for await (const chunk of stream2) {\n        console.log(chunk.text);\n        console.log(\"_\".repeat(80));\n      }\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    cs := model.StartChat()\n    \n    cs.History = []*genai.Content{\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Hello, I have 2 dogs in my house.\"),\n        },\n        Role: \"user\",\n      },\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Great to meet you. What would you like to know?\"),\n        },\n        Role: \"model\",\n      },\n    }\n    \n    iter := cs.SendMessageStream(ctx, genai.Text(\"How many paws are in my house?\"))\n    for {\n      resp, err := iter.Next()\n      if err == iterator.Done {\n        break\n      }\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp)\n    }\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse&key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"Hello\"\n              }\n            ]\n          },\n          {\n            \"role\": \"model\",\n            \"parts\": [\n              {\n                \"text\": \"Great to meet you. What would you like to know?\"\n              }\n            ]\n          },\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"I have two dogs in my house. How many paws are in my house?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            role: 'user',\n            parts: [\n              { text: 'Hello' },\n            ],\n          },\n          {\n            role: 'model',\n            parts: [\n              { text: 'Great to meet you. What would you like to know?' },\n            ],\n          },\n          {\n            role: 'user',\n            parts: [\n              { text: 'I have two dogs in my house. How many paws are in my house?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Configuration parameters\n\nEvery prompt you send to the model includes parameters that control how the model generates responses. You can configure these parameters, or let the model use the default options.\n\nThe following example shows how to configure model parameters:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[\"Explain how AI works\"],\n        config=types.GenerateContentConfig(\n            max_output_tokens=500,\n            temperature=0.1\n        )\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Explain how AI works\",\n        config: {\n          maxOutputTokens: 500,\n          temperature: 0.1,\n        },\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-pro-latest\")\n    model.SetTemperature(0.9)\n    model.SetTopP(0.5)\n    model.SetTopK(20)\n    model.SetMaxOutputTokens(100)\n    model.SystemInstruction = genai.NewUserContent(genai.Text(\"You are Yoda from Star Wars.\"))\n    model.ResponseMIMEType = \"application/json\"\n    resp, err := model.GenerateContent(ctx, genai.Text(\"What is the average size of a swallow?\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    printResponse(resp)\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Explain how AI works\"\n              }\n            ]\n          }\n        ],\n        \"generationConfig\": {\n          \"stopSequences\": [\n            \"Title\"\n          ],\n          \"temperature\": 1.0,\n          \"maxOutputTokens\": 800,\n          \"topP\": 0.8,\n          \"topK\": 10\n        }\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const generationConfig = {\n        temperature: 1,\n        topP: 0.95,\n        topK: 40,\n        maxOutputTokens: 8192,\n        responseMimeType: 'text/plain',\n      };\n    \n      const payload = {\n        generationConfig,\n        contents: [\n          {\n            parts: [\n              { text: 'Explain how AI works in a few words' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nHere are some of the model parameters you can configure. (Naming conventions vary by programming language.)\n\n  * `stopSequences`: Specifies the set of character sequences (up to 5) that will stop output generation. If specified, the API will stop at the first appearance of a `stop_sequence`. The stop sequence won't be included as part of the response.\n  * `temperature`: Controls the randomness of the output. Use higher values for more creative responses, and lower values for more deterministic responses. Values can range from [0.0, 2.0].\n  * `maxOutputTokens`: Sets the maximum number of tokens to include in a candidate.\n  * `topP`: Changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the `topP` value. The default `topP` value is 0.95.\n  * `topK`: Changes how the model selects tokens for output. A `topK` of 1 means the selected token is the most probable among all the tokens in the model's vocabulary, while a `topK` of 3 means that the next token is selected from among the 3 most probable using the temperature. Tokens are further filtered based on `topP` with the final token selected using temperature sampling.\n\n\n\n## System instructions\n\nSystem instructions let you steer the behavior of a model based on your specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full interaction with the user, enabling you to specify product-level behavior separate from the prompts provided by end users.\n\nYou can set system instructions when you initialize your model:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=types.GenerateContentConfig(\n            system_instruction=\"You are a cat. Your name is Neko.\"),\n        contents=\"Hello there\"\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Hello there\",\n        config: {\n          systemInstruction: \"You are a cat. Your name is Neko.\",\n        },\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    // import packages here\n    \n    func main() {\n      ctx := context.Background()\n      client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n      if err != nil {\n        log.Fatal(err)\n      }\n      defer client.Close()\n    \n      model := client.GenerativeModel(\"gemini-2.0-flash\")\n      model.SystemInstruction = &genai.Content{\n        Parts: []genai.Part{genai.Text(`\n          You are a cat. Your name is Neko.\n        `)},\n      }\n      resp, err := model.GenerateContent(ctx, genai.Text(\"Hello there\"))\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp) // helper function for printing content parts\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -d '{\n        \"system_instruction\": {\n          \"parts\": [\n            {\n              \"text\": \"You are a cat. Your name is Neko.\"\n            }\n          ]\n        },\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Hello there\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const systemInstruction = {\n        parts: [{\n          text: 'You are a cat. Your name is Neko.'\n        }]\n      };\n    \n      const payload = {\n        systemInstruction,\n        contents: [\n          {\n            parts: [\n              { text: 'Hello there' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nThen, you can send requests to the model as usual.\n\n## Supported models\n\nThe entire Gemini family of models supports text generation. To learn more about the models and their capabilities, see [Models](/gemini-api/docs/models).\n\n## Prompting tips\n\nFor basic text generation use cases, your prompt might not need to include any output examples, system instructions, or formatting information. This is a [zero-shot](/gemini-api/docs/models/generative-models#zero-shot-prompts) approach. For some use cases, a [one-shot](/gemini-api/docs/models/generative-models#one-shot-prompts) or [few-shot](/gemini-api/docs/models/generative-models#few-shot-prompts) prompt might produce output that's more aligned with user expectations. In some cases, you might also want to provide system instructions to help the model understand the task or follow specific guidelines.\n\n## What's next\n\n  * Try the [Gemini API getting started Colab](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb).\n  * Learn how to use Gemini's [vision understanding](/gemini-api/docs/vision) to process images and videos.\n  * Learn how to use Gemini's [audio understanding](/gemini-api/docs/audio) to process audio files.\n  * Learn about multimodal [file prompting strategies](/gemini-api/docs/file-prompting-strategies).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/image-generation",
    "title": "Generate images  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Generate images \n\nThe Gemini API supports image generation using Gemini 2.0 Flash Experimental and using Imagen 3. This guide helps you get started with both models.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Generate images using Gemini\n\nGemini 2.0 Flash Experimental supports the ability to output text and inline images. This lets you use Gemini to conversationally edit images or generate outputs with interwoven text (for example, generating a blog post with text and images in a single turn). All generated images include a [SynthID watermark](/responsible/docs/safeguards/synthid), and images in Google AI Studio include a visible watermark as well.\n\n**Note:** Make sure to include `responseModalities`: [\"Text\", \"Image\"] in your generation configuration for text and image output with `gemini-2.0-flash-exp-image-generation`. Image only is not allowed.\n\nThe following example shows how to use Gemini 2.0 to generate text-and-image output:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    from PIL import Image\n    from io import BytesIO\n    import base64\n    \n    client = genai.Client()\n    \n    contents = ('Hi, can you create a 3d rendered image of a pig '\n                'with wings and a top hat flying over a happy '\n                'futuristic scifi city with lots of greenery?')\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash-exp-image-generation\",\n        contents=contents,\n        config=types.GenerateContentConfig(\n          response_modalities=['Text', 'Image']\n        )\n    )\n    \n    for part in response.candidates[0].content.parts:\n      if part.text is not None:\n        print(part.text)\n      elif part.inline_data is not None:\n        image = Image.open(BytesIO((part.inline_data.data)))\n        image.save('gemini-native-image.png')\n        image.show()\n    \n\n### JavaScript\n\n**Note:** We've released the [Google SDK for TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai) in [preview launch stage](https://github.com/googleapis/js-genai?tab=readme-ov-file#preview-launch). Use this SDK for image generation features.\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    import * as fs from \"node:fs\";\n    \n    async function main() {\n    \n      const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n      const contents =\n        \"Hi, can you create a 3d rendered image of a pig \" +\n        \"with wings and a top hat flying over a happy \" +\n        \"futuristic scifi city with lots of greenery?\";\n    \n      // Set responseModalities to include \"Image\" so the model can generate  an image\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash-exp-image-generation\",\n        contents: contents,\n        config: {\n          responseModalities: [\"Text\", \"Image\"],\n        },\n      });\n      for (const part of response.candidates[0].content.parts) {\n        // Based on the part type, either show the text or save the image\n        if (part.text) {\n          console.log(part.text);\n        } else if (part.inlineData) {\n          const imageData = part.inlineData.data;\n          const buffer = Buffer.from(imageData, \"base64\");\n          fs.writeFileSync(\"gemini-native-image.png\", buffer);\n          console.log(\"Image saved as gemini-native-image.png\");\n        }\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl -s -X POST \\\n      \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\n        \"contents\": [{\n          \"parts\": [\n            {\"text\": \"Hi, can you create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?\"}\n          ]\n        }],\n        \"generationConfig\":{\"responseModalities\":[\"Text\",\"Image\"]}\n      }' \\\n      | grep -o '\"data\": \"[^\"]*\"' \\\n      | cut -d'\"' -f4 \\\n      | base64 --decode > gemini-native-image.png\n    \n\nAI-generated image of a fantastical flying pig\n\nDepending on the prompt and context, Gemini will generate content in different modes (text to image, text to image and text, etc.). Here are some examples:\n\n  * Text to image \n    * **Example prompt:** \"Generate an image of the Eiffel tower with fireworks in the background.\"\n  * Text to image(s) and text (interleaved) \n    * **Example prompt:** \"Generate an illustrated recipe for a paella.\"\n  * Image(s) and text to image(s) and text (interleaved) \n    * **Example prompt:** (With an image of a furnished room) \"What other color sofas would work in my space? can you update the image?\"\n  * Image editing (text and image to image) \n    * **Example prompt:** \"Edit this image to make it look like a cartoon\"\n    * **Example prompt:** [image of a cat] + [image of a pillow] + \"Create a cross stitch of my cat on this pillow.\"\n  * Multi-turn image editing (chat) \n    * **Example prompts:** [upload an image of a blue car.] \"Turn this car into a convertible.\" \"Now change the color to yellow.\"\n\n\n\n### Image editing with Gemini\n\nTo perform image editing, add an image as input. The following example demonstrats uploading base64 encoded images. For multiple images and larger payloads, check the [image input](/gemini-api/docs/vision#image-input) section.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    from PIL import Image\n    from io import BytesIO\n    \n    import PIL.Image\n    \n    image = PIL.Image.open('/path/to/image.png')\n    \n    client = genai.Client()\n    \n    text_input = ('Hi, This is a picture of me.'\n                'Can you add a llama next to me?',)\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash-exp-image-generation\",\n        contents=[text_input, image],\n        config=types.GenerateContentConfig(\n          response_modalities=['Text', 'Image']\n        )\n    )\n    \n    for part in response.candidates[0].content.parts:\n      if part.text is not None:\n        print(part.text)\n      elif part.inline_data is not None:\n        image = Image.open(BytesIO(part.inline_data.data))\n        image.show()\n    \n\n### JavaScript\n\n**Note:** We've released the [Google SDK for TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai) in [preview launch stage](https://github.com/googleapis/js-genai?tab=readme-ov-file#preview-launch). Use this SDK for image generation features.\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    import * as fs from \"node:fs\";\n    \n    async function main() {\n    \n      const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n      // Load the image from the local file system\n      const imagePath = \"path/to/image.png\";\n      const imageData = fs.readFileSync(imagePath);\n      const base64Image = imageData.toString(\"base64\");\n    \n      // Prepare the content parts\n      const contents = [\n        { text: \"Can you add a llama next to the image?\" },\n        {\n          inlineData: {\n            mimeType: \"image/png\",\n            data: base64Image,\n          },\n        },\n      ];\n    \n      // Set responseModalities to include \"Image\" so the model can generate an image\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash-exp-image-generation\",\n        contents: contents,\n        config: {\n          responseModalities: [\"Text\", \"Image\"],\n        },\n      });\n      for (const part of response.candidates[0].content.parts) {\n        // Based on the part type, either show the text or save the image\n        if (part.text) {\n          console.log(part.text);\n        } else if (part.inlineData) {\n          const imageData = part.inlineData.data;\n          const buffer = Buffer.from(imageData, \"base64\");\n          fs.writeFileSync(\"gemini-native-image.png\", buffer);\n          console.log(\"Image saved as gemini-native-image.png\");\n        }\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    IMG_PATH=/path/to/your/image1.jpeg\n    \n    if [[ \"$(base64 --version 2>&1)\" = *\"FreeBSD\"* ]]; then\n      B64FLAGS=\"--input\"\n    else\n      B64FLAGS=\"-w0\"\n    fi\n    \n    IMG_BASE64=$(base64 \"$B64FLAGS\" \"$IMG_PATH\" 2>&1)\n    \n    curl -X POST \\\n      \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY\" \\\n        -H 'Content-Type: application/json' \\\n        -d \"{\n          \\\"contents\\\": [{\n            \\\"parts\\\":[\n                {\\\"text\\\": \\\"'Hi, This is a picture of me. Can you add a llama next to me\\\"},\n                {\n                  \\\"inline_data\\\": {\n                    \\\"mime_type\\\":\\\"image/jpeg\\\",\n                    \\\"data\\\": \\\"$IMG_BASE64\\\"\n                  }\n                }\n            ]\n          }],\n          \\\"generationConfig\\\": {\\\"responseModalities\\\": [\\\"Text\\\", \\\"Image\\\"]}\n        }\"  \\\n      | grep -o '\"data\": \"[^\"]*\"' \\\n      | cut -d'\"' -f4 \\\n      | base64 --decode > gemini-edited-image.png\n    \n\n### Limitations\n\n  * For best performance, use the following languages: EN, es-MX, ja-JP, zh-CN, hi-IN.\n  * Image generation does not support audio or video inputs.\n  * Image generation may not always trigger: \n    * The model may output text only. Try asking for image outputs explicitly (e.g. \"generate an image\", \"provide images as you go along\", \"update the image\").\n    * The model may stop generating partway through. Try again or try a different prompt.\n  * When generating text for an image, Gemini works best if you first generate the text and then ask for an image with the text.\n\n\n\n## Choose a model\n\nWhich model should you use to generate images? It depends on your use case.\n\nGemini 2.0 is best for producing contextually relevant images, blending text + images, incorporating world knowledge, and reasoning about images. You can use it to create accurate, contextually relevant visuals embedded in long text sequences. You can also edit images conversationally, using natural language, while maintaining context throughout the conversation.\n\nIf image quality is your top priority, then Imagen 3 is a better choice. Imagen 3 excels at photorealism, artistic detail, and specific artistic styles like impressionism or anime. Imagen 3 is also a good choice for specialized image editing tasks like updating product backgrounds, upscaling images, and infusing branding and style into visuals. You can use Imagen 3 to create logos or other branded product designs.\n\n## Generate images using Imagen 3\n\nThe Gemini API provides access to [Imagen 3](https://deepmind.google/technologies/imagen-3/), Google's highest quality text-to-image model, featuring a number of new and improved capabilities. Imagen 3 can do the following:\n\n  * Generate images with better detail, richer lighting, and fewer distracting artifacts than previous models\n  * Understand prompts written in natural language\n  * Generate images in a wide range of formats and styles\n  * Render text more effectively than previous models\n\n**Note:** Imagen 3 is only available on the [Paid Tier](/gemini-api/docs/pricing) and always includes a [SynthID](https://deepmind.google/technologies/synthid/) watermark.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    from PIL import Image\n    from io import BytesIO\n    \n    client = genai.Client(api_key='GEMINI_API_KEY')\n    \n    response = client.models.generate_images(\n        model='imagen-3.0-generate-002',\n        prompt='Robot holding a red skateboard',\n        config=types.GenerateImagesConfig(\n            number_of_images= 4,\n        )\n    )\n    for generated_image in response.generated_images:\n      image = Image.open(BytesIO(generated_image.image.image_bytes))\n      image.show()\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    import * as fs from \"node:fs\";\n    \n    async function main() {\n    \n      const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n      const response = await ai.models.generateImages({\n        model: 'imagen-3.0-generate-002',\n        prompt: 'Robot holding a red skateboard',\n        config: {\n          numberOfImages: 4,\n        },\n      });\n    \n      let idx = 1;\n      for (const generatedImage of response.generatedImages) {\n        let imgBytes = generatedImage.image.imageBytes;\n        const buffer = Buffer.from(imgBytes, \"base64\");\n        fs.writeFileSync(`imagen-${idx}.png`, buffer);\n        idx++;\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl -X POST \\\n        \"https://generativelanguage.googleapis.com/v1beta/models/imagen-3.0-generate-002:predict?key=GEMINI_API_KEY\" \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\n            \"instances\": [\n              {\n                \"prompt\": \"Robot holding a red skateboard\"\n              }\n            ],\n            \"parameters\": {\n              \"sampleCount\": 4\n            }\n          }'\n    \n\nAI-generated image of two fuzzy bunnies in the kitchen\n\nImagen supports English only prompts at this time and the following parameters:\n\n### Imagen model parameters\n\n(Naming conventions vary by programming language.)\n\n  * `numberOfImages`: The number of images to generate, from 1 to 4 (inclusive). The default is 4.\n  * `aspectRatio`: Changes the aspect ratio of the generated image. Supported values are `\"1:1\"`, `\"3:4\"`, `\"4:3\"`, `\"9:16\"`, and `\"16:9\"`. The default is `\"1:1\"`.\n  * `personGeneration`: Allow the model to generate images of people. The following values are supported: \n    * `\"DONT_ALLOW\"`: Block generation of images of people.\n    * `\"ALLOW_ADULT\"`: Generate images of adults, but not children. This is the default.\n\n\n\n## What's next\n\n  * To learn more about prompt writing for Imagen, see the [Imagen prompt guide](/gemini-api/docs/imagen-prompt-guide).\n  * To learn more about Gemini 2.0 models, see [Gemini models](/gemini-api/docs/models/gemini) and [Experimental models](/gemini-api/docs/models/experimental-models).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/vision",
    "title": "Explore vision capabilities with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Explore vision capabilities with the Gemini API \n\nPython Node.js Go REST\n\nGemini models are able to process images and videos, enabling many frontier developer use cases that would have historically required domain specific models. Some of Gemini's vision capabilities include the ability to:\n\n  * Caption and answer questions about images\n  * Transcribe and reason over PDFs, including up to 2 million tokens\n  * Describe, segment, and extract information from videos up to 90 minutes long\n  * Detect objects in an image and return bounding box coordinates for them\n\n\n\nGemini was built to be multimodal from the ground up and we continue to push the frontier of what is possible.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## What's next\n\nThis guide shows how to upload image and video files using the File API and then generate text outputs from image and video inputs. To learn more, see the following resources:\n\n  * [File prompting strategies](/gemini-api/docs/file-prompting-strategies): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.\n  * [System instructions](/gemini-api/docs/system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.\n  * [Safety guidance](/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/audio",
    "title": "Audio understanding  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Audio understanding \n\nGemini can analyze and understand audio input, enabling use cases like the following:\n\n  * Describe, summarize, or answer questions about audio content.\n  * Provide a transcription of the audio.\n  * Analyze specific segments of the audio.\n\n\n\nThis guide shows you how to use the Gemini API to generate a text response to audio input.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Input audio\n\nYou can provide audio data to Gemini in the following ways:\n\n  * Upload an audio file before making a request to `generateContent`.\n  * Pass inline audio data with the request to `generateContent`.\n\n\n\n### Upload an audio file\n\nYou can use the [Files API](/gemini-api/docs/files) to upload an audio file. Always use the Files API when the total request size (including the files, text prompt, system instructions, etc.) is larger than 20 MB.\n\nThe following code uploads an audio file and then uses the file in a call to `generateContent`.\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GOOGLE_API_KEY\")\n    \n    myfile = client.files.upload(file=\"path/to/sample.mp3\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\", contents=[\"Describe this audio clip\", myfile]\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import {\n      GoogleGenAI,\n      createUserContent,\n      createPartFromUri,\n    } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    \n    async function main() {\n      const myfile = await ai.files.upload({\n        file: \"path/to/sample.mp3\",\n        config: { mimeType: \"audio/mp3\" },\n      });\n    \n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: createUserContent([\n          createPartFromUri(myfile.uri, myfile.mimeType),\n          \"Describe this audio clip\",\n        ]),\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    file, err := client.UploadFileFromPath(ctx, \"path/to/sample.mp3\", nil)\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer client.DeleteFile(ctx, file.Name)\n    \n    model := client.GenerativeModel(\"gemini-2.0-flash\")\n    resp, err := model.GenerateContent(ctx,\n        genai.FileData{URI: file.URI},\n        genai.Text(\"Describe this audio clip\"))\n    if err != nil {\n        log.Fatal(err)\n    }\n    \n    printResponse(resp)\n    \n\n### REST\n    \n    \n    AUDIO_PATH=\"path/to/sample.mp3\"\n    MIME_TYPE=$(file -b --mime-type \"${AUDIO_PATH}\")\n    NUM_BYTES=$(wc -c < \"${AUDIO_PATH}\")\n    DISPLAY_NAME=AUDIO\n    \n    tmp_header_file=upload-header.tmp\n    \n    # Initial resumable request defining metadata.\n    # The upload url is in the response headers dump them to a file.\n    curl \"https://generativelanguage.googleapis.com/upload/v1beta/files?key=${GOOGLE_API_KEY}\" \\\n      -D upload-header.tmp \\\n      -H \"X-Goog-Upload-Protocol: resumable\" \\\n      -H \"X-Goog-Upload-Command: start\" \\\n      -H \"X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}\" \\\n      -H \"X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}\" \\\n      -H \"Content-Type: application/json\" \\\n      -d \"{'file': {'display_name': '${DISPLAY_NAME}'}}\" 2> /dev/null\n    \n    upload_url=$(grep -i \"x-goog-upload-url: \" \"${tmp_header_file}\" | cut -d\" \" -f2 | tr -d \"\\r\")\n    rm \"${tmp_header_file}\"\n    \n    # Upload the actual bytes.\n    curl \"${upload_url}\" \\\n      -H \"Content-Length: ${NUM_BYTES}\" \\\n      -H \"X-Goog-Upload-Offset: 0\" \\\n      -H \"X-Goog-Upload-Command: upload, finalize\" \\\n      --data-binary \"@${AUDIO_PATH}\" 2> /dev/null > file_info.json\n    \n    file_uri=$(jq \".file.uri\" file_info.json)\n    echo file_uri=$file_uri\n    \n    # Now generate content using that file\n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY\" \\\n        -H 'Content-Type: application/json' \\\n        -X POST \\\n        -d '{\n          \"contents\": [{\n            \"parts\":[\n              {\"text\": \"Describe this audio clip\"},\n              {\"file_data\":{\"mime_type\": \"${MIME_TYPE}\", \"file_uri\": '$file_uri'}}]\n            }]\n          }' 2> /dev/null > response.json\n    \n    cat response.json\n    echo\n    \n    jq \".candidates[].content.parts[].text\" response.json\n    \n\nTo learn more about working with media files, see [Files API](/gemini-api/docs/files).\n\n### Pass audio data inline\n\nInstead of uploading an audio file, you can pass inline audio data in the request to `generateContent`:\n\n### Python\n    \n    \n    from google.genai import types\n    \n    with open('path/to/small-sample.mp3', 'rb') as f:\n        audio_bytes = f.read()\n    \n    response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      contents=[\n        'Describe this audio clip',\n        types.Part.from_bytes(\n          data=audio_bytes,\n          mime_type='audio/mp3',\n        )\n      ]\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    import * as fs from \"node:fs\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    const base64AudioFile = fs.readFileSync(\"path/to/small-sample.mp3\", {\n      encoding: \"base64\",\n    });\n    \n    const contents = [\n      { text: \"Please summarize the audio.\" },\n      {\n        inlineData: {\n          mimeType: \"audio/mp3\",\n          data: base64AudioFile,\n        },\n      },\n    ];\n    \n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: contents,\n    });\n    console.log(response.text);\n    \n\n### Go\n    \n    \n    // Initialize a Gemini model appropriate for your use case.\n    model := client.GenerativeModel(\"gemini-2.0-flash\")\n    \n    bytes, err := os.ReadFile(\"path/to/small-sample.mp3\")\n    if err != nil {\n      log.Fatal(err)\n    }\n    \n    prompt := []genai.Part{\n      genai.Blob{MIMEType: \"audio/mp3\", Data: bytes},\n      genai.Text(\"Please summarize the audio.\"),\n    }\n    \n    // Generate content using the prompt.\n    resp, err := model.GenerateContent(ctx, prompt...)\n    if err != nil {\n      log.Fatal(err)\n    }\n    \n    // Handle the response of generated text\n    for _, c := range resp.Candidates {\n      if c.Content != nil {\n        fmt.Println(*c.Content)\n      }\n    }\n    \n\nA few things to keep in mind about inline audio data:\n\n  * The maximum request size is 20 MB, which includes text prompts, system instructions, and files provided inline. If your file's size will make the _total request size_ exceed 20 MB, then use the Files API to upload an audio file for use in the request.\n  * If you're using an audio sample multiple times, it's more efficient to upload an audio file.\n\n\n\n## Get a transcript\n\nTo get a transcript of audio data, just ask for it in the prompt:\n\n### Python\n    \n    \n    myfile = client.files.upload(file='path/to/sample.mp3')\n    prompt = 'Generate a transcript of the speech.'\n    \n    response = client.models.generate_content(\n      model='gemini-2.0-flash',\n      contents=[prompt, myfile]\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import {\n      GoogleGenAI,\n      createUserContent,\n      createPartFromUri,\n    } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    const myfile = await ai.files.upload({\n      file: \"path/to/sample.mp3\",\n      config: { mimeType: \"audio/mpeg\" },\n    });\n    \n    const result = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: createUserContent([\n        createPartFromUri(myfile.uri, myfile.mimeType),\n        \"Generate a transcript of the speech.\",\n      ]),\n    });\n    console.log(\"result.text=\", result.text);\n    \n\n### Go\n    \n    \n    // Initialize a Gemini model appropriate for your use case.\n    model := client.GenerativeModel(\"gemini-2.0-flash\")\n    \n    // Create a prompt using text and the URI reference for the uploaded file.\n    prompt := []genai.Part{\n      genai.FileData{URI: sampleAudio.URI},\n      genai.Text(\"Generate a transcript of the speech.\"),\n    }\n    \n    // Generate content using the prompt.\n    resp, err := model.GenerateContent(ctx, prompt...)\n    if err != nil {\n      log.Fatal(err)\n    }\n    \n    // Handle the response of generated text\n    for _, c := range resp.Candidates {\n      if c.Content != nil {\n        fmt.Println(*c.Content)\n      }\n    }\n    \n\n## Refer to timestamps\n\nYou can refer to specific sections of an audio file using timestamps of the form `MM:SS`. For example, the following prompt requests a transcript that\n\n  * Starts at 2 minutes 30 seconds from the beginning of the file.\n  * Ends at 3 minutes 29 seconds from the beginning of the file.\n\n\n\n\n### Python\n    \n    \n    # Create a prompt containing timestamps.\n    prompt = \"Provide a transcript of the speech from 02:30 to 03:29.\"\n    \n\n### JavaScript\n    \n    \n    // Create a prompt containing timestamps.\n    const prompt = \"Provide a transcript of the speech from 02:30 to 03:29.\"\n    \n\n### Go\n    \n    \n    // Create a prompt containing timestamps.\n    prompt := []genai.Part{\n        genai.FileData{URI: sampleAudio.URI},\n        genai.Text(\"Provide a transcript of the speech from 02:30 to 03:29.\"),\n    }\n    \n\n## Count tokens\n\nCall the `countTokens` method to get a count of the number of tokens in an audio file. For example:\n\n### Python\n    \n    \n    response = client.models.count_tokens(\n      model='gemini-2.0-flash',\n      contents=[myfile]\n    )\n    \n    print(response)\n    \n\n### JavaScript\n    \n    \n    import {\n      GoogleGenAI,\n      createUserContent,\n      createPartFromUri,\n    } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    const myfile = await ai.files.upload({\n      file: \"path/to/sample.mp3\",\n      config: { mimeType: \"audio/mpeg\" },\n    });\n    \n    const countTokensResponse = await ai.models.countTokens({\n      model: \"gemini-2.0-flash\",\n      contents: createUserContent([\n        createPartFromUri(myfile.uri, myfile.mimeType),\n      ]),\n    });\n    console.log(countTokensResponse.totalTokens);\n    \n\n### Go\n    \n    \n    tokens, err := model.CountTokens(ctx, genai.FileData{URI: sampleAudio.URI})\n    if err != nil {\n        log.Fatal(err)\n    }\n    fmt.Printf(\"File %s is %d tokens\", sampleAudio.DisplayName, tokens.TotalTokens)\n    \n\n## Supported audio formats\n\nGemini supports the following audio format MIME types:\n\n  * WAV - `audio/wav`\n  * MP3 - `audio/mp3`\n  * AIFF - `audio/aiff`\n  * AAC - `audio/aac`\n  * OGG Vorbis - `audio/ogg`\n  * FLAC - `audio/flac`\n\n\n\n## Technical details about audio\n\n  * Gemini represents each second of audio as 32 tokens; for example, one minute of audio is represented as 1,920 tokens.\n  * Gemini can only infer responses to English-language speech.\n  * Gemini can \"understand\" non-speech components, such as birdsong or sirens.\n  * The maximum supported length of audio data in a single prompt is 9.5 hours. Gemini doesn't limit the _number_ of audio files in a single prompt; however, the total combined length of all audio files in a single prompt can't exceed 9.5 hours.\n  * Gemini downsamples audio files to a 16 Kbps data resolution.\n  * If the audio source contains multiple channels, Gemini combines those channels into a single channel.\n\n\n\n## What's next\n\nThis guide shows how to generate text in response to audio data. To learn more, see the following resources:\n\n  * [File prompting strategies](/gemini-api/docs/file-prompting-strategies): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.\n  * [System instructions](/gemini-api/docs/system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.\n  * [Safety guidance](/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-10 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-10 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/long-context",
    "title": "Long context  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Long context \n\nGemini 2.0 Flash and Gemini 1.5 Flash come with a 1-million-token context window, and Gemini 1.5 Pro comes with a 2-million-token context window. Historically, large language models (LLMs) were significantly limited by the amount of text (or tokens) that could be passed to the model at one time. The Gemini 1.5 long context window, with [near-perfect retrieval (>99%)](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf), unlocks many new use cases and developer paradigms.\n\nThe code you already use for cases like [text generation](/gemini-api/docs/text-generation) or [multimodal inputs](/gemini-api/docs/vision) will work out of the box with long context.\n\nThroughout this guide, you briefly explore the basics of the context window, how developers should think about long context, various real world use cases for long context, and ways to optimize the usage of long context.\n\n## What is a context window?\n\nThe basic way you use the Gemini models is by passing information (context) to the model, which will subsequently generate a response. An analogy for the context window is short term memory. There is a limited amount of information that can be stored in someone's short term memory, and the same is true for generative models.\n\nYou can read more about how models work under the hood in our [generative models guide](/gemini-api/docs/models/generative-models).\n\n## Getting started with long context\n\nMost generative models created in the last few years were only capable of processing 8,000 tokens at a time. Newer models pushed this further by accepting 32,000 tokens or 128,000 tokens. Gemini 1.5 is the first model capable of accepting 1 million tokens, and now [2 million tokens with Gemini 1.5 Pro](https://developers.googleblog.com/en/new-features-for-the-gemini-api-and-google-ai-studio/).\n\nIn practice, 1 million tokens would look like:\n\n  * 50,000 lines of code (with the standard 80 characters per line)\n  * All the text messages you have sent in the last 5 years\n  * 8 average length English novels\n  * Transcripts of over 200 average length podcast episodes\n\n\n\nEven though the models can take in more and more context, much of the conventional wisdom about using large language models assumes this inherent limitation on the model, which as of 2024, is no longer the case.\n\nSome common strategies to handle the limitation of small context windows included:\n\n  * Arbitrarily dropping old messages / text from the context window as new text comes in\n  * Summarizing previous content and replacing it with the summary when the context window gets close to being full\n  * Using RAG with semantic search to move data out of the context window and into a vector database\n  * Using deterministic or generative filters to remove certain text / characters from prompts to save tokens\n\n\n\nWhile many of these are still relevant in certain cases, the default place to start is now just putting all of the tokens into the context window. Because Gemini models were purpose-built with a long context window, they are much more capable of in-context learning. For example, with only instructional materials (a 500-page reference grammar, a dictionary, and ≈ 400 extra parallel sentences) all provided in context, Gemini 1.5 Pro and Gemini 1.5 Flash are [capable of learning to translate](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf) from English to Kalamang— a Papuan language with fewer than 200 speakers and therefore almost no online presence—with quality similar to a person who learned from the same materials.\n\nThis example underscores how you can start to think about what is possible with long context and the in-context learning capabilities of Gemini models.\n\n## Long context use cases\n\nWhile the standard use case for most generative models is still text input, the Gemini 1.5 model family enables a new paradigm of multimodal use cases. These models can natively understand text, video, audio, and images. They are accompanied by the [Gemini API that takes in multimodal file types](/gemini-api/docs/prompting_with_media) for convenience.\n\n### Long form text\n\nText has proved to be the layer of intelligence underpinning much of the momentum around LLMs. As mentioned earlier, much of the practical limitation of LLMs was because of not having a large enough context window to do certain tasks. This led to the rapid adoption of retrieval augmented generation (RAG) and other techniques which dynamically provide the model with relevant contextual information. Now, with larger and larger context windows (currently up to 2 million on Gemini 1.5 Pro), there are new techniques becoming available which unlock new use cases.\n\nSome emerging and standard use cases for text based long context include:\n\n  * Summarizing large corpuses of text \n    * Previous summarization options with smaller context models would require a sliding window or another technique to keep state of previous sections as new tokens are passed to the model\n  * Question and answering \n    * Historically this was only possible with RAG given the limited amount of context and models' factual recall being low\n  * Agentic workflows \n    * Text is the underpinning of how agents keep state of what they have done and what they need to do; not having enough information about the world and the agent's goal is a limitation on the reliability of agents\n\n\n\n[Many-shot in-context learning](https://arxiv.org/pdf/2404.11018) is one of the most unique capabilities unlocked by long context models. Research has shown that taking the common \"single shot\" or \"multi-shot\" example paradigm, where the model is presented with one or a few examples of a task, and scaling that up to hundreds, thousands, or even hundreds of thousands of examples, can lead to novel model capabilities. This many-shot approach has also been shown to perform similarly to models which were fine-tuned for a specific task. For use cases where a Gemini model's performance is not yet sufficient for a production rollout, you can try the many-shot approach. As you might explore later in the long context optimization section, context caching makes this type of high input token workload much more economically feasible and even lower latency in some cases.\n\n### Long form video\n\nVideo content's utility has long been constrained by the lack of accessibility of the medium itself. It was hard to skim the content, transcripts often failed to capture the nuance of a video, and most tools don't process image, text, and audio together. With Gemini 1.5, the long-context text capabilities translate to the ability to reason and answer questions about multimodal inputs with sustained performance. Gemini 1.5 Flash, when tested on the needle in a video haystack problem with 1M tokens, obtained >99.8% recall of the video in the context window, and 1.5 Pro reached state of the art performance on the [Video-MME benchmark](https://video-mme.github.io/home_page.html).\n\nSome emerging and standard use cases for video long context include:\n\n  * Video question and answering\n  * Video memory, as shown with [Google's Project Astra](https://deepmind.google/technologies/gemini/project-astra/)\n  * Video captioning\n  * Video recommendation systems, by enriching existing metadata with new multimodal understanding\n  * Video customization, by looking at a corpus of data and associated video metadata and then removing parts of videos that are not relevant to the viewer\n  * Video content moderation\n  * Real-time video processing\n\n\n\nWhen working with videos, it is important to consider how the [videos are processed into tokens](/gemini-api/docs/tokens#media-token), which affects billing and usage limits. You can learn more about prompting with video files in the [Prompting guide](/gemini-api/docs/prompting_with_media?lang=python#prompting-with-videos).\n\n### Long form audio\n\nThe Gemini 1.5 models were the first natively multimodal large language models that could understand audio. Historically, the typical developer workflow would involve stringing together multiple domain specific models, like a speech-to-text model and a text-to-text model, in order to process audio. This led to additional latency required by performing multiple round-trip requests and decreased performance usually attributed to disconnected architectures of the multiple model setup.\n\nOn standard audio-haystack evaluations, Gemini 1.5 Pro is able to find the hidden audio in 100% of the tests and Gemini 1.5 Flash is able to find it in 98.7% [of the tests](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf). Gemini 1.5 Flash accepts up to 9.5 hours of [audio in a single request](/gemini-api/docs/prompting_with_media?lang=python#audio_formats) and Gemini 1.5 Pro can accept up to 19 hours of audio using the 2-million-token context window. Further, on a test set of 15-minute audio clips, Gemini 1.5 Pro archives a word error rate (WER) of ~5.5%, much lower than even specialized speech-to-text models, without the added complexity of extra input segmentation and pre-processing.\n\nSome emerging and standard use cases for audio context include:\n\n  * Real-time transcription and translation\n  * Podcast / video question and answering\n  * Meeting transcription and summarization\n  * Voice assistants\n\n\n\nYou can learn more about prompting with audio files in the [Prompting guide](/gemini-api/docs/prompting_with_media?lang=python#prompting-with-videos).\n\n## Long context optimizations\n\nThe primary optimization when working with long context and the Gemini 1.5 models is to use [context caching](/gemini-api/docs/caching). Beyond the previous impossibility of processing lots of tokens in a single request, the other main constraint was the cost. If you have a \"chat with your data\" app where a user uploads 10 PDFs, a video, and some work documents, you would historically have to work with a more complex retrieval augmented generation (RAG) tool / framework in order to process these requests and pay a significant amount for tokens moved into the context window. Now, you can cache the files the user uploads and pay to store them on a per hour basis. The input / output cost per request with Gemini 1.5 Flash for example is ~4x less than the standard input / output cost, so if the user chats with their data enough, it becomes a huge cost saving for you as the developer.\n\n## Long context limitations\n\nIn various sections of this guide, we talked about how Gemini 1.5 models achieve high performance across various needle-in-a-haystack retrieval evals. These tests consider the most basic setup, where you have a single needle you are looking for. In cases where you might have multiple \"needles\" or specific pieces of information you are looking for, the model does not perform with the same accuracy. Performance can vary to a wide degree depending on the context. This is important to consider as there is an inherent tradeoff between getting the right information retrieved and cost. You can get ~99% on a single query, but you have to pay the input token cost every time you send that query. So for 100 pieces of information to be retrieved, if you needed 99% performance, you would likely need to send 100 requests. This is a good example of where context caching can significantly reduce the cost associated with using Gemini models while keeping the performance high.\n\n## FAQs\n\n### Do I lose model performance when I add more tokens to a query?\n\nGenerally, if you don't need tokens to be passed to the model, it is best to avoid passing them. However, if you have a large chunk of tokens with some information and want to ask questions about that information, the model is highly capable of extracting that information (up to 99% accuracy in many cases).\n\n### How does Gemini 1.5 Pro perform on the standard needle-in-a-haystack test?\n\nGemini 1.5 Pro achieves 100% recall up to 530k tokens and >99.7% recall [up to 1M tokens](https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf).\n\n### How can I lower my cost with long-context queries?\n\nIf you have a similar set of tokens / context that you want to re-use many times, [context caching](/gemini-api/docs/caching) can help reduce the costs associated with asking questions about that information.\n\n### How can I get access to the 2-million-token context window?\n\nAll developers now have access to the 2-million-token context window with Gemini 1.5 Pro.\n\n### Does the context length affect the model latency?\n\nThere is some fixed amount of latency in any given request, regardless of the size, but generally longer queries will have higher latency (time to first token).\n\n### Do the long context capabilities differ between Gemini 1.5 Flash and Gemini 1.5 Pro?\n\nYes, some of the numbers were mentioned in different sections of this guide, but generally Gemini 1.5 Pro is more performant on most long context use cases.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/code-execution",
    "title": "Code execution  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Code execution \n\nPython JavaScript Go REST\n\nThe Gemini API code execution feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n\n**Note:** Gemini is only able to execute code in Python. You can still ask Gemini to generate code in another language, but the model can't use the code execution tool to run it.\n\nCode execution is available in both AI Studio and the Gemini API. In AI Studio, you can enable code execution in the right panel under **Tools**. The Gemini API provides code execution as a tool, similar to [function calling](/gemini-api/docs/function-calling). After you add code execution as a tool, the model decides when to use it.\n\nThe code execution environment includes the following libraries: `altair`, `chess`, `cv2`, `matplotlib`, `mpmath`, `numpy`, `pandas`, `pdfminer`, `reportlab`, `seaborn`, `sklearn`, `statsmodels`, `striprtf`, `sympy`, and `tabulate`. You can't install your own libraries.\n\n**Note:** Only `matplotlib` is supported for graph rendering using code execution.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Get started with code execution\n\n## Input/output (I/O)\n\nStarting with [Gemini 2.0 Flash](/gemini-api/docs/models/gemini#gemini-2.0-flash), code execution supports file input and graph output. Using these new input and output capabilities, you can upload CSV and text files, ask questions about the files, and have [Matplotlib](https://matplotlib.org/) graphs generated as part of the response.\n\n### I/O pricing\n\nWhen using code execution I/O, you're charged for input tokens and output tokens:\n\n**Input tokens:**\n\n  * User prompt\n\n\n\n**Output tokens:**\n\n  * Code generated by the model\n  * Code execution output in the code environment\n  * Summary generated by the model\n\n\n\n### I/O details\n\nWhen you're working with code execution I/O, be aware of the following technical details:\n\n  * The maximum runtime of the code environment is 30 seconds.\n  * If the code environment generates an error, the model may decide to regenerate the code output. This can happen up to 5 times.\n  * The maximum file input size is limited by the model token window. In AI Studio, using Gemini Flash 2.0, the maximum input file size is 1 million tokens (roughly 2MB for text files of the supported input types). If you upload a file that's too large, AI Studio won't let you send it.\n\n| Single turn | Bidirectional (Multimodal Live API)  \n---|---|---  \nModels supported | All Gemini 2.0 models | Only Flash experimental models  \nFile input types supported | .png, .jpeg, .csv, .xml, .cpp, .java, .py, .js, .ts | .png, .jpeg, .csv, .xml, .cpp, .java, .py, .js, .ts  \nPlotting libraries supported | Matplotlib | Matplotlib  \n[Multi-tool use](/gemini-api/docs/function-calling#multi-tool-use) | No | Yes  \n  \n## Billing\n\nThere's no additional charge for enabling code execution from the Gemini API. You'll be billed at the current rate of input and output tokens based on the Gemini model you're using.\n\nHere are a few other things to know about billing for code execution:\n\n  * You're only billed once for the input tokens you pass to the model, and you're billed for the final output tokens returned to you by the model.\n  * Tokens representing generated code are counted as output tokens. Generated code can include text and multimodal output like images.\n  * Code execution results are also counted as output tokens.\n\n\n\nThe billing model is shown in the following diagram:\n\n  * You're billed at the current rate of input and output tokens based on the Gemini model you're using.\n  * If Gemini uses code execution when generating your response, the original prompt, the generated code, and the result of the executed code are labeled _intermediate tokens_ and are billed as _input tokens_.\n  * Gemini then generates a summary and returns the generated code, the result of the executed code, and the final summary. These are billed as _output tokens_.\n  * The Gemini API includes an intermediate token count in the API response, so you know why you're getting additional input tokens beyond your initial prompt.\n\n\n\n## Limitations\n\n  * The model can only generate and execute code. It can't return other artifacts like media files.\n  * In some cases, enabling code execution can lead to regressions in other areas of model output (for example, writing a story).\n  * There is some variation in the ability of the different models to use code execution successfully.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/structured-output",
    "title": "Generate structured output with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Generate structured output with the Gemini API \n\nPython JavaScript Go REST\n\n  \n\n\nGemini generates unstructured text by default, but some applications require structured text. For these use cases, you can constrain Gemini to respond with JSON, a structured data format suitable for automated processing. You can also constrain the model to respond with one of the options specified in an enum.\n\nHere are a few use cases that might require structured output from the model:\n\n  * Build a database of companies by pulling company information out of newspaper articles.\n  * Pull standardized information out of resumes.\n  * Extract ingredients from recipes and display a link to a grocery website for each ingredient.\n\n\n\nIn your prompt, you can ask Gemini to produce JSON-formatted output, but note that the model is not guaranteed to produce JSON and nothing but JSON. For a more deterministic response, you can pass a specific JSON schema in a [`responseSchema`](/api/rest/v1beta/GenerationConfig#FIELDS.response_schema) field so that Gemini always responds with an expected structure. To learn more about working with schemas, see More about JSON schemas.\n\nThis guide shows you how to generate JSON using the [`generateContent`](/api/rest/v1/models/generateContent) method through the SDK of your choice or using the REST API directly. The examples show text-only input, although Gemini can also produce JSON responses to multimodal requests that include [images](/gemini-api/docs/vision), [videos](/gemini-api/docs/vision), and [audio](/gemini-api/docs/audio).\n\n## More about JSON schemas\n\nWhen you configure the model to return a JSON response, you can use a `Schema` object to define the shape of the JSON data. The `Schema` represents a select subset of the [OpenAPI 3.0 Schema object](https://spec.openapis.org/oas/v3.0.3#schema-object).\n\nHere's a pseudo-JSON representation of all the `Schema` fields:\n    \n    \n    {\n      \"type\": enum (Type),\n      \"format\": string,\n      \"description\": string,\n      \"nullable\": boolean,\n      \"enum\": [\n        string\n      ],\n      \"maxItems\": string,\n      \"minItems\": string,\n      \"properties\": {\n        string: {\n          object (Schema)\n        },\n        ...\n      },\n      \"required\": [\n        string\n      ],\n      \"propertyOrdering\": [\n        string\n      ],\n      \"items\": {\n        object (Schema)\n      }\n    }\n    \n\nThe `Type` of the schema must be one of the OpenAPI [Data Types](https://spec.openapis.org/oas/v3.0.3#data-types). Only a subset of fields is valid for each `Type`. The following list maps each `Type` to valid fields for that type:\n\n  * `string` -> enum, format\n  * `integer` -> format\n  * `number` -> format\n  * `boolean`\n  * `array` -> minItems, maxItems, items\n  * `object` -> properties, required, propertyOrdering, nullable\n\n\n\nHere are some example schemas showing valid type-and-field combinations:\n    \n    \n    { \"type\": \"string\", \"enum\": [\"a\", \"b\", \"c\"] }\n    \n    { \"type\": \"string\", \"format\": \"date-time\" }\n    \n    { \"type\": \"integer\", \"format\": \"int64\" }\n    \n    { \"type\": \"number\", \"format\": \"double\" }\n    \n    { \"type\": \"boolean\" }\n    \n    { \"type\": \"array\", \"minItems\": 3, \"maxItems\": 3, \"items\": { \"type\": ... } }\n    \n    { \"type\": \"object\",\n      \"properties\": {\n        \"a\": { \"type\": ... },\n        \"b\": { \"type\": ... },\n        \"c\": { \"type\": ... }\n      },\n      \"nullable\": true,\n      \"required\": [\"c\"],\n      \"propertyOrdering\": [\"c\", \"b\", \"a\"]\n    }\n    \n\nFor complete documentation of the Schema fields as they're used in the Gemini API, see the [Schema reference](/api/caching#Schema).\n\n### Property ordering\n\nWhen you're working with JSON schemas in the Gemini API, the order of properties is important. By default, the API orders properties alphabetically and does not preserve the order in which the properties are defined (although the [Google Gen AI SDKs](/gemini-api/docs/sdks) may preserve this order). If you're providing examples to the model with a schema configured, and the property ordering of the examples is not consistent with the property ordering of the schema, the output could be rambling or unexpected.\n\nTo ensure a consistent, predictable ordering of properties, you can use the optional `propertyOrdering[]` field.\n    \n    \n    \"propertyOrdering\": [\"recipe_name\", \"ingredients\"]\n    \n\n`propertyOrdering[]` – not a standard field in the OpenAPI specification – is an array of strings used to determine the order of properties in the response. By specifying the order of properties and then providing examples with properties in that same order, you can potentially improve the quality of results.\n\n**Key Point:** To improve results when you're using a JSON schema, set `propertyOrdering[]` and provide examples with a matching property ordering.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/thinking",
    "title": "Use Gemini thinking  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Use Gemini thinking \n\nGemini 2.5 Pro Experimental and Gemini 2.0 Flash Thinking Experimental are models that use an internal \"thinking process\" during response generation. This process contributes to their improved reasoning capabilities and allows them to solve complex tasks. This guide shows you how to use Gemini models with thinking capabilities.\n\n[ Try Gemini 2.5 Pro Preview in Google AI Studio ](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro-preview-03-25)\n\n**Note:** 'gemini-2.5-pro-preview-03-25' is a billed model, you can continue to use 'gemini-2.5-pro-exp-03-25' on the free tier.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Use thinking models\n\nModels with thinking capabilities are available in [Google AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro-preview-03-25) and through the Gemini API. Note that the thinking process is visible within Google AI Studio but is not provided as part of the API output.\n\n### Send a basic request\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    prompt = \"Explain the concept of Occam's Razor and provide a simple, everyday example.\"\n    response = client.models.generate_content(\n        model=\"gemini-2.5-pro-exp-03-25\",  # or gemini-2.0-flash-thinking-exp\n        contents=prompt\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const prompt = \"Explain the concept of Occam's Razor and provide a simple, everyday example.\";\n    \n      const response = await ai.models.generateContent({\n        model: \"gemini-2.5-pro-preview-03-25\",  // or gemini-2.0-flash-thinking-exp\n        contents: prompt,\n      });\n    \n      console.log(response.text);\n    }\n    \n    main();\n    \n\n### Go\n    \n    \n    // import packages here\n    \n    func main() {\n      ctx := context.Background()\n      client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n      if err != nil {\n        log.Fatal(err)\n      }\n      defer client.Close()\n    \n      model := client.GenerativeModel(\"gemini-2.5-pro-preview-03-25\")  // or gemini-2.0-flash-thinking-exp\n      resp, err := model.GenerateContent(ctx, genai.Text(\"Explain the concept of Occam's Razor and provide a simple, everyday example.\"))\n      if err != nil {\n        log.Fatal(err)\n      }\n      fmt.Println(resp.Text())\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?key=$YOUR_API_KEY\" \\\n     -H 'Content-Type: application/json' \\\n     -X POST \\\n     -d '{\n       \"contents\": [\n         {\n           \"parts\": [\n             {\n               \"text\": \"Explain the concept of Occam\\''s Razor and provide a simple, everyday example.\"\n             }\n           ]\n         }\n       ]\n     }'\n     ```\n    \n\n### Multi-turn thinking conversations\n\nTo take the previous chat history into account, you can use multi-turn conversations.\n\nWith the SDKs, you can create a [chat session](/gemini-api/docs/text-generation#multi-turn-conversations) to manage the state of the conversation.\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key='GEMINI_API_KEY')\n    \n    chat = client.aio.chats.create(\n        model='gemini-2.5-pro-preview-03-25',  # or gemini-2.0-flash-thinking-exp\n    )\n    response = await chat.send_message('What is your name?')\n    print(response.text)\n    response = await chat.send_message('What did you just say before this?')\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n        const chat = ai.chats.create({\n            model: 'gemini-2.5-pro-preview-03-25'  // or gemini-2.0-flash-thinking-exp\n        });\n    \n        const response = await chat.sendMessage({\n            message: 'What is your name?'\n        });\n        console.log(response.text);\n    \n        response = await chat.sendMessage({\n            message: 'What did you just say before this?'\n        });\n        console.log(response.text);\n    }\n    \n    main();\n    \n\n## Use tools with thinking models\n\nThinking models can use tools to perform actions beyond generating text. This allows them to interact with external systems, execute code, or access real-time information, incorporating the results into their reasoning and final response.\n\n### Search Tool\n\nThe [Search tool](/gemini-api/docs/grounding) allows the model to query external search engines to find up-to-date information or information beyond its training data. This is useful for questions about recent events or highly specific topics.\n\nTo configure the search tool, see [Configure the Search tool](/gemini-api/docs/grounding#configure-search-tool).\n\n**Prompt:**\n    \n    \n    What were the major scientific breakthroughs announced last week?\n      \n\n**Response (using Search Tool):**\n    \n    \n    Based on recent search results, here are some highlights from the past week in science:\n    \n    ...\n      \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Code Execution\n\nThe [Code execution tool](/gemini-api/docs/code-execution) enables the model to generate and run Python code to perform calculations, manipulate data, or solve problems that are best handled algorithmically. The model receives the code's output and can use it in its response.\n\nTo configure code execution, see [Enable code execution on the model](/gemini-api/docs/code-execution#enable-on-model).\n\n**Prompt:**\n    \n    \n    What is the sum of the first 50 prime numbers?\n    Generate and run Python code for the calculation, and make sure you get all 50.\n    Provide the final sum clearly.\n      \n\n**Response (using Code Execution Tool):**\n    \n    \n    Okay, I can calculate that. I will generate Python code to find the first 50 prime numbers and sum them up.\n    \n    ```python\n    def is_prime(num):\n        if num < 2:\n            return False\n        for i in range(2, int(num**0.5) + 1):\n            if num % i == 0:\n                return False\n        return True\n    \n    primes = []\n    num = 2\n    while len(primes) < 50:\n        if is_prime(num):\n            primes.append(num)\n        num += 1\n    \n    total_sum = sum(primes)\n    print(total_sum)\n    ```\n    \n    Running the code...\n    \n    Code Execution Result:\n    Outcome: OK\n    Output:\n    \n    5117\n    Use code with caution.\n    The sum of the first 50 prime numbers is 5117.\n    \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Structured Output\n\nWith [structured output](/gemini-api/docs/structured-output), you can constrain Gemini to respond with JSON, a structured output format suitable for automated processing. This is particularly useful for integrating the model's output into applications.\n\nYou can specify the format directly in the prompt or [supply a schema through model configuration](/gemini-api/docs/structured-output#supply-schema-in-config).\n\n**Prompt:**\n    \n    \n    Provide a list of 3 famous physicists and their key contributions\n    in JSON format.\n    Use this JSON schema:\n    \n    Physicist = {'name': str, 'contribution': str, 'era': str}\n    Return: list[Physicist]\n    \n\n**Response (instructed for JSON):**\n    \n    \n    ```json\n    [\n    {\n    \"name\": \"Albert Einstein\",\n    \"contribution\": \"Theory of Relativity (Special and General)\",\n    \"era\": \"Early 20th Century\"\n    },\n    {\n    \"name\": \"Isaac Newton\",\n    \"contribution\": \"Laws of Motion and Universal Gravitation\",\n    \"era\": \"Late 17th / Early 18th Century\"\n    },\n    {\n    \"name\": \"Marie Curie\",\n    \"contribution\": \"Pioneering research on radioactivity\",\n    \"era\": \"Late 19th / Early 20th Century\"\n    }\n    ]\n    ```\n    \n\n(gemini-2.5-pro-preview-03-25)  \n---  \n  \n## Prompting techniques for thinking models\n\nEffective prompting is crucial for maximizing the benefits of the Gemini thinking models. This section outlines some prompting practices for thinking models.\n\n### Getting started\n\nBegin with general instructions that describe the desired outcome. Start with a simple prompt and observe the model's initial thoughts on how to tackle the challenge.\n\n**Prompt:**\n    \n    \n    Describe the most unusual or striking architectural feature you've ever seen in a building or structure.\n      \n\n**Response:**\n    \n    \n    <...>\n    The most unusual architectural feature that has truly stayed with me is the \"floating roof\" of the\n      Heydar Aliyev Center in Baku, Azerbaijan, designed by Zaha Hadid Architects.\n    \n    Visually, it's breathtaking. Imagine a vast, undulating landscape of pure white, seemingly draped\n    over the building. It's not just a roof in the traditional sense; it's an extension of the ground,\n    flowing seamlessly upwards and over the building's various spaces before curving back down to meet\n    the earth again. There are no sharp angles, no rigid lines. Instead, everything is fluid, organic,\n    and continuous. It resembles a giant, pristine white wave frozen in motion, or perhaps a sand dune\n    sculpted by an unseen wind.\n    <...>\n      \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Provide Step-by-Step Instructions\n\nIf the initial attempt does not yield the desired result, refine your prompt by providing more explicit step-by-step instructions to guide Gemini through the problem-solving process for a more focused approach. This allows you steer the model's initial understanding of the task with explicit suggestions and guidance.\n\n**Prompt:**\n    \n    \n    Now imagine that architectural feature is not just for looks, but has a specific function or\n    purpose related to the building's environment or the people who use it. Explain what that\n    function could be and how it enhances the building's overall design and usability.\n      \n\n**Response:**\n    \n    \n    <...>\n    Let's imagine that the \"floating roof\" of the Heydar Aliyev Center is not purely aesthetic but\n    serves a crucial environmental and user-centric function: it's designed as a sophisticated climate\n    moderator and natural ventilation system for the Baku environment.\n    \n    Here's how this function would work and enhance the building:\n    \n    Function: Passive Climate Control and Natural Ventilation System\n    \n    Shading and Solar Heat Mitigation: The undulating curves of the white GFRC roof are strategically\n    shaped and oriented to provide optimal shading throughout the day, year-round. Baku experiences\n    intense sun, especially during the summer. The overhanging curves, acting like giant brise-soleils,\n    would block direct sunlight from hitting the building's glass facade, significantly reducing solar\n    heat gain. The varying heights and depths of the curves would create dynamic shade patterns, ensuring\n    that different parts of the building are shaded at different times of the day, optimizing comfort\n    and reducing the need for excessive air conditioning. The white color of the GFRC further enhances\n    this by reflecting a large portion of the solar radiation.\n    <...>\n      \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Multishot Prompting with Thinking\n\nMultishot prompting in conjunction with thinking can further enhance Gemini's understanding and improve the accuracy of its responses. Provide several examples of input-output pairs, demonstrating the desired behavior and output format in your prompt.\n\n**Prompt:**\n    \n    \n    Example 1:\n    User: What is the tallest mountain in the world?\n    Assistant: Mount Everest\n    \n    Example 2:\n    User: What is the largest ocean?\n    Assistant: Pacific Ocean\n    \n    User: What is the longest river in the world?\n    Assistant:\n      \n\n**Response:**\n    \n    \n    Amazon River\n      \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Defining Output and Behavior\n\nWhen building an application where users will interact directly with the model, you may want to provide guidance for how Gemini's output and responses will sound and be formatted.\n\n#### System Instructions\n\nSystem instructions are a set of instructions that the model processes before it processes prompts. They are recalled each time the model is prompted and tell the model how you want it to behave and respond. For example, you can include things like the role or persona, contextual information, and formatting instructions.\n\n**Prompt:**\n    \n    \n    System Instruction: You are a helpful and concise coding tutor. Always provide code examples and explain them briefly.\n    \n    User Prompt: Explain what is a closure in Javascript and give an example.\n      \n\n**Response:**\n    \n    \n        function outerFunction(outerVar) {\n          function innerFunction(innerVar) {\n            console.log(outerVar); // Accessing outerVar from outerFunction's scope\n            console.log(innerVar);\n          }\n          return innerFunction;\n        }\n    \n        const closureExample = outerFunction('I am from outer');\n        closureExample('I am from inner');\n    \n    Explanation:\n    \n    A closure is created when a function (innerFunction in this example) is defined inside another function\n    (outerFunction) and the inner function remembers and has access to the variables from the outer\n    function's scope, even after the outer function has finished executing.\n    \n    In the example:\n    \n    outerFunction is called with 'I am from outer' and returns innerFunction.\n    \n    closureExample now holds a reference to innerFunction.\n    \n    When closureExample('I am from inner') is called, innerFunction executes.\n    \n    Even though outerFunction has already finished, innerFunction can still access and use outerVar\n    (which is 'I am from outer') because of the closure.\n    \n    In essence, the innerFunction \"closes over\" the variables of its outer function's scope, creating\n    a closure.\n      \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n#### Verification and Reflection\n\nInclude specific verification steps in either the System Instructions or your prompts directly. For example, ask Gemini to verify its sources, review its reasoning, identify potential errors, and check its final answer.\n\n**Prompt:**\n    \n    \n        Verify the following statement and provide your sources: The capital of Brazil is Rio de Janeiro.\n      \n\n**Response:**\n    \n    \n    The statement \"The capital of Brazil is Rio de Janeiro\" is incorrect.\n    \n    The current capital of Brazil is Brasília.\n    \n    Rio de Janeiro was the capital of Brazil for a significant period, from 1763 to 1960. However, the\n    capital was moved to Brasília in 1960 as part of a plan to develop the country's interior and create\n    a more centrally located capital.\n    \n    Verification and Sources:\n    \n    You can easily verify this information from numerous reliable sources:\n    \n    Encyclopedias:\n    \n    Britannica: Searching for \"Brazil\" on Britannica will clearly state Brasília as the capital.\n    \n    https://www.britannica.com/place/Brazil (Look for the \"Capital\" information in the introductory summary or \"Geography\" section)\n    \n    Wikipedia: The Wikipedia article for \"Brazil\" also states Brasília as the capital.\n    \n    https://en.wikipedia.org/wiki/Brazil (Look in the infobox or introductory section)\n    \n    Government and Official Sources:\n    \n    CIA World Factbook: This is a highly reliable source for country information. Searching for \"Brazil\"\n    will list Brasília as the capital.\n    \n    https://www.cia.gov/the-world-factbook/countries/brazil/ (Look under \"Government\" > \"Capital\")\n    \n    Official Website of the Brazilian Government (if accessible and in English): While navigating official\n    government websites can sometimes be complex, they would definitively list Brasília as the capital.\n    \n    Geographic Resources:\n    \n    National Geographic: Their website and publications will confirm Brasília as the capital of Brazil.\n    \n    Atlases and Geography Textbooks: Any modern atlas or geography textbook will list Brasília as the capital.\n    \n    Why the confusion?\n    \n    The confusion might arise because:\n    \n    Rio de Janeiro is a very famous Brazilian city: It's a major tourist destination, known for its\n    beaches, Carnival, and iconic landmarks like Christ the Redeemer. Its past status as capital adds\n    to its historical significance and public recognition.\n    \n    Historical context: Rio de Janeiro was the capital for a long time. People who learned about Brazil\n    in the past might remember it as the capital.\n    \n    In conclusion, the statement is false. The capital of Brazil is Brasília, not Rio de Janeiro. Rio\n    de Janeiro was the former capital but is no longer the current capital.\n    \n    You can confidently use the sources listed above to verify this information.\n      \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Debugging and Steering\n\nGoogle AI Studio provides developers access to the model's thoughts in the UI. This is helpful as a developer reviewing your prompt when debugging:\n\n#### Analyze the Thought Process\n\nCarefully review Gemini's thought process to identify any areas where it may have gone astray. This analysis can provide valuable clues for refining your prompts and getting your expected response consistently in your application.\n\n_Example Scenario: If Gemini's thought process shows it misunderstood a step in a complex instruction, you can refine the prompt to clarify that specific step._\n\n**Prompt:**\n    \n    \n    Step 1: Summarize the following article about quantum physics.\n    Step 2: Translate the summary into Spanish.\n    Step 3: Extract the key entities from the Spanish summary.\n    \n    Article: ... [Article Text] ...\n    \n\n**Gemini's Thought Process Analysis (Example):**\n    \n    \n    The user wants me to process an article about Google's new quantum chip called Willow in three steps:\n    Step 1: Summarize the article.\n    Step 2: Translate the summary into Spanish.\n    Step 3: Extract the key entities from the Spanish summary.\n    \n    Let's break down each step.\n    \n    Step 1: Summarize the article.\n    I need to read the article and identify the main points. The article is about Google's new quantum\n    chip, Willow, and its achievements. The key achievements are:\n    \n    Exponential error reduction as qubits scale up.\n    \n    Outperforming supercomputers in a benchmark computation (RCS).\n    \n    Willow is a step towards commercially relevant quantum computing.\n    \n    It's the first system below threshold for quantum error correction.\n    \n    It achieved a benchmark computation in under 5 minutes that would take a supercomputer 10 septillion years.\n    \n    It was fabricated in a new facility.\n    \n    It has state-of-the-art performance and high quality qubits.\n    \n    Google is looking towards real-world applications with Willow.\n    \n    Quantum computing will benefit AI.\n    \n    I will synthesize these points into a concise summary.\n    \n    Step 2: Translate the summary into Spanish.\n    Once I have the summary in English, I will translate it into Spanish. I need to ensure the translation\n    is accurate and natural-sounding in Spanish.\n    \n    Step 3: Extract the key entities from the Spanish summary.\n    After translating the summary, I will identify and extract the key entities. Key entities are typically\n    nouns or noun phrases that represent important objects, concepts, or actors. In this context, they\n    will likely include:\n    \n    Names of chips (Willow, Sycamore)\n    \n    Companies/Organizations (Google Quantum AI, Google Research)\n    \n    Technologies (quantum chip, quantum computer, qubits, quantum error correction, superconducting\n    quantum system, classical computers, supercomputers, AI)\n    \n    Benchmarks (random circuit sampling, RCS)\n    \n    People (Julian Kelly, Peter Shor, David Deutsch, Sergio Boixo, Hartmut Neven, John Preskill)\n    \n    Concepts (exponential error reduction, below threshold, beyond breakeven, state-of-the-art performance,\n    commercially relevant applications, quantum mechanics, multiverse, T1 times, system benchmarks,\n    algorithmic benchmarks)\n    \n    I will need to carefully review the Spanish summary to identify these entities and list them out.\n    \n      \n  \n---  \n  \n#### Provide Corrective Feedback\n\nIf you observe errors or inconsistencies in Gemini's reasoning, provide corrective feedback in your prompt and/or System Instructions to anticipate where the model's response may deviate from your desired outcome.\n\n**Prompt:**\n    \n    \n    You are an expert in botany. When asked to identify a plant, focus on leaf shape, flower structure, and stem characteristics.\n    \n    Identify this plant based on these characteristics: ... [Plant Characteristics Description] ...\n    \n    Previous incorrect response focused on habitat only. Please disregard habitat and focus on morphology.\n    \n\n**Refined Prompt (if needed based on thought process):**\n    \n    \n    Previous incorrect response focused on habitat only. Please disregard habitat and focus on morphology.\n    \n\n**Response:**\n    \n    \n    Tabebuia vellosoi\n    \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Making the Best of Long Outputs and Longform Thinking\n\nThinking generates “thoughts” as part of the Token Output that showcases Gemini's detailed reasoning. To make the most of these outputs:\n\n#### Review the Reasoning\n\nPay close attention to Gemini's reasoning process (available in the Google AI Studio UI), how it broke down the task and the choices it made for your output. This can help you understand how it arrived at its conclusion.\n\n#### Provide Guidance in Reasoning\n\nIf your desired output is to be particularly lengthy, you may want to provide more guidance in your prompt to constrain the amount of thinking needed to accomplish the task presented in the prompt. This allows you to reserve more of the Token Output for your response.\n\n_Example: For a detailed report, you might specify sections and sub-sections in your prompt to guide Gemini's thinking and structure the output efficiently._\n\n**Prompt:**\n    \n    \n    Generate a detailed report on the impact of climate change on coastal ecosystems.\n    \n    Report Structure:\n    \n    Introduction\n    \n    Rising Sea Levels\n    2.1. Impact on Mangroves\n    2.2. Impact on Coral Reefs\n    \n    Ocean Acidification\n    3.1. Impact on Shellfish\n    3.2. Impact on Plankton\n    \n    Extreme Weather Events\n    4.1. Increased Storm Intensity\n    4.2. Coastal Erosion\n    \n    Conclusion\n    \n    Provide detailed analysis within each section.\n    \n\n**Response:**\n    \n    \n    <...>\n    Climate Change Impact on Coastal Ecosystems: A Detailed Report\n    \n    1. Introduction\n    \n    Coastal ecosystems, the dynamic interface between land and sea, are among the most productive and\n    biodiverse environments on Earth. They provide crucial ecosystem services, including coastal protection,\n    fisheries, carbon sequestration, and support for a vast array of species. However, these vital\n    ecosystems are increasingly threatened by the multifaceted impacts of climate change. Driven by\n    anthropogenic greenhouse gas emissions, climate change is manifesting in several interconnected ways\n    that are profoundly altering the physical and chemical conditions of coastal environments. This report\n    will delve into the key impacts of climate change on coastal ecosystems, focusing on rising sea levels,\n    ocean acidification, and extreme weather events, and their specific consequences for critical habitats\n    like mangroves and coral reefs, and key organisms like shellfish and plankton. Understanding these\n    impacts is crucial for developing effective conservation and management strategies to mitigate the\n    damage and enhance the resilience of these invaluable ecosystems.\n    \n    2. Rising Sea Levels\n    \n    Rising sea levels are a direct and significant consequence of global warming, driven by thermal\n    expansion of seawater and the melting of land ice (glaciers and ice sheets). This phenomenon poses a\n    direct threat to low-lying coastal ecosystems by inundating habitats, altering salinity regimes, and\n    increasing erosion rates. The rate of sea level rise is accelerating, and projections for the 21st\n    century indicate a continued upward trend, with potentially devastating consequences for coastal\n    environments.\n    \n    2.1. Impact on Mangroves\n    \n    Mangrove forests are unique coastal ecosystems found in tropical and subtropical intertidal zones.\n    They provide numerous benefits, including coastal protection against storms, nursery grounds for\n    fish and invertebrates, and significant carbon sequestration...\n    <...>\n    \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n## What's next?\n\n  * Try Gemini 2.5 Pro Preview in [Google AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro-preview-03-25).\n  * For more info about Gemini 2.5 Pro Preview and Gemini Flash 2.0 Thinking, see the [model page](/gemini-api/docs/gemini-api/docs/models).\n  * Try more examples in the [Thinking cookbook](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/function-calling",
    "title": "Function Calling with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Function Calling with the Gemini API \n\nFunction calling lets you connect models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:\n\n  * **Augment Knowledge:** Access information from external sources like databases, APIs, and knowledge bases.\n  * **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.\n  * **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices\n\n\n\nGet Weather Schedule Meeting Create Chart\n\n## How Function Calling Works\n\nFunction calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:\n\n  1. **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.\n  2. **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\n  3. **Execute Function Code (Your Responsibility):** The Model _does not_ execute the function itself. It's your application's responsibility to process the response and check for Function Call, if \n     * **Yes** : Extract the name and args of the function and execute the corresponding function in your application.\n     * **No:** The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).\n  4. **Create User friendly response:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.\n\n\n\nThis process can be repeated over multiple turns, allowing for complex interactions and workflows. The model also supports calling multiple functions in a single turn ([parallel function calling](/gemini-api/docs/function-calling#parallel_function_calling)) and in sequence ([compositional function calling](/gemini-api/docs/function-calling#compositional_function_calling)).\n\n### Step 1: Define Function Declaration\n\nDefine a function and its declaration within your application code that allows users to set light values and make an API request. This function could call external services or APIs.\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Define a function that the model can call to control smart lights\n    set_light_values_declaration = {\n        \"name\": \"set_light_values\",\n        \"description\": \"Sets the brightness and color temperature of a light.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n                },\n                \"color_temp\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"daylight\", \"cool\", \"warm\"],\n                    \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n                },\n            },\n            \"required\": [\"brightness\", \"color_temp\"],\n        },\n    }\n    \n    # This is the actual function that would be called based on the model's suggestion\n    def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n        \"\"\"Set the brightness and color temperature of a room light. (mock API).\n    \n        Args:\n            brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n            color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    \n        Returns:\n            A dictionary containing the set brightness and color temperature.\n        \"\"\"\n        return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n    \n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    // Define a function that the model can call to control smart lights\n    const setLightValuesFunctionDeclaration = {\n      name: 'set_light_values',\n      description: 'Sets the brightness and color temperature of a light.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',\n          },\n          color_temp: {\n            type: Type.STRING,\n            enum: ['daylight', 'cool', 'warm'],\n            description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',\n          },\n        },\n        required: ['brightness', 'color_temp'],\n      },\n    };\n    \n    /**\n    * Set the brightness and color temperature of a room light. (mock API)\n    * @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness\n    * @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    * @return {Object} A dictionary containing the set brightness and color temperature.\n    */\n    function setLightValues(brightness, color_temp) {\n      return {\n        brightness: brightness,\n        colorTemperature: color_temp\n      };\n    }\n    \n\n### Step 2: Call the model with function declarations\n\nOnce you have defined your function declarations, you can prompt the model to use the function. It analyzes the prompt and function declarations and decides to respond directly or to call a function. If a function is called the response object will contain a function call suggestion.\n\n### Python\n    \n    \n    from google import genai\n    \n    # Generation Config with Function Declaration\n    tools = types.Tool(function_declarations=[set_light_values_declaration])\n    config = types.GenerateContentConfig(tools=[tools])\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Define user prompt\n    contents = [\n        types.Content(\n            role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n        )\n    ]\n    \n    # Send request with function declarations\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\", config=config, contents=contents\n    )\n    \n    print(response.candidates[0].content.parts[0].function_call)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Generation Config with Function Declaration\n    const config = {\n      tools: [{\n        functionDeclarations: [setLightValuesFunctionDeclaration]\n      }]\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Define user prompt\n    const contents = [\n      {\n        role: 'user',\n        parts: [{ text: 'Turn the lights down to a romantic level' }]\n      }\n    ];\n    \n    // Send request with function declarations\n    const response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(response.functionCalls[0]); \n    \n\nThe model then returns a `functionCall` object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.\n\n### Python\n    \n    \n    id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'\n    \n\n### JavaScript\n    \n    \n    {\n      name: 'set_light_values',\n      args: { brightness: 25, color_temp: 'warm' }\n    }\n    \n\n### Step 3: Execute set_light_values function code\n\nExtract the function call details from the model's response, parse the arguments , and execute the `set_light_values` function in our code.\n\n### Python\n    \n    \n    # Extract tool call details\n    tool_call = response.candidates[0].content.parts[0].function_call\n    \n    if tool_call.name == \"set_light_values\":\n        result = set_light_values(**tool_call.args)\n        print(f\"Function execution result: {result}\")\n    \n\n### JavaScript\n    \n    \n    // Extract tool call details\n    const tool_call = response.functionCalls[0]\n    \n    let result;\n    if (tool_call.name === 'set_light_values') {\n      result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);\n      console.log(`Function execution result: ${JSON.stringify(result)}`);\n    }\n    \n\n### Step 4: Create User friendly response with function result and call the model again\n\nFinally, send the result of the function execution back to the model so it can incorporate this information into its final response to the user.\n\n### Python\n    \n    \n    # Create a function response part\n    function_response_part = types.Part.from_function_response(\n        name=tool_call.name,\n        response={\"result\": result},\n    )\n    \n    # Append function call and result of the function execution to contents\n    contents.append(types.Content(role=\"model\", parts=[types.Part(function_call=tool_call)])) # Append the model's function call message\n    contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n    \n    final_response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=config,\n        contents=contents,\n    )\n    \n    print(final_response.text)\n    \n\n### JavaScript\n    \n    \n    // Create a function response part\n    const function_response_part = {\n      name: tool_call.name,\n      response: { result }\n    }\n    \n    // Append function call and result of the function execution to contents\n    contents.push({ role: 'model', parts: [{ functionCall: tool_call }] });\n    contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });\n    \n    // Get the final response from the model\n    const final_response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(final_response.text);\n    \n\nThis completes the function calling flow. The Model successfully used the `set_light_values` function to perform the request action of the user.\n\n## Function declarations\n\nWhen you implement function calling in a prompt, you create a `tools` object, which contains one or more _`function declarations`_. You define functions using JSON, specifically with a [select subset](https://ai.google.dev/api/caching#Schema) of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schemawr) format. A single function declaration can include the following parameters:\n\n  * `name` (string): A unique name for the function (`get_weather_forecast`, `send_email`). Use descriptive names without spaces or special characters (use underscores or camelCase).\n  * `description` (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful (\"Finds theaters based on location and optionally movie title which is currently playing in theaters.\").\n  * `parameters` (object): Defines the input parameters the function expects. \n    * `type` (string): Specifies the overall data type, such as `object`.\n    * `properties` (object): Lists individual parameters, each with: \n      * `type` (string): The data type of the parameter, such as `string`, `integer`, `boolean, array`.\n      * `description` (string): A description of the parameter's purpose and format. Provide examples and constraints (\"The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.\").\n      * `enum` (array, optional): If the parameter values are from a fixed set, use \"enum\" to list the allowed values instead of just describing them in the description. This improves accuracy (\"enum\": [\"daylight\", \"cool\", \"warm\"]).\n    * `required` (array): An array of strings listing the parameter names that are mandatory for the function to operate.\n\n\n\n## Parallel Function Calling\n\nIn addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco.\n\n### Python\n    \n    \n    power_disco_ball = {\n        \"name\": \"power_disco_ball\",\n        \"description\": \"Powers the spinning disco ball.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"power\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether to turn the disco ball on or off.\",\n                }\n            },\n            \"required\": [\"power\"],\n        },\n    }\n    \n    start_music = {\n        \"name\": \"start_music\",\n        \"description\": \"Play some music matching the specified parameters.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"energetic\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is energetic or not.\",\n                },\n                \"loud\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is loud or not.\",\n                },\n            },\n            \"required\": [\"energetic\", \"loud\"],\n        },\n    }\n    \n    dim_lights = {\n        \"name\": \"dim_lights\",\n        \"description\": \"Dim the lights.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"number\",\n                    \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n                }\n            },\n            \"required\": [\"brightness\"],\n        },\n    }\n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    const powerDiscoBall = {\n      name: 'power_disco_ball',\n      description: 'Powers the spinning disco ball.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          power: {\n            type: Type.BOOLEAN,\n            description: 'Whether to turn the disco ball on or off.'\n          }\n        },\n        required: ['power']\n      }\n    };\n    \n    const startMusic = {\n      name: 'start_music',\n      description: 'Play some music matching the specified parameters.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          energetic: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is energetic or not.'\n          },\n          loud: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is loud or not.'\n          }\n        },\n        required: ['energetic', 'loud']\n      }\n    };\n    \n    const dimLights = {\n      name: 'dim_lights',\n      description: 'Dim the lights.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'\n          }\n        },\n        required: ['brightness']\n      }\n    };\n    \n\nCall the model with an instruction that could use all of the specified tools. This example uses a `tool_config`. To learn more you can read about [configuring function calling](/gemini-api/docs/function-calling#function_calling_modes).\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Set up function declarations\n    house_tools = [\n        types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])\n    ]\n    \n    config = {\n        \"tools\": house_tools,\n        \"automatic_function_calling\": {\"disable\": True},\n        # Force the model to call 'any' function, instead of chatting.\n        \"tool_config\": {\"function_calling_config\": {\"mode\": \"any\"}},\n    }\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Turn this place into a party!\")\n    \n    # Print out each of the function calls requested from this single call\n    print(\"Example 1: Forced function calling\")\n    for fn in response.function_calls:\n        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n        print(f\"{fn.name}({args})\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Set up function declarations\n    const houseFns = [powerDiscoBall, startMusic, dimLights];\n    \n    const config = {\n        tools: [{\n            functionDeclarations: houseFns\n        }],\n        // Force the model to call 'any' function, instead of chatting.\n        toolConfig: {\n            functionCallingConfig: {\n            mode: 'any'\n            }\n        }\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Create a chat session\n    const chat = ai.chats.create({\n        model: 'gemini-2.0-flash',\n        config: config\n    });\n    const response = await chat.sendMessage({message: 'Turn this place into a party!'});\n    \n    // Print out each of the function calls requested from this single call\n    console.log(\"Example 1: Forced function calling\");\n    for (const fn of response.functionCalls) {\n        const args = Object.entries(fn.args)\n            .map(([key, val]) => `${key}=${val}`)\n            .join(', ');\n        console.log(`${fn.name}(${args})`);\n    }\n    \n\nEach of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested. \n\nThe Python SDK supports a feature called [automatic function calling](/gemini-api/docs/function-calling#automatic_function_calling_python_only) which converts the Python function to declarations, handles the function call execution and response cycle for you. Following is an example for our disco use case.\n\n**Note:** Automatic Function Calling is a Python SDK only feature at the moment.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Actual implementation functions\n    def power_disco_ball_impl(power: bool) -> dict:\n        \"\"\"Powers the spinning disco ball.\n    \n        Args:\n            power: Whether to turn the disco ball on or off.\n    \n        Returns:\n            A status dictionary indicating the current state.\n        \"\"\"\n        return {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n    \n    def start_music_impl(energetic: bool, loud: bool) -> dict:\n        \"\"\"Play some music matching the specified parameters.\n    \n        Args:\n            energetic: Whether the music is energetic or not.\n            loud: Whether the music is loud or not.\n    \n        Returns:\n            A dictionary containing the music settings.\n        \"\"\"\n        music_type = \"energetic\" if energetic else \"chill\"\n        volume = \"loud\" if loud else \"quiet\"\n        return {\"music_type\": music_type, \"volume\": volume}\n    \n    def dim_lights_impl(brightness: float) -> dict:\n        \"\"\"Dim the lights.\n    \n        Args:\n            brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n    \n        Returns:\n            A dictionary containing the new brightness setting.\n        \"\"\"\n        return {\"brightness\": brightness}\n    \n    config = {\n        \"tools\": [power_disco_ball_impl, start_music_impl, dim_lights_impl],\n    }\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Do everything you need to this place into party!\")\n    \n    print(\"\\nExample 2: Automatic function calling\")\n    print(response.text)\n    # I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!\n    \n\n## Compositional Function Calling\n\nGemini 2.0 supports compositional function calling, meaning the model can chain multiple function calls together. For example, to answer \"Get the temperature in my current location\", the Gemini API might invoke both a `get_current_location()` function and a `get_weather()` function that takes the location as a parameter.\n\n**Note:** Compositional function calling is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Light control schemas\n    turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}\n    turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}\n    \n    prompt = \"\"\"\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n      \"\"\"\n    \n    tools = [\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}\n    ]\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Light control schemas\n    const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };\n    const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };\n    \n    const prompt = `\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n    `;\n    \n    const tools = [\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }\n    ];\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n## Function calling modes\n\nThe Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the `function_calling_config`.\n\n  * `AUTO (Default)`: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n  * `ANY`: The model is constrained to always predict a function call and guarantee function schema adherence. If `allowed_function_names` is not specified, the model can choose from any of the provided function declarations. If `allowed_function_names` is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call in response to every prompt (if applicable).\n  * `NONE`: The model is _prohibited_ from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.\n\n\n\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Configure function calling mode\n    tool_config = types.ToolConfig(\n        function_calling_config=types.FunctionCallingConfig(\n            mode=\"ANY\", allowed_function_names=[\"get_current_temperature\"]\n        )\n    )\n    \n    # Create the generation config\n    config = types.GenerateContentConfig(\n        temperature=0,\n        tools=[tools],  # not defined here.\n        tool_config=tool_config,\n    )\n    \n\n### JavaScript\n    \n    \n    import { FunctionCallingConfigMode } from '@google/genai';\n    \n    // Configure function calling mode\n    const toolConfig = {\n      functionCallingConfig: {\n        mode: FunctionCallingConfigMode.ANY,\n        allowedFunctionNames: ['get_current_temperature']\n      }\n    };\n    \n    // Create the generation config\n    const config = {\n      temperature: 0,\n      tools: tools, // not defined here.\n      toolConfig: toolConfig,\n    };\n    \n\n## Automatic Function Calling (Python Only)\n\nWhen using the Python SDK, you can provide Python functions directly as tools. The SDK automatically converts the Python function to declarations, handles the function call execution and response cycle for you. The Python SDK then automatically:\n\n  1. Detects function call responses from the model.\n  2. Call the corresponding Python function in your code.\n  3. Sends the function response back to the model.\n  4. Returns the model's final text response.\n\n\n\nTo use this, define your function with type hints and a docstring, and then pass the function itself (not a JSON declaration) as a tool:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Define the function with type hints and docstring\n    def get_current_temperature(location: str) -> dict:\n        \"\"\"Gets the current temperature for a given location.\n    \n        Args:\n            location: The city and state, e.g. San Francisco, CA\n    \n        Returns:\n            A dictionary containing the temperature and unit.\n        \"\"\"\n        # ... (implementation) ...\n        return {\"temperature\": 25, \"unit\": \"Celsius\"}\n    \n    # Configure the client and model\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))  # Replace with your actual API key setup\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature]\n    )  # Pass the function itself\n    \n    # Make the request\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=\"What's the temperature in London?\",\n        config=config,\n    )\n    \n    print(response.text)  # The SDK handles the function call and returns the final text\n    \n\nYou can disable automatic function calling with:\n\n### Python\n    \n    \n    # To disable automatic function calling:\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature],\n        automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n    )\n    \n\n### Automatic Function schema declaration\n\nAutomatic schema extraction from Python functions doesn't work in all cases. For example: it doesn't handle cases where you describe the fields of a nested dictionary-object. The API is able to describe any of the following types:\n\n### Python\n    \n    \n    AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])\n    \n\nTo see what the inferred schema looks like, you can convert it using [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable):\n\n### Python\n    \n    \n    def multiply(a: float, b: float):\n        \"\"\"Returns a * b.\"\"\"\n        return a * b\n    \n    fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)\n    \n    # to_json_dict() provides a clean JSON representation.\n    print(fn_decl.to_json_dict())\n    \n\n## Multi-tool use: Combine Native Tools with Function Calling\n\nWith Gemini 2.0, you can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, [Grounding with Google Search](/gemini-api/docs/grounding) and [code execution](/gemini-api/docs/code-execution), in a request using the [Live API](/gemini-api/docs/live).\n\n**Note:** Multi-tool use is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Multiple tasks example - combining lights, code execution, and search\n    prompt = \"\"\"\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n      \"\"\"\n    \n    tools = [\n        {'google_search': {}},\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.\n    ]\n    \n    # Execute the prompt with specified tools in audio modality\n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Multiple tasks example - combining lights, code execution, and search\n    const prompt = `\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n    `;\n    \n    const tools = [\n      { googleSearch: {} },\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.\n    ];\n    \n    // Execute the prompt with specified tools in audio modality\n    await run(prompt, {tools: tools, modality: \"AUDIO\"});\n    \n\nPython developers can try this out in the [Live API Tool Use notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).\n\n## Use Model Context Protocol (MCP)\n\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard to connect AI applications with external tools, data sources, and systems. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts. You can use models with MCP server using their tool calling capabilities.\n\nMCP servers expose the tools as JSON schema definitions, which can be used with Gemini compatible function declarations. This lets you to use a MCP server with Gemini models directly. Here, you can find an example of how to use a local MCP server with Gemini SDK and the `mcp` SDK.\n\n### Python\n    \n    \n    import asyncio\n    import os\n    from datetime import datetime\n    from google import genai\n    from google.genai import types\n    from mcp import ClientSession, StdioServerParameters\n    from mcp.client.stdio import stdio_client\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Create server parameters for stdio connection\n    server_params = StdioServerParameters(\n        command=\"npx\",  # Executable\n        args=[\"-y\", \"@philschmid/weather-mcp\"],  # Weather MCP Server\n        env=None,  # Optional environment variables\n    )\n    \n    async def run():\n        async with stdio_client(server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                # Prompt to get the weather for the current day in London.\n                prompt = f\"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?\"\n                # Initialize the connection between client and server\n                await session.initialize()\n    \n                # Get tools from MCP session and convert to Gemini Tool objects\n                mcp_tools = await session.list_tools()\n                tools = [\n                    types.Tool(\n                        function_declarations=[\n                            {\n                                \"name\": tool.name,\n                                \"description\": tool.description,\n                                \"parameters\": {\n                                    k: v\n                                    for k, v in tool.inputSchema.items()\n                                    if k not in [\"additionalProperties\", \"$schema\"]\n                                },\n                            }\n                        ]\n                    )\n                    for tool in mcp_tools.tools\n                ]\n    \n                # Send request to the model with MCP function declarations\n                response = client.models.generate_content(\n                    model=\"gemini-2.0-flash\",\n                    contents=prompt,\n                    config=types.GenerateContentConfig(\n                        temperature=0,\n                        tools=tools,\n                    ),\n                )\n    \n                # Check for a function call\n                if response.candidates[0].content.parts[0].function_call:\n                    function_call = response.candidates[0].content.parts[0].function_call\n                    print(function_call)\n                    # Call the MCP server with the predicted tool\n                    result = await session.call_tool(\n                        function_call.name, arguments=function_call.args\n                    )\n                    print(result.content[0].text)\n                    # Continue as shown in step 4 of \"How Function Calling Works\"\n                    # and create a user friendly response\n                else:\n                    print(\"No function call found in the response.\")\n                    print(response.text)\n    \n    # Start the asyncio event loop and run the main function\n    asyncio.run(run())\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    import { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\n    import { StdioClientTransport } from \"@modelcontextprotocol/sdk/client/stdio.js\";\n    \n    // Create server parameters for stdio connection\n    const serverParams = new StdioClientTransport({\n      command: \"npx\",\n      args: [\"-y\", \"@philschmid/weather-mcp\"]\n    });\n    \n    const client = new Client(\n      {\n        name: \"example-client\",\n        version: \"1.0.0\"\n      }\n    );\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Initialize the connection between client and server\n    await client.connect(serverParams);\n    \n    // Get tools from MCP session and convert to Gemini Tool objects\n    const mcpTools = await client.listTools();\n    const tools = mcpTools.tools.map((tool) => {\n      // Filter the parameters to exclude not supported keys\n      const parameters = Object.fromEntries(\n        Object.entries(tool.inputSchema).filter(([key]) => ![\"additionalProperties\", \"$schema\"].includes(key))\n      );\n      return {\n        name: tool.name,\n        description: tool.description,\n        parameters: parameters\n      };\n    });\n    \n    // Send request to the model with MCP function declarations\n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: \"What is the weather in London in the UK on 2024-04-04?\",\n      config: {\n        tools: [{\n          functionDeclarations: tools\n        }],\n      },\n    });\n    \n    // Check for function calls in the response\n    if (response.functionCalls && response.functionCalls.length > 0) {\n      const functionCall = response.functionCalls[0]; // Assuming one function call\n      console.log(`Function to call: ${functionCall.name}`);\n      console.log(`Arguments: ${JSON.stringify(functionCall.args)}`);\n      // Call the MCP server with the predicted tool\n      const result = await client.callTool({name: functionCall.name, arguments: functionCall.args});\n      console.log(result.content[0].text);\n      // Continue as shown in step 4 of \"How Function Calling Works\"\n      // and create a user friendly response\n    } else {\n      console.log(\"No function call found in the response.\");\n      console.log(response.text);\n    }\n    \n    // Close the connection\n    await client.close();\n    \n\n## Supported Models\n\nExperimental models are not included. You can find their capabilities on the [model overview](https://ai.google.dev/gemini-api/docs/models) page.\n\nModel | Function Calling | Parallel Function Calling | Compositional Function Calling(Live API only)  \n---|---|---|---  \nGemini 2.0 Flash | ✔️ | ✔️ | ✔️  \nGemini 2.0 Flash-Lite | X | X | X  \nGemini 1.5 Flash | ✔️ | ✔️ | ✔️  \nGemini 1.5 Pro | ✔️ | ✔️ | ✔️  \n  \n## Best Practices\n\n  * **Function and Parameter Descriptions:** Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.\n  * **Naming:** Use descriptive function names (without spaces, periods, or dashes).\n  * **Strong Typing:** Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.\n  * **Tool Selection:** While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.\n  * **Prompt Engineering:**\n    * Provide context: Tell the model its role (e.g., \"You are a helpful weather assistant.\").\n    * Give instructions: Specify how and when to use functions (e.g., \"Don't guess dates; always use a future date for forecasts.\").\n    * Encourage clarification: Instruct the model to ask clarifying questions if needed.\n  * **Temperature:** Use a low temperature (e.g., 0) for more deterministic and reliable function calls.\n  * **Validation:** If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.\n  * **Error Handling** : Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.\n  * **Security:** Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.\n  * **Token Limits:** Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.\n\n\n\n## Notes and Limitations\n\n  * Only a [subset of the OpenAPI schema](https://ai.google.dev/api/caching#FunctionDeclaration) is supported.\n  * Supported parameter types in Python are limited.\n  * Automatic function calling is a Python SDK feature only.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-07 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-07 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/document-processing",
    "title": "Explore document processing capabilities with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Explore document processing capabilities with the Gemini API \n\nPython JavaScript Go REST\n\nThe Gemini API supports PDF input, including long documents (up to 3600 pages). Gemini models process PDFs with native vision, and are therefore able to understand both text and image contents inside documents. With native PDF vision support, Gemini models are able to:\n\n  * Analyze diagrams, charts, and tables inside documents.\n  * Extract information into structured output formats.\n  * Answer questions about visual and text contents in documents.\n  * Summarize documents.\n  * Transcribe document content (e.g. to HTML) preserving layouts and formatting, for use in downstream applications (such as in RAG pipelines).\n\n\n\nThis tutorial demonstrates some possible ways to use the Gemini API with PDF documents. All output is text-only.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## What's next\n\nThis guide shows how to use [`generateContent`](/api/generate-content#method:-models.generatecontent) and to generate text outputs from processed documents. To learn more, see the following resources:\n\n  * [File prompting strategies](/gemini-api/docs/file-prompting-strategies): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.\n  * [System instructions](/gemini-api/docs/system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.\n  * [Safety guidance](/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-10 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-10 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/grounding",
    "title": "Grounding with Google Search  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Grounding with Google Search \n\nPython JavaScript REST\n\nThe Grounding with Google Search feature in the Gemini API and AI Studio can be used to improve the accuracy and recency of responses from the model. In addition to more factual responses, when Grounding with Google Search is enabled, the Gemini API returns grounding sources (in-line supporting links) and Google Search Suggestions along with the response content. The Search Suggestions point users to the search results corresponding to the grounded response.\n\nThis guide will help you get started with Grounding with Google Search.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Google Search Suggestions\n\nTo use Grounding with Google Search, you have to display Google Search Suggestions, which are suggested queries included in the metadata of the grounded response. To learn more about the display requirements, see [Use Google Search Suggestions](/gemini-api/docs/grounding/search-suggestions).\n\n## Google Search retrieval\n\n**Note:** Google Search retrieval is only compatible with Gemini 1.5 models. For Gemini 2.0 models, you should use Search as a tool.\n\n### Dynamic retrieval\n\n**Note:** Dynamic retrieval is only compatible with Gemini 1.5 Flash. For Gemini 2.0, you should use Search as a tool, as shown above.\n\nSome queries are likely to benefit more from Grounding with Google Search than others. The _dynamic retrieval_ feature gives you additional control over when to use Grounding with Google Search.\n\nIf the dynamic retrieval mode is unspecified, Grounding with Google Search is always triggered. If the mode is set to dynamic, the model decides when to use grounding based on a threshold that you can configure. The threshold is a floating-point value in the range [0,1] and defaults to 0.3. If the threshold value is 0, the response is always grounded with Google Search; if it's 1, it never is.\n\n#### How dynamic retrieval works\n\nYou can use dynamic retrieval in your request to choose when to turn on Grounding with Google Search. This is useful when the prompt doesn't require an answer grounded in Google Search and the model can provide an answer based on its own knowledge without grounding. This helps you manage latency, quality, and cost more effectively.\n\nBefore you invoke the dynamic retrieval configuration in your request, understand the following terminology:\n\n  * **Prediction score** : When you request a grounded answer, Gemini assigns a _prediction score_ to the prompt. The prediction score is a floating point value in the range [0,1]. Its value depends on whether the prompt can benefit from grounding the answer with the most up-to-date information from Google Search. Thus, if a prompt requires an answer grounded in the most recent facts on the web, it has a higher prediction score. A prompt for which a model-generated answer is sufficient has a lower prediction score.\n\nHere are examples of some prompts and their prediction scores.\n\n**Note:** The prediction scores are assigned by Gemini and can vary over time depending on several factors. Prompt | Prediction score | Comment  \n---|---|---  \n\"Write a poem about peonies\" | 0.13 | The model can rely on its knowledge and the answer doesn't need grounding.  \n\"Suggest a toy for a 2yo child\" | 0.36 | The model can rely on its knowledge and the answer doesn't need grounding.  \n\"Can you give a recipe for an asian-inspired guacamole?\" | 0.55 | Google Search can give a grounded answer, but grounding isn't strictly required; the model knowledge might be sufficient.  \n\"What's Agent Builder? How is grounding billed in Agent Builder?\" | 0.72 | Requires Google Search to generate a well-grounded answer.  \n\"Who won the latest F1 grand prix?\" | 0.97 | Requires Google Search to generate a well-grounded answer.  \n  * **Threshold** : In your API request, you can specify a dynamic retrieval configuration with a threshold. The threshold is a floating point value in the range [0,1] and defaults to 0.3. If the threshold value is zero, the response is always grounded with Google Search. For all other values of threshold, the following is applicable:\n\n    * If the prediction score is greater than or equal to the threshold, the answer is grounded with Google Search. A lower threshold implies that more prompts have responses that are generated using Grounding with Google Search.\n    * If the prediction score is less than the threshold, the model might still generate the answer, but it isn't grounded with Google Search.\n\n\n\nTo learn how to set the dynamic retrieval threshold using an SDK or the REST API, see the appropriate code example.\n\nTo find a good threshold that suits your business needs, you can create a representative set of queries that you expect to encounter. Then you can sort the queries according to the prediction score in the response and select a good threshold for your use case.\n\n## A grounded response\n\nIf your prompt successfully grounds to Google Search, the response will include `groundingMetadata`. A grounded response might look something like this (parts of the response have been omitted for brevity):\n    \n    \n    {\n      \"candidates\": [\n        {\n          \"content\": {\n            \"parts\": [\n              {\n                \"text\": \"Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships. He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall. \\n\"\n              }\n            ],\n            \"role\": \"model\"\n          },\n          ...\n          \"groundingMetadata\": {\n            \"searchEntryPoint\": {\n              \"renderedContent\": \"\\u003cstyle\\u003e\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n\\u003c/style\\u003e\\n\\u003cdiv class=\\\"container\\\"\\u003e\\n  \\u003cdiv class=\\\"headline\\\"\\u003e\\n    \\u003csvg class=\\\"logo-light\\\" width=\\\"18\\\" height=\\\"18\\\" viewBox=\\\"9 9 35 35\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\"\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\\\" fill=\\\"#4285F4\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\\\" fill=\\\"#34A853\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\\\" fill=\\\"#FBBC05\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\\\" fill=\\\"#EA4335\\\"/\\u003e\\n    \\u003c/svg\\u003e\\n    \\u003csvg class=\\\"logo-dark\\\" width=\\\"18\\\" height=\\\"18\\\" viewBox=\\\"0 0 48 48\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\"\\u003e\\n      \\u003ccircle cx=\\\"24\\\" cy=\\\"23\\\" fill=\\\"#FFF\\\" r=\\\"22\\\"/\\u003e\\n      \\u003cpath d=\\\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\\\" fill=\\\"#4285F4\\\"/\\u003e\\n      \\u003cpath d=\\\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\\\" fill=\\\"#34A853\\\"/\\u003e\\n      \\u003cpath d=\\\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\\\" fill=\\\"#FBBC05\\\"/\\u003e\\n      \\u003cpath d=\\\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\\\" fill=\\\"#EA4335\\\"/\\u003e\\n    \\u003c/svg\\u003e\\n    \\u003cdiv class=\\\"gradient-container\\\"\\u003e\\u003cdiv class=\\\"gradient\\\"\\u003e\\u003c/div\\u003e\\u003c/div\\u003e\\n  \\u003c/div\\u003e\\n  \\u003cdiv class=\\\"carousel\\\"\\u003e\\n    \\u003ca class=\\\"chip\\\" href=\\\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4x8Epe-gzpwRBvp7o3RZh2m1ygq1EHktn0OWCtvTXjad4bb1zSuqfJd6OEuZZ9_SXZ_P2SvCpJM7NaFfQfiZs6064MeqXego0vSbV9LlAZoxTdbxWK1hFeqTG6kA13YJf7Fbu1SqBYM0cFM4zo0G_sD9NKYWcOCQMvDLDEJFhjrC9DM_QobBIAMq-gWN95G5tvt6_z6EuPN8QY=\\\"\\u003ewho won wimbledon 2024\\u003c/a\\u003e\\n  \\u003c/div\\u003e\\n\\u003c/div\\u003e\\n\"\n            },\n            \"groundingChunks\": [\n              {\n                \"web\": {\n                  \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4whET1ta3sDETZvcicd8FeNe4z0VuduVsxrT677KQRp2rYghXI0VpfYbIMVI3THcTuMwggRCbFXS_wVvW0UmGzMe9h2fyrkvsnQPJyikJasNIbjJLPX0StM4Bd694-ZVle56MmRA4YiUvwSqad1w6O2opmWnw==\",\n                  \"title\": \"wikipedia.org\"\n                }\n              },\n              {\n                \"web\": {\n                  \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wR1M-9-yMPUr_KdHlnoAmQ8ZX90DtQ_vDYTjtP2oR5RH4tRP04uqKPLmesvo64BBkPeYLC2EpVDxv9ngO3S1fs2xh-e78fY4m0GAtgNlahUkm_tBm_sih5kFPc7ill9u2uwesNGUkwrQlmP2mfWNU5lMMr23HGktr6t0sV0QYlzQq7odVoBxYWlQ_sqWFH\",\n                  \"title\": \"wikipedia.org\"\n                }\n              },\n              {\n                \"web\": {\n                  \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wsDmROzbP-tmt8GdwCW_pqISTZ4IRbBuoaMyaHfcQg8WW-yKRQQvMDTPAuLxJh-8_U8_iw_6JKFbQ8M9oVYtaFdWFK4gOtL4RrC9Jyqc5BNpuxp6uLEKgL5-9TggtNvO97PyCfziDFXPsxylwI1HcfQdrz3Jy7ZdOL4XM-S5rC0lF2S3VWW0IEAEtS7WX861meBYVjIuuF_mIr3spYPqWLhbAY2Spj-4_ba8DjRvmevIFUhRuESTKvBfmpxNSM\",\n                  \"title\": \"cbssports.com\"\n                }\n              },\n              {\n                \"web\": {\n                  \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4yzjLkorHiUKjhOPkWaZ9b4cO-cLG-02vlEl6xTBjMUjyhK04qSIclAa7heR41JQ6AAVXmNdS3WDrLOV4Wli-iezyzW8QPQ4vgnmO_egdsuxhcGk3-Fp8-yfqNLvgXFwY5mPo6QRhvplOFv0_x9mAcka18QuAXtj0SPvJfZhUEgYLCtCrucDS5XFc5HmRBcG1tqFdKSE1ihnp8KLdaWMhrUQI21hHS9\",\n                  \"title\": \"jagranjosh.com\"\n                }\n              },\n              {\n                \"web\": {\n                  \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4y9L4oeNGWCatFz63b9PpP3ys-Wi_zwnkUT5ji9lY7gPUJQcsmmE87q88GSdZqzcx5nZG9usot5FYk2yK-FAGvCRE6JsUQJB_W11_kJU2HVV1BTPiZ4SAgm8XDFIxpCZXnXmEx5HUfRqQm_zav7CvS2qjA2x3__qLME6Jy7R5oza1C5_aqjQu422le9CaigThS5bvJoMo-ZGcXdBUCj2CqoXNVjMA==\",\n                  \"title\": \"apnews.com\"\n                }\n              }\n            ],\n            \"groundingSupports\": [\n              {\n                \"segment\": {\n                  \"endIndex\": 85,\n                  \"text\": \"Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships.\"\n                },\n                \"groundingChunkIndices\": [\n                  0,\n                  1,\n                  2,\n                  3\n                ],\n                \"confidenceScores\": [\n                  0.97380733,\n                  0.97380733,\n                  0.97380733,\n                  0.97380733\n                ]\n              },\n              {\n                \"segment\": {\n                  \"startIndex\": 86,\n                  \"endIndex\": 210,\n                  \"text\": \"He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall.\"\n                },\n                \"groundingChunkIndices\": [\n                  1,\n                  0,\n                  4\n                ],\n                \"confidenceScores\": [\n                  0.96145374,\n                  0.96145374,\n                  0.96145374\n                ]\n              }\n            ],\n            \"webSearchQueries\": [\n              \"who won wimbledon 2024\"\n            ]\n          }\n        }\n      ],\n      ...\n    }\n    \n\nIf the response doesn't include `groundingMetadata`, this means the response wasn't successfully grounded. There are several reasons this could happen, including low source relevance or incomplete information within the model response.\n\nWhen a grounded result is generated, the metadata contains URIs that redirect to the publishers of the content that was used to generate the grounded result. These URIs contain the `vertexaisearch` subdomain, as in this truncated example: `https://vertexaisearch.cloud.google.com/grounding-api-redirect/...`. The metadata also contains the publishers' domains. The provided URIs remain accessible for 30 days after the grounded result is generated.\n\n**Important:** The provided URIs must be directly accessible by the end users and must not be queried programmatically through automated means. If automated access is detected, the grounded answer generation service might stop providing the redirection URIs.\n\nThe `renderedContent` field within `searchEntryPoint` is the provided code for implementing Google Search Suggestions. See [Use Google Search Suggestions](/gemini-api/docs/grounding/search-suggestions) to learn more.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-11 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-11 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/grounding/search-suggestions",
    "title": "Use Google Search Suggestions  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Use Google Search Suggestions \n\nTo use Grounding with Google Search, you must enable Google Search Suggestions, which help users find search results corresponding to a grounded response.\n\nSpecifically, you need to display the search queries that are included in the grounded response's metadata. The response includes:\n\n  * `content`: LLM generated response\n  * `webSearchQueries`: The queries to be used for Google Search Suggestions\n\n\n\nFor example, in the following code snippet, Gemini responds to a search-grounded prompt which is asking about a type of tropical plant.\n    \n    \n    \"predictions\": [\n      {\n        \"content\": \"Monstera is a type of vine that thrives in bright indirect light…\",\n        \"groundingMetadata\": {\n          \"webSearchQueries\": [\"What's a monstera?\"],\n        }\n      }\n    ]\n    \n\nYou can take this output and display it by using Google Search Suggestions.\n\n## Requirements for Google Search Suggestions\n\n**Do** :\n\n  * Display the Search Suggestion exactly as provided without any modifications while complying with the Display Requirements.\n  * Take users directly to the Google Search results page (SRP) when they interact with the Search Suggestion.\n\n\n\n**Don't** :\n\n  * Include any interstitial screens or additional steps between the user's tap and the display of the SRP.\n  * Display any other search results or suggestions alongside the Search Suggestion or associated grounded LLM response.\n\n\n\n### Display requirements\n\n  * Display the Search Suggestion exactly as provided and don't make any modifications to colors, fonts, or appearance. Ensure the Search Suggestion renders as specified in the following mocks, including for light and dark mode: \n\n  * Whenever a grounded response is shown, its corresponding Google Search Suggestion should remain visible.\n\n  * Branding: You must strictly follow [Google's Guidelines for Third Party Use of Google Brand Features](https://about.google/brand-resource-center/).\n\n  * Google Search Suggestions should be at minimum the full width of the grounded response.\n\n\n\n\n### Behavior on tap\n\nWhen a user taps the chip, they are taken directly to a Google Search results page (SRP) for the search term displayed in the chip. The SRP can open either within your in-app browser or in a separate browser app. It's important to not minimize, remove, or obstruct the SRP's display in any way. The following animated mockup illustrates the tap-to-SRP interaction.\n\n## Code to implement a Google Search Suggestion\n\nWhen you use the API to ground a response to search, the model response provides compliant HTML and CSS styling in the `renderedContent` field which you implement to display Search Suggestions in your application. To see an example of the API response, see the response section in [Grounding with Google Search](/gemini-api/docs/grounding).\n\n**Note:** The provided HTML and CSS provided in the API response automatically adapts to the user's device settings, displaying in either light or dark mode based on the user's preference indicated by `@media(prefers-color-scheme)`.\n\n## What's next\n\n  * Learn how to [build an interactive chat](/gemini-api/docs/text-generation?lang=node#chat).\n  * Learn how to [use Gemini safely and responsibly](/gemini-api/docs/safety-guidance).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/model-tuning",
    "title": "Fine-tuning with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Fine-tuning with the Gemini API \n\nPrompt design strategies such as few-shot prompting may not always produce the results you need. _Fine-tuning_ is a process that can improve your model's performance on specific tasks or help the model adhere to specific output requirements when instructions aren't sufficient and you have a set of examples that demonstrate the outputs you want.\n\nThis page provides a conceptual overview of fine-tuning the text model behind the Gemini API text service. When you're ready to start tuning, try the [fine-tuning tutorial](/gemini-api/docs/model-tuning/tutorial). If you'd like a more general introduction to customizing LLMs for specific use cases, check out [LLMs: Fine-tuning, distillation, and prompt engineering](https://developers.google.com/machine-learning/crash-course/llm/tuning) in the [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/).\n\n## How fine-tuning works\n\nThe goal of fine-tuning is to further improve the performance of the model for your specific task. Fine-tuning works by providing the model with a training dataset containing many examples of the task. For niche tasks, you can get significant improvements in model performance by tuning the model on a modest number of examples. This kind of model tuning is sometimes referred to as _supervised fine-tuning_ , to distinguish it from other kinds of fine-tuning.\n\nYour training data should be structured as examples with prompt inputs and expected response outputs. You can also tune models using example data directly in Google AI Studio. The goal is to teach the model to mimic the wanted behavior or task, by giving it many examples illustrating that behavior or task.\n\nWhen you run a tuning job, the model learns additional parameters that help it encode the necessary information to perform the wanted task or learn the wanted behavior. These parameters can then be used at inference time. The output of the tuning job is a new model, which is effectively a combination of the newly learned parameters and the original model.\n\n## Prepare your dataset\n\nBefore you can start fine-tuning, you need a dataset to tune the model with. For the best performance, the examples in the dataset should be of high quality, diverse, and representative of real inputs and outputs.\n\n### Format\n\n**Note:** Fine-tuning only supports input-output pair examples. Chat-style multi-turn conversations are not supported at this time.\n\nThe examples included in your dataset should match your expected production traffic. If your dataset contains specific formatting, keywords, instructions, or information, the production data should be formatted in the same way and contain the same instructions.\n\nFor example, if the examples in your dataset include a `\"question:\"` and a `\"context:\"`, production traffic should also be formatted to include a `\"question:\"` and a `\"context:\"` in the same order as it appears in the dataset examples. If you exclude the context, the model can't recognize the pattern, even if the exact question was in an example in the dataset.\n\nAs another example, here's Python training data for an application that generates the next number in a sequence:\n    \n    \n    training_data = [\n      {\"text_input\": \"1\", \"output\": \"2\"},\n      {\"text_input\": \"3\", \"output\": \"4\"},\n      {\"text_input\": \"-3\", \"output\": \"-2\"},\n      {\"text_input\": \"twenty two\", \"output\": \"twenty three\"},\n      {\"text_input\": \"two hundred\", \"output\": \"two hundred one\"},\n      {\"text_input\": \"ninety nine\", \"output\": \"one hundred\"},\n      {\"text_input\": \"8\", \"output\": \"9\"},\n      {\"text_input\": \"-98\", \"output\": \"-97\"},\n      {\"text_input\": \"1,000\", \"output\": \"1,001\"},\n      {\"text_input\": \"10,100,000\", \"output\": \"10,100,001\"},\n      {\"text_input\": \"thirteen\", \"output\": \"fourteen\"},\n      {\"text_input\": \"eighty\", \"output\": \"eighty one\"},\n      {\"text_input\": \"one\", \"output\": \"two\"},\n      {\"text_input\": \"three\", \"output\": \"four\"},\n      {\"text_input\": \"seven\", \"output\": \"eight\"},\n    ]\n    \n\nAdding a prompt or preamble to each example in your dataset can also help improve the performance of the tuned model. Note, if a prompt or preamble is included in your dataset, it should also be included in the prompt to the tuned model at inference time.\n\n### Limitations\n\n**Note:** Fine-tuning datasets for Gemini 1.5 Flash have the following limitations:\n\n  * The maximum input size per example is 40,000 characters.\n  * The maximum output size per example is 5,000 characters.\n\n\n\n### Training data size\n\nYou can fine-tune a model with as little as 20 examples. Additional data generally improves the quality of the responses. You should target between 100 and 500 examples, depending on your application. The following table shows recommended dataset sizes for fine-tuning a text model for various common tasks:\n\nTask  | No. of examples in dataset   \n---|---  \nClassification  | 100+   \nSummarization  | 100-500+   \nDocument search  | 100+   \n  \n## Upload your tuning dataset\n\nData is either passed inline using the API or through files uploaded in Google AI Studio.\n\nTo use the client library, provide the data file in the `createTunedModel` call. File size limit is 4 MB. See the [fine-tuning quickstart with Python](/gemini-api/docs/model-tuning/tutorial?lang=python) to get started.\n\nTo call the REST API using cURL, provide training examples in JSON format to the `training_data` argument. See the [tuning quickstart with cURL](/gemini-api/docs/model-tuning/tutorial?lang=rest) to get started.\n\n## Advanced tuning settings\n\nWhen creating a tuning job, you can specify the following advanced settings:\n\n  * **Epochs:** A full training pass over the entire training set such that each example has been processed once.\n  * **Batch size:** The set of examples used in one training [iteration](https://developers.google.com/machine-learning/glossary#iteration). The batch size determines the number of examples in a batch.\n  * **Learning rate:** A floating-point number that tells the algorithm how strongly to adjust the model parameters on each iteration. For example, a learning rate of 0.3 would adjust weights and biases three times more powerfully than a learning rate of 0.1. High and low learning rates have their own unique trade-offs and should be adjusted based on your use case.\n  * **Learning rate multiplier:** The rate multiplier modifies the model's original learning rate. A value of 1 uses the original learning rate of the model. Values greater than 1 increase the learning rate and values between 1 and 0 lower the learning rate.\n\n\n\n### Recommended configurations\n\nThe following table shows the recommended configurations for fine-tuning a foundation model:\n\nHyperparameter | Default value | Recommended adjustments  \n---|---|---  \nEpoch | 5 |  If the loss starts to plateau before 5 epochs, use a smaller value. If the loss is converging and doesn't seem to plateau, use a higher value.  \nBatch size | 4 |   \nLearning rate | 0.001 | Use a smaller value for smaller datasets.  \n  \nThe loss curve shows how much the model's prediction deviates from the ideal predictions in the training examples after each epoch. Ideally you want to stop training at the lowest point in the curve right before it plateaus. For example, the graph below shows the loss curve plateauing at about epoch 4-6 which means you can set the `Epoch` parameter to 4 and still get the same performance.\n\n## Check the tuning job status\n\nYou can check the status of your tuning job in Google AI Studio under the **My Library** tab or using the `metadata` property of the tuned model in the Gemini API.\n\n## Troubleshoot errors\n\nThis section includes tips on how to resolve errors you may encounter while creating your tuned model.\n\n### Authentication\n\n**Note:** Starting September 30, 2024, OAuth authentication is no longer required. New projects should use API key authentication instead.\n\nTuning using the API and client library requires authentication. You can set up authentication using either an API key (recommended) or using OAuth credentials. For documentation on setting up an API key, see [Set up API key](/gemini-api/docs/quickstart#set-up-api-key).\n\nIf you see a `'PermissionDenied: 403 Request had insufficient authentication scopes'` error, you may need to set up user authentication using OAuth credentials. To configure OAuth credentials for Python, visit our [the OAuth setup tutorial](/gemini-api/docs/oauth).\n\n### Canceled models\n\nYou can cancel a fine-tuning job any time before the job is finished. However, the inference performance of a canceled model is unpredictable, particularly if the tuning job is canceled early in the training. If you canceled because you want to stop the training at an earlier epoch, you should create a new tuning job and set the epoch to a lower value.\n\n## Limitations of tuned models\n\n**Note:** Tuned models have the following limitations:\n\n  * The input limit of a tuned Gemini 1.5 Flash model is 40,000 characters.\n  * JSON mode is not supported with tuned models.\n  * Only text input is supported.\n\n\n\n## What's next\n\nGet started with the fine-tuning tutorials:\n\n  * [Fine-tuning tutorial (Python)](/gemini-api/docs/model-tuning/tutorial?lang=python)\n  * [Fine-tuning tutorial (REST)](/gemini-api/docs/model-tuning/tutorial?lang=rest)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial",
    "title": "Fine-tuning tutorial  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Fine-tuning tutorial \n\nPython REST\n\nThis tutorial will help you get started with the Gemini API tuning service using either the Python SDK or the REST API using [curl](https://curl.se/). The examples show how to tune the text model behind the Gemini API text generation service.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/embeddings",
    "title": "Embeddings  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Embeddings \n\n**Note:** Introducing our first Gemini embedding model, [available now to developers](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) as `gemini-embedding-exp-03-07` in the API.\n\nThe Gemini API supports several embedding models that generate embeddings for words, phrases, code, and sentences. The resulting embeddings can then be used for tasks such as semantic search, text classification, and clustering, among many others.\n\n## What are embeddings?\n\nEmbeddings are numerical representations of text (or other media formats) that capture relationships between inputs. Text embeddings work by converting text into arrays of floating point numbers, called _vectors_. These vectors are designed to capture the meaning of the text. The length of the embedding array is called the vector's _dimensionality_. A passage of text might be represented by a vector containing hundreds of dimensions.\n\nEmbeddings capture semantic meaning and context, which results in text with similar meanings having \"closer\" embeddings. For example, the sentence \"I took my dog to the vet\" and \"I took my cat to the vet\" would have embeddings that are close to each other in the vector space.\n\nYou can use embeddings to compare different texts and understand how they relate. For example, if the embeddings of the text \"cat\" and \"dog\" are close together you can infer that these words are similar in meaning, context, or both. This enables a variety of [common AI use cases](/gemini-api/docs/embeddings#use-cases).\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Generate embeddings\n\nUse the `embedContent` method to generate text embeddings:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    result = client.models.embed_content(\n            model=\"gemini-embedding-exp-03-07\",\n            contents=\"What is the meaning of life?\")\n    \n    print(result.embeddings)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    async function main() {\n    \n        const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n        const response = await ai.models.embedContent({\n            model: 'gemini-embedding-exp-03-07',\n            contents: 'What is the meaning of life?',\n        });\n    \n        console.log(response.embeddings);\n    }\n    \n    main();\n    \n\n### Go\n    \n    \n    ctx := context.Background()\n    \n    client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer client.Close()\n    \n    em := client.EmbeddingModel(\"gemini-embedding-exp-03-07\")\n    res, err := em.EmbedContent(ctx, genai.Text(\"What is the meaning of life?\"))\n    \n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(res.Embedding.Values)\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-exp-03-07:embedContent?key=$GEMINI_API_KEY\" \\\n    -H 'Content-Type: application/json' \\\n    -d '{\"model\": \"models/gemini-embedding-exp-03-07\",\n         \"content\": {\n         \"parts\":[{\n         \"text\": \"What is the meaning of life?\"}]}\n        }'\n    \n\nYou can also generate embeddings for multiple chunks at once by passing them in as a list of strings.\n\n## Task types\n\nWhen building Retrieval Augmented Generation (RAG) systems, a common design is to use text embeddings to perform a similarity search. In some cases this can lead to degraded quality, because questions and their answers are not semantically similar. For example, a question like \"Why is the sky blue?\" and its answer \"The scattering of sunlight causes the blue color,\" have distinctly different meanings as statements, which means that a RAG system won't automatically recognize their relation.\n\nTask types enable you to generate optimized embeddings for specific tasks, saving you time and cost and improving performance.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    result = client.models.embed_content(\n            model=\"gemini-embedding-exp-03-07\",\n            contents=\"What is the meaning of life?\",\n            config=types.EmbedContentConfig(task_type=\"SEMANTIC_SIMILARITY\")\n    )\n    print(result.embeddings)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    async function main() {\n    \n        const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n        const response = await ai.models.embedContent({\n            model: 'gemini-embedding-exp-03-07',\n            contents: 'What is the meaning of life?',\n            config: {\n                taskType: \"SEMANTIC_SIMILARITY\",\n            }\n        });\n    \n        console.log(response.embeddings);\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-exp-03-07:embedContent?key=$GEMINI_API_KEY\" \\\n    -H 'Content-Type: application/json' \\\n    -d '{\"model\": \"models/gemini-embedding-exp-03-07\",\n         \"content\": {\n         \"parts\":[{\n         \"text\": \"What is the meaning of life?\"}]},\n         \"taskType\": \"SEMANTIC_SIMILARITY\"\n        }'\n    \n\n### Supported task types\n\nTask type | Description  \n---|---  \n`SEMANTIC_SIMILARITY` | Used to generate embeddings that are optimized to assess text similarity.  \n`CLASSIFICATION` | Used to generate embeddings that are optimized to classify texts according to preset labels.  \n`CLUSTERING` | Used to generate embeddings that are optimized to cluster texts based on their similarities.  \n`RETRIEVAL_DOCUMENT`, `RETRIEVAL_QUERY`, `QUESTION_ANSWERING`, and `FACT_VERIFICATION` | Used to generate embeddings that are optimized for document search or information retrieval.  \n`CODE_RETRIEVAL_QUERY` | Used to retrieve a code block based on a natural language query, such as sort an array or reverse a linked list. Embeddings of the code blocks are computed using `RETRIEVAL_DOCUMENT`.  \n  \n## Use cases\n\nText embeddings are used in a variety of common AI use cases, such as:\n\n  * **Information retrieval:** You can use embeddings to retrieve semantically similar text given a piece of input text.\n\n[ Document search tutorialtask ](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/document_search.ipynb)\n\n  * **Clustering:** Comparing groups of embeddings can help identify hidden trends.\n\n[ Embedding clustering tutorialbubble_chart ](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/clustering_with_embeddings.ipynb)\n\n  * **Vector database:** As you take different embedding use cases to production, it is common to store embeddings in a vector database.\n\n[ Vector database tutorialbolt ](https://github.com/google-gemini/cookbook/blob/main/examples/chromadb/Vectordb_with_chroma.ipynb)\n\n  * **Classification:** You can train a model using embeddings to classify documents into categories.\n\n[ Classification tutorialtoken ](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/text_classifier_embeddings.ipynb)\n\n\n\n\n## Embedding models\n\nThe Gemini API offers three models that generate text embeddings:\n\n  * [gemini-embedding-exp-03-07](/gemini-api/docs/models#gemini-embedding)\n  * [text-embedding-004](/gemini-api/docs/models/gemini#text-embedding)\n  * [embedding-001](/gemini-api/docs/models/gemini#embedding)\n\n\n\n## What's next\n\nCheck out the [embeddings quickstart notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Embeddings.ipynb).\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/live",
    "title": "Live API  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Live API \n\n**Preview:** The Live API is in preview.\n\nThe Live API enables low-latency bidirectional voice and video interactions with Gemini. Using the Live API, you can provide end users with the experience of natural, human-like voice conversations, and with the ability to interrupt the model's responses using voice commands. The model can process text, audio, and video input, and it can provide text and audio output.\n\nYou can try the Live API in [Google AI Studio](https://aistudio.google.com/app/live).\n\nTo try a tutorial that lets you use your voice and camera to talk to Gemini through the Live API, see the [`Web Console Demo` project](https://github.com/google-gemini/live-api-web-console). \n\n## What's new\n\nThe Live API has new features and capabilities!\n\n**New capabilities:**\n\n  * Two new voices and 30 new languages, with configurable output language\n  * Configurable image resolutions 66/256 tokens\n  * Configurable turn coverage: Send all inputs all the time or only when the user is speaking\n  * Configure if input should interrupt the model or not\n  * Configurable Voice Activity Detection and new client events for end of turn signaling\n  * Token counts\n  * A client event for signaling end of stream\n  * Text streaming\n  * Configurable session resumption, with session data stored on the server for 24 hours\n  * Longer session support with a sliding context window\n\n\n\n**New client events:**\n\n  * End of audio stream / mic closed\n  * Activity start/end events for manually controlling turn transition\n\n\n\n**New server events:**\n\n  * Go away notification signaling a need to restart a session\n  * Generation complete\n\n\n\n## Use the Live API\n\nThis section describes how to use the Live API with one of our SDKs. For more information about the underlying WebSockets API, see the [WebSockets API reference](/api/live).\n\n### Send and receive text\n    \n    \n    import asyncio\n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    model = \"gemini-2.0-flash-live-001\"\n    \n    config = {\"response_modalities\": [\"TEXT\"]}\n    \n    async def main():\n        async with client.aio.live.connect(model=model, config=config) as session:\n            while True:\n                message = input(\"User> \")\n                if message.lower() == \"exit\":\n                    break\n                await session.send_client_content(\n                    turns={\"role\": \"user\", \"parts\": [{\"text\": message}]}, turn_complete=True\n                )\n    \n                async for response in session.receive():\n                    if response.text is not None:\n                        print(response.text, end=\"\")\n    \n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    \n\n### Receive audio\n\nThe following example shows how to receive audio data and write it to a `.wav` file.\n    \n    \n    import asyncio\n    import wave\n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\", http_options={'api_version': 'v1alpha'})\n    model = \"gemini-2.0-flash-live-001\"\n    \n    config = {\"response_modalities\": [\"AUDIO\"]}\n    \n    async def main():\n        async with client.aio.live.connect(model=model, config=config) as session:\n            wf = wave.open(\"audio.wav\", \"wb\")\n            wf.setnchannels(1)\n            wf.setsampwidth(2)\n            wf.setframerate(24000)\n    \n            message = \"Hello? Gemini are you there?\"\n            await session.send_client_content(\n                turns={\"role\": \"user\", \"parts\": [{\"text\": message}]}, turn_complete=True\n            )\n    \n            async for idx,response in async_enumerate(session.receive()):\n                if response.data is not None:\n                    wf.writeframes(response.data)\n    \n                # Un-comment this code to print audio data info\n                # if response.server_content.model_turn is not None:\n                #      print(response.server_content.model_turn.parts[0].inline_data.mime_type)\n    \n            wf.close()\n    \n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    \n\n#### Audio formats\n\nThe Live API supports the following audio formats:\n\n  * Input audio format: Raw 16 bit PCM audio at 16kHz little-endian\n  * Output audio format: Raw 16 bit PCM audio at 24kHz little-endian\n\n\n\n### Stream audio and video\n\nTo see an example of how to use the Live API in a streaming audio and video format, run the \"Live API - Quickstart\" file in the cookbooks repository: \n\n[View on GitHub](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py)\n\n### System instructions\n\nSystem instructions let you steer the behavior of a model based on your specific needs and use cases. System instructions can be set in the setup configuration and will remain in effect for the entire session.\n    \n    \n    from google.genai import types\n    \n    config = {\n        \"system_instruction\": types.Content(\n            parts=[\n                types.Part(\n                    text=\"You are a helpful assistant and answer in a friendly tone.\"\n                )\n            ]\n        ),\n        \"response_modalities\": [\"TEXT\"],\n    }\n    \n\n### Incremental content updates\n\nUse incremental updates to send text input, establish session context, or restore session context. For short contexts you can send turn-by-turn interactions to represent the exact sequence of events:\n\n### Python\n    \n    \n    turns = [\n        {\"role\": \"user\", \"parts\": [{\"text\": \"What is the capital of France?\"}]},\n        {\"role\": \"model\", \"parts\": [{\"text\": \"Paris\"}]},\n    ]\n    \n    await session.send_client_content(turns=turns, turn_complete=False)\n    \n    turns = [{\"role\": \"user\", \"parts\": [{\"text\": \"What is the capital of Germany?\"}]}]\n    \n    await session.send_client_content(turns=turns, turn_complete=True)\n    \n\n### JSON\n    \n    \n    {\n      \"clientContent\": {\n        \"turns\": [\n          {\n            \"parts\":[\n              {\n                \"text\": \"\"\n              }\n            ],\n            \"role\":\"user\"\n          },\n          {\n            \"parts\":[\n              {\n                \"text\": \"\"\n              }\n            ],\n            \"role\":\"model\"\n          }\n        ],\n        \"turnComplete\": true\n      }\n    }\n    \n\nFor longer contexts it's recommended to provide a single message summary to free up the context window for subsequent interactions.\n\n### Change voices\n\nThe Live API supports the following voices: Puck, Charon, Kore, Fenrir, Aoede, Leda, Orus, and Zephyr.\n\nTo specify a voice, set the voice name within the `speechConfig` object as part of the session configuration:\n\n### Python\n    \n    \n    from google.genai import types\n    \n    config = types.LiveConnectConfig(\n        response_modalities=[\"AUDIO\"],\n        speech_config=types.SpeechConfig(\n            voice_config=types.VoiceConfig(\n                prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name=\"Kore\")\n            )\n        )\n    )\n    \n\n### JSON\n    \n    \n    {\n      \"voiceConfig\": {\n        \"prebuiltVoiceConfig\": {\n          \"voiceName\": \"Kore\"\n        }\n      }\n    }\n    \n\n### Use function calling\n\nYou can define tools with the Live API. See the [Function calling tutorial](https://ai.google.dev/gemini-api/docs/function-calling) to learn more about function calling.\n\nTools must be defined as part of the session configuration:\n    \n    \n    config = types.LiveConnectConfig(\n        response_modalities=[\"TEXT\"],\n        tools=[set_light_values]\n    )\n    \n    async with client.aio.live.connect(model=model, config=config) as session:\n        await session.send_client_content(\n            turns={\n                \"role\": \"user\",\n                \"parts\": [{\"text\": \"Turn the lights down to a romantic level\"}],\n            },\n            turn_complete=True,\n        )\n    \n        async for response in session.receive():\n            print(response.tool_call)\n    \n\nFrom a single prompt, the model can generate multiple function calls and the code necessary to chain their outputs. This code executes in a sandbox environment, generating subsequent [BidiGenerateContentToolCall](/api/live#bidigeneratecontenttoolcall) messages. The execution pauses until the results of each function call are available, which ensures sequential processing.\n\nThe client should respond with [BidiGenerateContentToolResponse](/api/live#bidigeneratecontenttoolresponse).\n\nAudio inputs and audio outputs negatively impact the model's ability to use function calling.\n\n### Handle interruptions\n\nUsers can interrupt the model's output at any time. When [Voice activity detection](/api/live#voice-activity-detection) (VAD) detects an interruption, the ongoing generation is canceled and discarded. Only the information already sent to the client is retained in the session history. The server then sends a [BidiGenerateContentServerContent](/api/live#bidigeneratecontentservercontent) message to report the interruption.\n\nIn addition, the Gemini server discards any pending function calls and sends a `BidiGenerateContentServerContent` message with the IDs of the canceled calls.\n    \n    \n    async for response in session.receive():\n        if response.server_content.interrupted is not None:\n            # The generation was interrupted\n    \n\n### Configure voice activity detection (VAD)\n\nBy default, the model automatically performs voice activity detection (VAD) on a continuous audio input stream. VAD can be configured with the [`realtimeInputConfig.automaticActivityDetection`](/api/live#RealtimeInputConfig.AutomaticActivityDetection) field of the [setup configuration](/api/live#BidiGenerateContentSetup).\n\nWhen the audio stream is paused for more than a second (for example, because the user switched off the microphone), an [`audioStreamEnd`](/api/live#BidiGenerateContentRealtimeInput.FIELDS.bool.BidiGenerateContentRealtimeInput.audio_stream_end) event should be sent to flush any cached audio. The client can resume sending audio data at any time.\n\nAlternatively, the automatic VAD can be disabled by setting `realtimeInputConfig.automaticActivityDetection.disabled` to `true` in the setup message. In this configuration the client is responsible for detecting user speech and sending [`activityStart`](/api/live#BidiGenerateContentRealtimeInput.FIELDS.BidiGenerateContentRealtimeInput.ActivityStart.BidiGenerateContentRealtimeInput.activity_start) and [`activityEnd`](/api/live#BidiGenerateContentRealtimeInput.FIELDS.BidiGenerateContentRealtimeInput.ActivityEnd.BidiGenerateContentRealtimeInput.activity_end) messages at the appropriate times. An `audioStreamEnd` isn't sent in this configuration. Instead, any interruption of the stream is marked by an `activityEnd` message.\n\n_SDK support for this feature will be available in the coming weeks._\n\n### Get the token count\n\nYou can find the total number of consumed tokens in the [usageMetadata](/api/live#usagemetadata) field of the returned server message.\n    \n    \n    from google.genai import types\n    \n    async with client.aio.live.connect(\n        model='gemini-2.0-flash-live-001',\n        config=types.LiveConnectConfig(\n            response_modalities=['AUDIO'],\n        ),\n    ) as session:\n        # Session connected\n        while True:\n            await session.send_client_content(\n                turns=types.Content(role='user', parts=[types.Part(text='Hello world!')])\n            )\n            async for message in session.receive():\n                # The server will periodically send messages that include\n                # UsageMetadata.\n                if message.usage_metadata:\n                    usage = message.usage_metadata\n                    print(\n                        f'Used {usage.total_token_count} tokens in total. Response token'\n                        ' breakdown:'\n                    )\n                for detail in usage.response_tokens_details:\n                    match detail:\n                      case types.ModalityTokenCount(modality=modality, token_count=count):\n                          print(f'{modality}: {count}')\n    \n                # For the purposes of this example, placeholder input is continually fed\n                # to the model. In non-sample code, the model inputs would come from\n                # the user.\n                if message.server_content and message.server_content.turn_complete:\n                    break\n    \n\n### Configure session resumption\n\nTo prevent session termination when the server periodically resets the WebSocket connection, configure the [sessionResumption](/api/live#BidiGenerateContentSetup.FIELDS.SessionResumptionConfig.BidiGenerateContentSetup.session_resumption) field within the [setup configuration](/api/live#BidiGenerateContentSetup).\n\nPassing this configuration causes the server to send [SessionResumptionUpdate](/api/live#SessionResumptionUpdate) messages, which can be used to resume the session by passing the last resumption token as the [`SessionResumptionConfig.handle`](/api/liveSessionResumptionConfig.FIELDS.string.SessionResumptionConfig.handle) of the subsequent connection.\n    \n    \n    from google.genai import types\n    \n    print(f\"Connecting to the service with handle {previous_session_handle}...\")\n    async with client.aio.live.connect(\n        model=\"gemini-2.0-flash-live-001\",\n        config=types.LiveConnectConfig(\n            response_modalities=[\"AUDIO\"],\n            session_resumption=types.SessionResumptionConfig(\n                # The handle of the session to resume is passed here,\n                # or else None to start a new session.\n                handle=previous_session_handle\n            ),\n        ),\n    ) as session:\n        # Session connected\n        while True:\n            await session.send_client_content(\n                turns=types.Content(\n                    role=\"user\", parts=[types.Part(text=\"Hello world!\")]\n                )\n            )\n            async for message in session.receive():\n                # Periodically, the server will send update messages that may\n                # contain a handle for the current state of the session.\n                if message.session_resumption_update:\n                    update = message.session_resumption_update\n                    if update.resumable and update.new_handle:\n                        # The handle should be retained and linked to the session.\n                        return update.new_handle\n    \n                # For the purposes of this example, placeholder input is continually fed\n                # to the model. In non-sample code, the model inputs would come from\n                # the user.\n                if message.server_content and message.server_content.turn_complete:\n                    break\n    \n\n### Receive a message before the session disconnects\n\nThe server sends a [GoAway](/api/live#GoAway) message that signals that the current connection will soon be terminated. This message includes the [timeLeft](/api/live#GoAway.FIELDS.google.protobuf.Duration.GoAway.time_left), indicating the remaining time and lets you take further action before the connection will be terminated as ABORTED.\n\n### Receive a message when the generation is complete\n\nThe server sends a [generationComplete](/api/live#BidiGenerateContentServerContent.FIELDS.bool.BidiGenerateContentServerContent.generation_complete) message that signals that the model finished generating the response.\n\n### Enable context window compression\n\nTo enable longer sessions, and avoid abrupt connection termination, you can enable context window compression by setting the [contextWindowCompression](/api/live#BidiGenerateContentSetup.FIELDS.ContextWindowCompressionConfig.BidiGenerateContentSetup.context_window_compression) field as part of the session configuration.\n\nIn the [ContextWindowCompressionConfig](/api/live#contextwindowcompressionconfig), you can configure a [sliding-window mechanism](/api/live#ContextWindowCompressionConfig.FIELDS.ContextWindowCompressionConfig.SlidingWindow.ContextWindowCompressionConfig.sliding_window) and the [number of tokens](/api/live#ContextWindowCompressionConfig.FIELDS.int64.ContextWindowCompressionConfig.trigger_tokens) that triggers compression.\n    \n    \n    from google.genai import types\n    \n    config = types.LiveConnectConfig(\n        response_modalities=[\"AUDIO\"],\n        context_window_compression=(\n            # Configures compression with default parameters.\n            types.ContextWindowCompressionConfig(\n                sliding_window=types.SlidingWindow(),\n            )\n        ),\n    )\n    \n\n### Change the media resolution\n\nYou can specify the media resolution for the input media by setting the `mediaResolution` field as part of the session configuration:\n    \n    \n    from google.genai import types\n    \n    config = types.LiveConnectConfig(\n        response_modalities=[\"AUDIO\"],\n        media_resolution=types.MediaResolution.MEDIA_RESOLUTION_LOW,\n    )\n    \n\n## Limitations\n\nConsider the following limitations of the Live API and Gemini 2.0 when you plan your project.\n\n### Client authentication\n\nThe Live API only provides server to server authentication and isn't recommended for direct client use. Client input should be routed through an intermediate application server for secure authentication with the Live API.\n\n### Session duration\n\nSession duration can be extended to unlimited by enabling session [compression](/gemini-api/docs/live#context-window-compression). Without compression, audio-only sessions are limited to 15 minutes, and audio plus video sessions are limited to 2 minutes. Exceeding these limits without compression will terminate the connection.\n\n### Context window\n\nA session has a context window limit of 32k tokens.\n\n## Third-party integrations\n\nFor web and mobile app deployments, you can explore options from:\n\n  * [Daily](https://www.daily.co/products/gemini/multimodal-live-api/)\n  * [Livekit](https://docs.livekit.io/agents/integrations/google/#multimodal-live-api)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-11 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-11 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/caching",
    "title": "Context caching  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Context caching \n\nPython JavaScript Go REST\n\nIn a typical AI workflow, you might pass the same input tokens over and over to a model. Using the Gemini API context caching feature, you can pass some content to the model once, cache the input tokens, and then refer to the cached tokens for subsequent requests. At certain volumes, using cached tokens is lower cost than passing in the same corpus of tokens repeatedly.\n\nWhen you cache a set of tokens, you can choose how long you want the cache to exist before the tokens are automatically deleted. This caching duration is called the _time to live_ (TTL). If not set, the TTL defaults to 1 hour. The cost for caching depends on the input token size and how long you want the tokens to persist.\n\nContext caching supports both Gemini 1.5 Pro and Gemini 1.5 Flash.\n\n**Note:** Context caching is only available for stable models with fixed versions (for example, `gemini-1.5-pro-001`). You must include the version postfix (for example, the `-001` in `gemini-1.5-pro-001`).\n\n## When to use context caching\n\nContext caching is particularly well suited to scenarios where a substantial initial context is referenced repeatedly by shorter requests. Consider using context caching for use cases such as:\n\n  * Chatbots with extensive [system instructions](/gemini-api/docs/system-instructions)\n  * Repetitive analysis of lengthy video files\n  * Recurring queries against large document sets\n  * Frequent code repository analysis or bug fixing\n\n\n\n## How caching reduces costs\n\nContext caching is a paid feature designed to reduce overall operational costs. Billing is based on the following factors:\n\n  1. **Cache token count:** The number of input tokens cached, billed at a reduced rate when included in subsequent prompts.\n  2. **Storage duration:** The amount of time cached tokens are stored (TTL), billed based on the TTL duration of cached token count. There are no minimum or maximum bounds on the TTL.\n  3. **Other factors:** Other charges apply, such as for non-cached input tokens and output tokens.\n\n\n\nFor up-to-date pricing details, refer to the Gemini API [pricing page](/pricing). To learn how to count tokens, see the [Token guide](/gemini-api/docs/tokens).\n\n## How to use context caching\n\nThis section assumes that you've installed a Gemini SDK (or have curl installed) and that you've configured an API key, as shown in the [quickstart](/gemini-api/docs/quickstart).\n\n## Additional considerations\n\nKeep the following considerations in mind when using context caching:\n\n  * The _minimum_ input token count for context caching is 32,768, and the _maximum_ is the same as the maximum for the given model. (For more on counting tokens, see the [Token guide](/gemini-api/docs/tokens)).\n  * The model doesn't make any distinction between cached tokens and regular input tokens. Cached content is simply a prefix to the prompt.\n  * There are no special rate or usage limits on context caching; the standard rate limits for `GenerateContent` apply, and token limits include cached tokens.\n  * The number of cached tokens is returned in the `usage_metadata` from the create, get, and list operations of the cache service, and also in `GenerateContent` when using the cache.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-01 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-01 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/files",
    "title": "Files API  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Files API \n\nYou can use the Files API to upload and interact with media files. The Files API lets you store up to 20 GB of files per project, with a per-file maximum size of 2 GB. Files are stored for 48 hours. During that time, you can use the API to get metadata about the files, but you can't download the files. The Files API is available at no cost in all regions where the Gemini API is available.\n\nThis guide shows you how to work with media files using the Files API. The basic operations are the same for audio files, images, videos, documents, and other supported file types.\n\n### Upload a file\n\nYou can use the Files API to upload a media file. Always use the Files API when the total request size (including the files, text prompt, system instructions, etc.) is larger than 20 MB.\n\nThe following code uploads a file and then uses the file in a call to `generateContent`.\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GOOGLE_API_KEY\")\n    \n    myfile = client.files.upload(file=\"path/to/sample.mp3\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\", contents=[\"Describe this audio clip\", myfile]\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import {\n      GoogleGenAI,\n      createUserContent,\n      createPartFromUri,\n    } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GOOGLE_API_KEY\" });\n    \n    async function main() {\n      const myfile = await ai.files.upload({\n        file: \"path/to/sample.mp3\",\n        config: { mimeType: \"audio/mpeg\" },\n      });\n    \n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: createUserContent([\n          createPartFromUri(myfile.uri, myfile.mimeType),\n          \"Describe this audio clip\",\n        ]),\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    file, err := client.UploadFileFromPath(ctx, \"path/to/sample.mp3\", nil)\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer client.DeleteFile(ctx, file.Name)\n    \n    model := client.GenerativeModel(\"gemini-2.0-flash\")\n    resp, err := model.GenerateContent(ctx,\n        genai.FileData{URI: file.URI},\n        genai.Text(\"Describe this audio clip\"))\n    if err != nil {\n        log.Fatal(err)\n    }\n    \n    printResponse(resp)\n    \n\n### REST\n    \n    \n    AUDIO_PATH=\"path/to/sample.mp3\"\n    MIME_TYPE=$(file -b --mime-type \"${AUDIO_PATH}\")\n    NUM_BYTES=$(wc -c < \"${AUDIO_PATH}\")\n    DISPLAY_NAME=AUDIO\n    \n    tmp_header_file=upload-header.tmp\n    \n    # Initial resumable request defining metadata.\n    # The upload url is in the response headers dump them to a file.\n    curl \"${BASE_URL}/upload/v1beta/files?key=${GOOGLE_API_KEY}\" \\\n      -D upload-header.tmp \\\n      -H \"X-Goog-Upload-Protocol: resumable\" \\\n      -H \"X-Goog-Upload-Command: start\" \\\n      -H \"X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}\" \\\n      -H \"X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}\" \\\n      -H \"Content-Type: application/json\" \\\n      -d \"{'file': {'display_name': '${DISPLAY_NAME}'}}\" 2> /dev/null\n    \n    upload_url=$(grep -i \"x-goog-upload-url: \" \"${tmp_header_file}\" | cut -d\" \" -f2 | tr -d \"\\r\")\n    rm \"${tmp_header_file}\"\n    \n    # Upload the actual bytes.\n    curl \"${upload_url}\" \\\n      -H \"Content-Length: ${NUM_BYTES}\" \\\n      -H \"X-Goog-Upload-Offset: 0\" \\\n      -H \"X-Goog-Upload-Command: upload, finalize\" \\\n      --data-binary \"@${AUDIO_PATH}\" 2> /dev/null > file_info.json\n    \n    file_uri=$(jq \".file.uri\" file_info.json)\n    echo file_uri=$file_uri\n    \n    # Now generate content using that file\n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GOOGLE_API_KEY\" \\\n        -H 'Content-Type: application/json' \\\n        -X POST \\\n        -d '{\n          \"contents\": [{\n            \"parts\":[\n              {\"text\": \"Describe this audio clip\"},\n              {\"file_data\":{\"mime_type\": \"${MIME_TYPE}\", \"file_uri\": '$file_uri'}}]\n            }]\n          }' 2> /dev/null > response.json\n    \n    cat response.json\n    echo\n    \n    jq \".candidates[].content.parts[].text\" response.json\n    \n\n## Get metadata for a file\n\nYou can verify that the API successfully stored the uploaded file and get its metadata by calling `files.get`.\n\n### Python\n    \n    \n    myfile = client.files.upload(file='path/to/sample.mp3')\n    file_name = myfile.name\n    myfile = client.files.get(name=file_name)\n    print(myfile)\n    \n\n### JavaScript\n    \n    \n    const myfile = await ai.files.upload({\n      file: \"path/to/sample.mp3\",\n      config: { mimeType: \"audio/mpeg\" },\n    });\n    \n    const fileName = myfile.name;\n    const fetchedFile = await ai.files.get({ name: fileName });\n    console.log(fetchedFile);\n    \n\n### Go\n    \n    \n    file, err := client.UploadFileFromPath(ctx, \"path/to/sample.mp3\", nil)\n    if err != nil {\n        log.Fatal(err)\n    }\n    \n    gotFile, err := client.GetFile(ctx, file.Name)\n    if err != nil {\n        log.Fatal(err)\n    }\n    fmt.Println(\"Got file:\", gotFile.Name)\n    \n\n### REST\n    \n    \n    # file_info.json was created in the upload example\n    name=$(jq \".file.name\" file_info.json)\n    # Get the file of interest to check state\n    curl https://generativelanguage.googleapis.com/v1beta/files/$name > file_info.json\n    # Print some information about the file you got\n    name=$(jq \".file.name\" file_info.json)\n    echo name=$name\n    file_uri=$(jq \".file.uri\" file_info.json)\n    echo file_uri=$file_uri\n    \n\n## List uploaded files\n\nYou can upload multiple files using the Files API. The following code gets a list of all the files uploaded:\n\n### Python\n    \n    \n    print('My files:')\n    for f in client.files.list():\n        print(' ', f.name)\n    \n\n### JavaScript\n    \n    \n    const listResponse = await ai.files.list({ config: { pageSize: 10 } });\n    for await (const file of listResponse) {\n      console.log(file.name);\n    }\n    \n\n### Go\n    \n    \n    iter := client.ListFiles(ctx)\n    for {\n        ifile, err := iter.Next()\n        if err == iterator.Done {\n            break\n        }\n        if err != nil {\n            log.Fatal(err)\n        }\n        fmt.Println(ifile.Name)\n    }\n    \n\n### REST\n    \n    \n    echo \"My files: \"\n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/files?key=$GOOGLE_API_KEY\"\n    \n\n## Delete uploaded files\n\nFiles are automatically deleted after 48 hours. You can also manually delete an uploaded file:\n\n### Python\n    \n    \n    myfile = client.files.upload(file='path/to/sample.mp3')\n    client.files.delete(name=myfile.name)\n    \n\n### JavaScript\n    \n    \n    const myfile = await ai.files.upload({\n      file: \"path/to/sample.mp3\",\n      config: { mimeType: \"audio/mpeg\" },\n    });\n    \n    const fileName = myfile.name;\n    await ai.files.delete({ name: fileName });\n    \n\n### Go\n    \n    \n    file, err := client.UploadFileFromPath(ctx, \"path/to/sample.mp3\", nil)\n    if err != nil {\n        log.Fatal(err)\n    }\n    client.DeleteFile(ctx, file.Name)\n    \n\n### REST\n    \n    \n    curl --request \"DELETE\" https://generativelanguage.googleapis.com/v1beta/files/$name?key=$GOOGLE_API_KEY\n    \n\n## What's next\n\nThis guide shows how to work with files using the Files API. To learn more about prompting with files, see [File prompting strategies](/gemini-api/docs/file-prompting-strategies).\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-09 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-09 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/prompting-intro",
    "title": "Introduction to prompt design  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Introduction to prompt design \n\n_Prompt design_ is the process of creating prompts that elicit the desired response from language models. Writing well structured prompts is an essential part of ensuring accurate, high quality responses from a language model. This page introduces some basic concepts, strategies, and best practices to get you started in designing prompts.\n\n## What is a prompt\n\nA prompt is a natural language request submitted to a language model to receive a response back. Prompts can contain questions, instructions, contextual information, examples, and partial input for the model to complete or continue. After the model receives a prompt, depending on the type of model being used, it can generate text, embeddings, code, images, videos, music, and more.\n\n### Prompt content types\n\nPrompts can include one or more of the following types of content:\n\n  * Input (required)\n  * Context (optional)\n  * Examples (optional)\n\n\n\n#### Input\n\nAn input is the text in the prompt that you want the model to provide a response for, and it's a required content type. Inputs can be a question that the model answers (question input), a task the model performs (task input), an entity the model operates on (entity input), or partial input that the model completes or continues (completion input).\n\n##### Question input\n\nA question input is a question that you ask the model that the model provides an answer to.\n\n**Prompt:**\n    \n    \n    What's a good name for a flower shop that specializes in selling bouquets of\n    dried flowers? Create a list of 10 options with just the names.\n      \n\n**Response:**\n    \n    \n        Here are 10 names for a flower shop specializing in dried flowers:\n    \n        1.  Everlasting Blooms\n        2.  Dried & Delightful\n        3.  The Petal Preserve\n        4.  Whispers of Wildflowers\n        5.  Forever Blooms\n        6.  The Botanical Muse\n        7.  Timeless Blooms\n        8.  Dried Flower Co.\n        9.  Bloom & Co.\n        10.  The Sun-Kissed Petals    \n      \n\n(gemini-1.5-flash)  \n---  \n  \n##### Task input\n\nA task input is a task that you want the model to perform. For example, you can tell the model to give you ideas or suggestions for something.\n\n**Prompt:**\n    \n    \n    Give me a simple list of just the things that I must bring on a camping trip. The list should have 10 items.\n      \n\n**Response:**\n    \n    \n        Here's a list of 10 essential items for a camping trip:\n    \n        1. **Tent:** Your shelter from the elements.\n        2. **Sleeping Bag:** Stay warm and comfortable.\n        3. **Sleeping Pad:** Insulation and cushioning for a good night's sleep.\n        4. **Headlamp or Flashlight:** Light for nighttime activities.\n        5. **First Aid Kit:** Be prepared for minor injuries.\n        6. **Water Bottle/Hydration Pack:** Stay hydrated.\n        7. **Food and Snacks:** Fuel for your adventures.\n        8. **Cooking Gear:** Camp stove, fuel, utensils, etc.\n        9. **Knife and Multitool:** Essential for various tasks.\n        10. **Camp Chair:** Enjoy your campfire and meals comfortably.     \n      \n\n(gemini-1.5-flash)  \n---  \n  \n##### Entity input\n\nAn entity input is what the model performs an action on, such as classify or summarize. This type of input can benefit from the inclusion of instructions.\n\n**Prompt:**\n    \n    \n    Classify the following items as [large, small].\n    Elephant\n    Mouse\n    Snail\n      \n\n**Response:**\n    \n    \n        Here's the classification:\n    \n        * **Elephant:** Large\n        * **Mouse:** Small\n        * **Snail:** Small     \n      \n\n(gemini-1.5-flash)  \n---  \n  \n##### Completion input\n\nA completion input is text that the model is expected to complete or continue.\n\n**Prompt:**\n    \n    \n        Complete the following prompt: Some simple strategies for overcoming writer's block include...\n      \n\n**Response:**\n    \n    \n        Some simple strategies for overcoming writer's block include:\n    \n        * **Freewriting:** Write non-stop for a set amount of time (5-10 minutes) without worrying about grammar, spelling, or even making sense. Just keep your pen moving and let your thoughts flow freely. This can help you loosen up and access ideas you might not have considered otherwise.\n        * **Brainstorming:**  Make a list of everything that comes to mind related to your topic, no matter how silly or unrelated it seems. You can also use mind mapping or other visual brainstorming techniques.\n        * **Changing your environment:**  Sometimes a change of scenery can be all you need to get the creative juices flowing. Go for a walk, work in a coffee shop, or even just move to a different room in your house. \n        * **Reading:**  Immerse yourself in the work of other writers, especially those who inspire you. Pay attention to their style, their structure, and their use of language. \n        * **Taking a break:** Sometimes the best way to overcome writer's block is to simply step away from the project for a while. Go for a walk, watch a movie, listen to music, or do something else that you enjoy. You might be surprised at how much clearer your head feels when you return to your writing. \n        * **Focusing on a single element:**  Instead of tackling the entire project, focus on a single sentence, paragraph, or scene. Once you get something down on paper, it can be easier to build from there.\n        * **Starting with the ending:**  If you're stuck on the beginning, try writing the ending first. This can help you to clarify your overall vision for the project and provide a framework for your ideas.\n        * **Talking it out:** Share your ideas and challenges with a friend, a writing group, or a writing coach. Discussing your project with someone else can help you to see it from a new perspective and find solutions to your writing problems.\n        \n        Remember, writer's block is a common experience, and it's not a sign that you're a bad writer. Just be patient with yourself, try some of these strategies, and keep writing!     \n      \n\n(gemini-1.5-flash)  \n---  \n  \n#### Context\n\nContext can be one of the following:\n\n  * Instructions that specify how the model should behave.\n  * Information that the model uses or references to generate a response.\n\n\n\nAdd contextual information in your prompt when you need to give information to the model, or restrict the boundaries of the responses to only what's within the prompt.\n\n**Prompt:**\n    \n    \n    Marbles:\n    Color: red\n    Number: 12\n    Color: blue\n    Number: 28\n    Color: yellow\n    Number: 15\n    Color: green\n    Number: 17\n    \n    How many green marbles are there?\n      \n\n**Response:**\n    \n    \n    There are 17 green marbles.\n      \n\n(gemini-1.5-flash)  \n---  \n  \n#### Examples\n\nExamples are input-output pairs that you include in the prompt to give the model an example of an ideal response. Including examples in the prompt is an effective strategy for customizing the response format.\n\n**Prompt:**\n    \n    \n    Classify the following.\n    Options:\n    - red wine\n    - white wine\n    \n    Text: Chardonnay\n    The answer is: white wine\n    Text: Cabernet\n    The answer is: red wine\n    Text: Moscato\n    The answer is: white wine\n    \n    Text: Riesling\n    The answer is:\n      \n\n**Response:**\n    \n    \n    The answer is: white wine\n      \n\n(gemini-1.5-flash)  \n---  \n  \n## Next steps\n\n  * Now that you have an understanding of prompt design, try writing your own prompts using [Google AI Studio](http://aistudio.google.com).\n  * For a deeper understanding of prompt design, see the [prompt strategies](/gemini-api/docs/prompting-strategies) topic.\n  * To learn about multimodal prompting, see [Prompting with media files](/gemini-api/docs/prompting_with_media).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/prompting-strategies",
    "title": "Prompt design strategies  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Prompt design strategies \n\nThis page introduces you to some general prompt design strategies that you can employ when designing prompts.\n\nLarge language models (LLM) are trained on vast amounts of text data to learn the patterns and relationships between language. When given some text (the prompt), language models can predict what is likely to come next, like a sophisticated autocompletion tool.\n\nGoogle AI Studio hosts a simple [prompt gallery](https://aistudio.google.com/app/gallery) meant to interactively showcase many of the concepts shared in this guide. In the rest of this guide, you'll learn about the following prompt design strategies: \n\n  * Give clear and specific instructions\n  * Include few-shot examples\n  * Add contextual information\n  * Add prefixes\n  * Let the model complete partial input\n\n\n\n## Give clear and specific instructions\n\nGiving the model instructions on what to do is an effective and efficient way to customize model behavior. Ensure that the instructions you give are clear and specific. Instructions can be as simple as a list of step-by-step instructions or as complex as mapping out a user's experience and mindset.\n\n### Define the task to perform\n\nDescribe in detail the task that you want the model to perform. The task can be as simple as a step-by-step list of instructions or as complex as mapping out a user's experience and mindset. The following prompt tells the model to summarize a block of text:\n\n**Prompt:**\n    \n    \n    **Summarize this text:**\n    Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially\n    faster than any modern traditional computer. At very tiny scales, physical matter acts as both\n    particles and as waves, and quantum computing uses specialized hardware to leverage this behavior.\n    The operating principles of quantum devices is beyond the scope of classical physics. When deployed\n    at scale, quantum computers could be used in a wide variety of applications such as: in\n    cybersecurity to break existing encryption methods while helping researchers create new ones, in\n    meteorology to develop better weather forecasting etc. However, the current state of the art quantum\n    computers are still largely experimental and impractical.\n      \n\n**Response:**\n    \n    \n        Quantum computers leverage the wave-particle duality of matter at tiny scales to perform\n        calculations exponentially faster than traditional computers. They harness quantum phenomena,\n        which are beyond the scope of classical physics, to achieve this speed advantage. While\n        promising for applications like cybersecurity, weather forecasting, and beyond, current\n        quantum computers are still experimental and not yet practical for widespread use.\n      \n\n(gemini-1.5-flash)  \n---  \n  \n### Specify any constraints\n\nSpecify any constraints on reading the prompt or generating a response. You can tell the model what to do and not to do. From the previous example, the summarization is shorter than the original text, but maybe you want the summary to be shorter. In this case, you can specify a constraint in the prompt on how long you want the summary to be:\n\n**Prompt:**\n    \n    \n    **Summarize this text in two sentences:**\n    Text: A quantum computer exploits quantum mechanical phenomena to perform calculations exponentially\n    faster than any modern traditional computer. At very tiny scales, physical matter acts as both\n    particles and as waves, and quantum computing uses specialized hardware to leverage this behavior.\n    The operating principles of quantum devices is beyond the scope of classical physics. When deployed\n    at scale, quantum computers could be used in a wide variety of applications such as: in\n    cybersecurity to break existing encryption methods while helping researchers create new ones, in\n    meteorology to develop better weather forecasting etc. However, the current state of the art quantum\n    computers are still largely experimental and impractical.\n        \n\n**Response:**\n    \n    \n          Quantum computers harness the wave-particle duality of matter at the quantum level to perform\n          computations exponentially faster than traditional computers. While these machines hold\n          immense potential for fields like cybersecurity and weather forecasting, they are currently\n          in their early stages of development and not yet practical for widespread use.\n        \n\n(gemini-1.5-flash)  \n---  \n  \n### Define the format of the response\n\nYou can also give instructions that specify the format of the response. For example, you can ask for the response to be formatted as a table, bulleted list, elevator pitch, keywords, sentence, or paragraph. The following system message instructs the model to be more conversational in its response:\n\n**System message**\n    \n    \n        All questions should be answered comprehensively with details, unless the user requests a concise response specifically.\n      \n\n**Prompt**\n    \n    \n        What is a smart way to make a business that sells DVD's in 2025?\n      \n\n**Response:**\n    \n    \n        Okay, you're looking at a real uphill battle here. The DVD market is significantly diminished due to streaming services, digital downloads, and Blu-ray. However, that doesn't necessarily mean it's impossible to build a DVD-based business in 2025. It just requires a hyper-focused strategy and a deep understanding of the remaining market. Here's a breakdown of smart approaches:\n    \n        I. Understanding the Challenges and Opportunities\n        Challenges:\n        Dominance of Streaming: Netflix, Amazon Prime Video, Disney+, and countless other services offer vast libraries on-demand.\n        Digital Downloads/Rentals: Platforms like iTunes, Google Play, and Vudu make purchasing or renting digital versions easy.\n        Blu-ray & 4K Ultra HD: Offer superior picture and sound quality, often preferred by home theater enthusiasts.\n        ....\n    .....\n      \n\n(gemini-2.0-flash)  \n---  \nAnother example which we used in the past is:  **System message**\n    \n    \n          All questions should be answered comprehensively with details, unless the user requests a\n          concise response specifically. Respond in the same language as the query.\n        \n\n**Prompt**\n    \n    \n          What is a smart way to create a lemonade business?\n        \n\n**Response:**\n    \n    \n      Launching a Successful Lemonade Stand: A Smart Approach\n      Creating a lemonade business, whether a simple stand or a larger operation, requires strategic thinking. Here's a breakdown of how to approach it smartly:\n      \n      1. Define Your Business:\n      Scope: Are you aiming for a small, seasonal lemonade stand, a food truck, a delivery service, or something else entirely?\n      Target Audience: Who are you selling to? Families, office workers, tourists?\n      Unique Selling Proposition: What sets your lemonade apart? Special flavors, organic ingredients, a unique recipe?\n      \n      Pricing: Research competitor prices and determine a profitable pricing structure.\n      Location: Consider foot traffic, accessibility, and regulations for your chosen location.\n      .....\n        \n\n(gemini-1.5-flash)  \n---  \n  \n### Summary\n\n  * Give the models instructions on what to do.\n  * Make the instructions clear and specific.\n  * Specify any constraints or formatting requirements for the output.\n\n\n\n## Include few-shot examples\n\nYou can include examples in the prompt that show the model what getting it right looks like. The model attempts to identify patterns and relationships from the examples and applies them when generating a response. Prompts that contain a few examples are called _few-shot_ prompts, while prompts that provide no examples are called _zero-shot prompts_. Few-shot prompts are often used to regulate the formatting, phrasing, scoping, or general patterning of model responses. Use specific and varied examples to help the model narrow its focus and generate more accurate results.\n\nWe recommend to always include few-shot examples in your prompts. Prompts without few-shot examples are likely to be less effective. In fact, you can remove instructions from your prompt if your examples are clear enough in showing the task at hand.\n\n### Zero-shot vs few-shot prompts\n\nThe following zero-shot prompt asks the model to choose the best explanation.\n\n**Prompt:**\n    \n    \n    Please choose the best explanation to the question:\n    \n    Question: How is snow formed?\n    Explanation1: Snow is formed when water vapor in the air freezes into ice crystals in the\n    atmosphere, which can combine and grow into snowflakes as they fall through the atmosphere and\n    accumulate on the ground.\n    Explanation2: Water vapor freezes into ice crystals forming snow.\n    Answer:\n      \n\n**Response:**\n    \n    \n    Explanation 1 is the best explanation.\n      \n\n(gemini-1.5-flash)  \n---  \n  \nIf your use case requires the model to produce concise responses, you can include examples in the prompt that give preference to concise responses.\n\nThe following prompt provides two examples that show preference to the shorter explanations. In the response, you can see that the examples guided the model to choose the shorter explanation (Explanation2) as opposed to the longer explanation (Explanation1) like it did previously.\n\n**Prompt:**\n    \n    \n    Below are some examples showing a question, explanation, and answer format:\n    \n    Question: Why is sky blue?\n    Explanation1: The sky appears blue because of Rayleigh scattering, which causes shorter blue\n    wavelengths of light to be scattered more easily than longer red wavelengths, making the sky look\n    blue.\n    Explanation2: Due to Rayleigh scattering effect.\n    Answer: Explanation2\n    \n    Question: What is the cause of earthquakes?\n    Explanation1: Sudden release of energy in the Earth's crust.\n    Explanation2: Earthquakes happen when tectonic plates suddenly slip or break apart, causing a\n    release of energy that creates seismic waves that can shake the ground and cause damage.\n    Answer: Explanation1\n    \n    Now, Answer the following question given the example formats above:\n    \n    Question: How is snow formed?\n    Explanation1: Snow is formed when water vapor in the air freezes into ice crystals in the\n    atmosphere, which can combine and grow into snowflakes as they fall through the atmosphere and\n    accumulate on the ground.\n    Explanation2: Water vapor freezes into ice crystals forming snow.\n    Answer:\n      \n\n**Response:**\n    \n    \n    Answer: Explanation2\n      \n\n(gemini-1.5-flash)  \n---  \n  \n### Find the optimal number of examples\n\nYou can experiment with the number of examples to provide in the prompt for the most desired results. Models like Gemini can often pick up on patterns using a few examples, though you may need to experiment with what number of examples lead to the desired results. At the same time, if you include too many examples, the model may start to [overfit](https://developers.google.com/machine-learning/glossary#overfitting) the response to the examples.\n\n### Use examples to show patterns instead of antipatterns\n\nUsing examples to show the model a pattern to follow is more effective than using examples to show the model an antipattern to avoid.\n\n⛔ **Negative pattern** :\n\n**Example**\n    \n    \n    Don't end haikus with a question:\n    Haiku are fun\n    A short and simple poem\n    Don't you enjoy them?\n      \n\n  \n  \n---  \n  \n✅ **Positive pattern** :\n\n**Example**\n    \n    \n    Always end haikus with an assertion:\n    Haiku are fun\n    A short and simple poem\n    A joy to write\n      \n\n  \n  \n---  \n  \n### Use consistent formatting across examples\n\nMake sure that the structure and formatting of few-shot examples are the same to avoid responses with undesired formats. One of the primary objectives of adding few-shot examples in prompts is to show the model the response format. Therefore, it is essential to ensure a consistent format across all examples, especially paying attention to XML tags, white spaces, newlines, and example splitters.\n\n### Summary\n\n  * Including prompt-response examples in the prompt helps the model learn how to respond.\n  * Give the model examples of the patterns to follow instead of examples of patterns to avoid.\n  * Experiment with the number of prompts to include. Depending on the model, too few examples are ineffective at changing model behavior. Too many examples cause the model to overfit.\n  * Use consistent formatting across examples\n\n\n\n## Add contextual information\n\nYou can include in the prompt instructions and information that the model needs to solve a problem instead of assuming that the model has all of the required information. This contextual information helps the model understand the constraints and details of what you're asking for it to do.\n\nThe following example asks the model to give troubleshooting guidance for a router:\n\n**Prompt:**\n    \n    \n    What should I do to fix my disconnected wifi? **The light on my Google Wifi router is yellow and\n    blinking slowly.**\n      \n\n**Response:**\n    \n    \n      A yellow blinking light on your Google Wifi router usually indicates a problem with the internet connection. Here are some steps to troubleshoot and fix the issue:\n      1. Restart your router and modem:\n        - Unplug your modem from the power outlet and wait for 30 seconds.\n        - Plug your modem back in and wait for it to fully boot up.\n        - Unplug your Google Wifi router and wait for 30 seconds.\n        - Plug your Google Wifi router back in and wait for it to fully boot up.\n    .....\n      \n\n(gemini-1.5-flash)  \n---  \nThe response looks like generic troubleshooting information that's not specific to the router or the status of the LED indicator lights. To customize the response for the specific router, you can add to the prompt the router's troubleshooting guide as context for it to refer to when providing a response.  **Prompt:**\n    \n    \n    Answer the question using the text below. Respond with only the text provided.\n    Question: What should I do to fix my disconnected wifi? The light on my Google Wifi router is yellow and blinking slowly.\n    \n    Text:\n    Color: Slowly pulsing yellow\n    What it means: There is a network error.\n    What to do:\n    Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.\n    \n    Color: Fast blinking yellow\n    What it means: You are holding down the reset button and are factory resetting this device.\n    What to do:\n    If you keep holding down the reset button, after about 12 seconds, the light will turn solid yellow. Once it is solid yellow, let go of the factory reset button.\n    \n    Color: Solid yellow\n    What it means: Router is factory resetting.\n    What to do:\n    This can take up to 10 minutes. When it's done, the device will reset itself and start pulsing white, letting you know it's ready for setup.\n    \n    Color: Solid red\n    What it means: Something is wrong.\n    What to do:\n    Critical failure. Factory reset the router. If the light stays red, contact Wifi customer support.\n      \n\n**Response:**\n    \n    \n    Check that the Ethernet cable is connected to both your router and your modem and both devices are turned on. You might need to unplug and plug in each device again.\n      \n\n(gemini-1.5-flash)  \n---  \n  \n### Summary\n\n  * Include information (context) in the prompt that you want the model to use when generating a response.\n  * Give the model instructions on how to use the contextual information.\n\n\n\n## Add prefixes\n\nA prefix is a word or phrase that you add to the prompt content that can serve several purposes, depending on where you put the prefix:\n\n  * **Input prefix:** Adding a prefix to the input signals semantically meaningful parts of the input to the model. For example, the prefixes \"English:\" and \"French:\" demarcate two different languages.\n  * **Output prefix:** Even though the output is generated by the model, you can add a prefix for the output in the prompt. The output prefix gives the model information about what's expected as a response. For example, the output prefix \"JSON:\" signals to the model that the output should be in JSON format.\n  * **Example prefix:** In few-shot prompts, adding prefixes to the examples provides labels that the model can use when generating the output, which makes it easier to parse output content.\n\n\n\nIn the following example, \"Text:\" is the input prefix and \"The answer is:\" is the output prefix.\n\n**Prompt:**\n    \n    \n    Classify the text as one of the following categories.\n    - large\n    - small\n    Text: Rhino\n    The answer is: large\n    Text: Mouse\n    The answer is: small\n    Text: Snail\n    The answer is: small\n    Text: Elephant\n    The answer is:\n      \n\n**Response:**\n    \n    \n    The answer is: large\n      \n\n(gemini-1.5-flash)  \n---  \n  \n## Let the model complete partial input\n\nGenerative language models work like an advanced autocompletion tool. When you provide partial content, the model can provide the rest of the content or what it thinks is a continuation of that content as a response. When doing so, if you include any examples or context, the model can take those examples or context into account.\n\nThe following example provides a prompt with an instruction and an entity input:\n\n**Prompt:**\n    \n    \n    For the given order, return a JSON object that has the fields cheeseburger, hamburger, fries, or\n    drink, with the value being the quantity.\n    \n    Order: A burger and a drink.\n      \n\n**Response:**\n    \n    \n    ```\n    {\n      \"hamburger\": 1,\n      \"drink\": 1\n    }\n    ```\n      \n\n(gemini-1.5-flash)  \n---  \n  \nWhile the model did as prompted, writing out the instructions in natural language can sometimes be challenging and it leaves a lot to the model's interpretation. For example, a restaurants menu might contain many items. To reduce the size of the JSON response, you probably want to omit the items that weren't ordered. In this case, you can give an example and a response prefix and let the model complete it:\n\n**Prompt:**\n    \n    \n    Valid fields are cheeseburger, hamburger, fries, and drink.\n    Order: Give me a cheeseburger and fries\n    Output:\n    ```\n    {\n      \"cheeseburger\": 1,\n      \"fries\": 1\n    }\n    ```\n    Order: I want two burgers, a drink, and fries.\n    Output:\n      \n\n**Response:**\n    \n    \n    ```\n    {\n      \"hamburger\": 2,\n      \"drink\": 1,\n      \"fries\": 1\n    }\n    ```\n      \n\n(gemini-1.5-flash)  \n---  \n  \nNotice how \"cheeseburger\" was excluded from the output because it wasn't a part of the order.\n\n#### Prompt the model to format its response\n\nThe completion strategy can also help format the response. The following example prompts the model to create an essay outline:\n\n**Prompt:**\n    \n    \n    Create an outline for an essay about hummingbirds.\n      \n\n**Response:**\n    \n    \n      Hummingbirds: A Symphony of Speed and Color\n        I. Introduction\n        A. Hummingbirds are fascinating creatures known for their dazzling colors, lightning-fast flight, and unique feeding habits.\n        B. Brief overview of the diversity of hummingbird species and their geographic distribution.\n        C. Thesis statement: Hummingbirds are an embodiment of resilience, adaptation, and the wonders of the natural world.\n    .....\n      \n\n(gemini-1.5-flash)  \n---  \n  \nThe prompt didn't specify the format for the outline and the model chose a format for you. To get the model to return an outline in a specific format, you can add text that represents the start of the outline and let the model complete it based on the pattern that you initiated.\n\n**Prompt:**\n    \n    \n    Create an outline for an essay about hummingbirds.\n    I. Introduction\n       *\n      \n\n**Response:**\n    \n    \n      Hummingbirds: Jewels of the Air\n        **I. Introduction**\n           * **Hook:** Begin with a captivating image or anecdote about hummingbirds, emphasizing their vibrant colors, rapid flight, and unique feeding habits.\n           * **Background:** Briefly introduce hummingbirds as the smallest birds in the world and their fascinating adaptations for a nectar-rich diet.\n           * **Thesis:** State the main argument or focus of the essay. Examples: \n                * **The incredible adaptations of hummingbirds enable them to thrive in diverse ecosystems.**\n                * **Hummingbirds play a crucial role in pollination and biodiversity.**\n                * **The beauty and grace of hummingbirds make them captivating subjects for both scientific study and artistic appreciation.**\n      \n\n(gemini-1.5-flash)  \n---  \n  \n### Summary\n\n  * If you give the model a partial input, the model completes that input based on any available examples or context in the prompt.\n  * Having the model complete an input may sometimes be easier than describing the task in natural language.\n  * Adding a partial answer to a prompt can guide the model to follow a desired pattern or format.\n\n\n\n## Break down prompts into simple components\n\nFor use cases that require complex prompts, you can help the model manage this complexity by breaking things down into simpler components.\n\n### Break down instructions\n\nInstead of having many instructions in one prompt, create one prompt per instruction. You can choose which prompt to process based on the user's input.\n\n### Chain prompts\n\nFor complex tasks that involve multiple sequential steps, make each step a prompt and chain the prompts together in a sequence. In this sequential chain of prompts, the output of one prompt in the sequence becomes the input of the next prompt. The output of the last prompt in the sequence is the final output. \n\n### Aggregate responses\n\nAggregation is when you want to perform different parallel tasks on different portions of the data and aggregate the results to produce the final output. For example, you can tell the model to perform one operation on the first part of the data, perform another operation on the rest of the data and aggregate the results.\n\n### Summary\n\n  * Break down complex instructions into a prompt for each instruction and decide which prompt to apply based on the user's input.\n  * Break down multiple sequential steps into separate prompts and chain them such that the output on the preceding prompt becomes the input of the following prompt.\n  * Break down parallel tasks and aggregate the responses to produce the final output.\n\n\n\n## Experiment with different parameter values\n\nEach call that you send to a model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values. Experiment with different parameter values to get the best values for the task. The parameters available for different models may differ. The most common parameters are the following:\n\n  * Max output tokens\n  * Temperature\n  * Top-K\n  * Top-P\n\n\n\n### Max output tokens\n\nMaximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 20 words. \n\nSpecify a lower value for shorter responses and a higher value for longer responses.\n\n### Temperature\n\nThe temperature is used for sampling during response generation, which occurs when `topP` and `topK` are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic and less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of `0` is deterministic, meaning that the highest probability response is always selected. \n\nFor most use cases, try starting with a temperature of `0.2`. If the model returns a response that's too generic, too short, or the model gives a fallback response, try increasing the temperature.\n\n### Top-K\n\nTop-K changes how the model selects tokens for output. A top-K of `1` means the next selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-K of `3` means that the next token is selected from among the three most probable tokens by using temperature. \n\nFor each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further filtered based on top-P with the final token selected using temperature sampling.\n\nSpecify a lower value for less random responses and a higher value for more random responses. The default top-K is `40`.\n\n### Top-P\n\nTop-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K) to least probable until the sum of their probabilities equals the top-P value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-P value is `0.5`, then the model will select either A or B as the next token by using temperature and excludes C as a candidate. \n\nSpecify a lower value for less random responses and a higher value for more random responses. The default top-P is `0.95`.\n\n## Prompt iteration strategies\n\nPrompt design is an iterative process that often requires a few iterations before you get the desired response consistently. This section provides guidance on some things you can try when iterating on your prompts.\n\n### Use different phrasing\n\nUsing different words or phrasing in your prompts often yields different responses from the model even though they all mean the same thing. If you're not getting the expected results from your prompt, try rephrasing it.\n    \n    \n    Version 1:\n    How do I bake a pie?\n    \n    Version 2:\n    Suggest a recipe for a pie.\n    \n    Version 3:\n    What's a good pie recipe?\n      \n\n  \n  \n---  \n  \n### Switch to an analogous task\n\nIf you can't get the model to follow your instructions for a task, try giving it instructions for an analogous task that achieves the same result.\n\nThis prompt tells the model to categorize a book by using predefined categories.\n\n**Prompt:**\n    \n    \n    Which category does The Odyssey belong to:\n    thriller\n    sci-fi\n    mythology\n    biography\n      \n\n**Response:**\n    \n    \n        The Odyssey belongs to the category of **mythology**. \n    \n        Here's why:\n        \n        * **Mythology:** The Odyssey tells the story of Odysseus, a hero from Greek mythology, and his\n        journey home after the Trojan War. It features gods, monsters, and supernatural events common to\n        Greek mythology.\n    .....\n      \n\n(gemini-1.5-flash)  \n---  \n  \nThe response is correct, but the model didn't stay within the bounds of the options. You also want to model to just respond with one of the options instead of in a full sentence. In this case, you can rephrase the instructions as a multiple choice question and ask the model to choose an option.\n\n**Prompt:**\n    \n    \n    Multiple choice problem: Which of the following options describes the book The Odyssey?\n    Options:\n    - thriller\n    - sci-fi\n    - mythology\n    - biography\n      \n\n**Response:**\n    \n    \n    The correct answer is **mythology**. \n      \n\n(gemini-1.5-flash)  \n---  \n  \n### Change the order of prompt content\n\nThe order of the content in the prompt can sometimes affect the response. Try changing the content order and see how that affects the response.\n    \n    \n    Version 1:\n    [examples]\n    [context]\n    [input]\n    \n    Version 2:\n    [input]\n    [examples]\n    [context]\n    \n    Version 3:\n    [examples]\n    [input]\n    [context]\n    \n\n## Fallback responses\n\nA fallback response is a response returned by the model when either the prompt or the response triggers a safety filter. An example of a fallback response is \"I'm not able to help with that, as I'm only a language model.\"\n\nIf the model responds with a fallback response, try increasing the temperature.\n\n## Things to avoid\n\n  * Avoid relying on models to generate factual information.\n  * Use with care on math and logic problems.\n\n\n\n## Next steps\n\n  * Now that you have a deeper understanding of prompt design, try writing your own prompts using [Google AI Studio](http://aistudio.google.com).\n  * To learn about multimodal prompting, see [Prompting with media files](/gemini-api/docs/prompting_with_media).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/file-prompting-strategies",
    "title": "File prompting strategies  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  File prompting strategies \n\nThe Gemini family of artificial intelligence (AI) models is built to handle various types of input data, including text, images, and audio. Since these models can handle more than one type or _mode_ of data, the Gemini models are called _multimodal models_ or explained as having _multimodal capabilities_.\n\n**Note:** Prompting with media files is supported by specific Gemini models. For more information about which models support this feature, see the [Vision](/gemini-api/docs/vision) and [Audio](/gemini-api/docs/audio) guides.\n\nThis page provides guidance and best practices for using media files with prompts for the Gemini API. For information on using the Gemini Files API for uploading media files and including them in your prompts, see the [Vision](/gemini-api/docs/vision), [Audio](/gemini-api/docs/audio), and [Document processing](/gemini-api/docs/document-processing) guides.\n\n## Using media files with prompts\n\nBeing able to use various types of data in your prompts gives you more flexibility in terms of what tasks you can tackle with the Gemini API. For example, you can send the model a photo of a delicious meal and ask it to write a short blog about the meal.\n\n**Prompt** | **Response**  \n---|---  \nWrite a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.   \n|  Meal prepping is a great way to save time and money, and it can also help you to eat healthier. This meal is a great example of a healthy and delicious meal that can be easily prepped ahead of time. This meal features brown rice, roasted vegetables, and chicken teriyaki. The brown rice is a healthy and complex carbohydrate that will give you sustained energy. The roasted vegetables are a great way to get your daily dose of vitamins and minerals, and the chicken teriyaki is a delicious and protein-rich option. This meal is also very easy to prepare. Simply cook the brown rice, roast the vegetables, and cook the chicken teriyaki. Once everything is cooked, divide it into meal prep containers and store them in the refrigerator. You can then grab a container and go on busy mornings! If you're looking for a healthy and delicious meal that can be easily prepped ahead of time, this meal is a great option. It's packed with nutrients and flavor, and it's sure to keep you feeling full and satisfied. Here's to healthy and delicious meal prepping!  \n  \n## Strategies for prompting with media\n\nIf you are having trouble getting the output you want from prompts that use media files, there are some strategies that can help you get the results you want. The following sections provide design approaches and troubleshooting tips for improving prompts that use multimodal input.\n\nYou can improve your multimodal prompts by following these best practices:\n\n  * ### Prompt design fundamentals\n\n    * **Be specific in your instructions** : Craft clear and concise instructions that leave minimal room for misinterpretation.\n    * **Add a few examples to your prompt:** Use realistic few-shot examples to illustrate what you want to achieve.\n    * **Break it down step-by-step** : Divide complex tasks into manageable sub-goals, guiding the model through the process.\n    * **Specify the output format** : In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more. \n    * **Put your image first for single-image prompts** : While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image is placed before the text prompt.\n  * ### Troubleshooting your multimodal prompt\n\n    * **If the model is not drawing information from the relevant part of the image:** Drop hints with which aspects of the image you want the prompt to draw information from.\n    * **If the model output is too generic (not tailored enough to the image):** At the start of the prompt, try asking the model to describe the image(s) before providing the task instruction, or try asking the model to refer to what's in the image.\n\n\n\n## Prompt design fundamentals\n\nThis section expands on the best practices listed in the previous section.\n\nNote: Model results from the following Gemini examples can vary from run to run.\n\n### Be specific in your instructions\n\nPrompts have the most success when they are clear and detailed. If you have a specific output in mind, it's better to include that requirement in the prompt to ensure you get the output you want. \n\nSometimes, a prompt's intent might seem clear to the person who wrote it, but it is actually under-specified. To customize model behavior, consider how your prompt could be (mis)interpreted, and ensure that the instructions you give are specific and clear. Remember to leave minimal room for misinterpretation\n\nFor this image of an airport board, asking the model to just \"describe this image\" could generate a general description. If you need the model to parse the time and city from the image, you can include that request directly in your prompt.\n\nPrompt | Model response  \n---|---  \nDescribe this image. | The image shows an airport arrivals and departures board.  \n**Updated prompt** | **Improved response**  \n---|---  \nParse the time and city from the airport board shown in this image into a list. | \n* 10:50 Moscow\n* 11:05 Edinburgh\n* 11:05 London\n* 11:10 Bucharest\n* 11:30 Kiev\n* 11:35 Dublin\n* 11:45 East Midlands\n* 12:15 Sofia\n* 12:30 London\n* 12:30 Newcastle\n* 12:40 St Petersburg\n* 12:40 London\n* 12:45 Manchester  \n  \n### Add a few examples\n\nThe Gemini model can accept multiple inputs which it can use as examples to understand the output you want. Adding these examples can help the model identify the patterns and apply the relationship between the given images and responses to the new example. This is also called \"few-shot\" learning.\n\nIn the example below, the initial output is written in sentence form, and also contains the country (Brazil). Suppose you want the output in a different format or style, and you want only the city, not the country. Adding few-shot examples to your prompt can steer the model to respond in the way you want.\n\n**Prompt** | **Model response**  \n---|---  \nDetermine the city along with the landmark. | The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil.  \n**Updated prompt** | **Improved response**  \n---|---  \nDetermine the city along with the landmark. |  city: Rome, landmark: the Colosseum.  \n|  city: Beijing, landmark: Forbidden City  \n|  city: Rio de Janeiro, landmark: Christ the Redeemer statue  \n  \n### Break it down step-by-step\n\nFor complex tasks like the ones that require both visual understanding and reasoning, it can be helpful to split the task into smaller, more straightforward steps. Alternatively, it could also be effective if you directly ask the model to “think step by step” in your prompt.\n\nPrompt | Model response  \n---|---  \nWhen will I run out of toilet paper? | Soon, you only have 3 rolls left.  \n**Updated prompt** | **Improved response**  \n---|---  \n  \n1\\. First, count how many toilet paper rolls are in this picture.\n2\\. Then, determine how much toilet paper a typical person uses per day.\n3\\. Calculate how long these rolls of toilet paper will last.\n| \n\n1\\. There are 3 rolls of toilet paper in this picture.\n2\\. A typical person uses about 20 sheets of toilet paper per day.\n3\\. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month.\n  \n  \nMath problems or other types of word problems are great candidates for asking the model to think step-by-step.\n\n**Prompt** | **Response**  \n---|---  \nWhat is the 4th term in the sequence? | -135   \n  \nThe response from the model is incorrect. Some ways to improve this is to ask the model to parse the problem before answering the prompt.\n\n**Prompt** | **Response**  \n---|---  \nParse the formula in the image first. Then based on the formula, answer \"what's the 4th term in the sequence?\". |  _Click to expand the result_ $$ b(n) = b(n-1) * (-3) $$ $$ b(1) = 15 $$\n\n  1. Substitute n = 4 into the formula:\n$$ b(4) = b(4-1) * (-3) $$ $$ b(4) = b(3) * (-3) $$\n  2. We don't know the value of b(3), so substitute n = 3 into the formula:\n$$ b(3) = b(3-1) * (-3) $$ $$ b(3) = b(2) * (-3) $$\n  3. We don't know the value of b(2), so substitute n = 2 into the formula:\n$$ b(2) = b(2-1) * (-3) $$ $$ b(2) = b(1) * (-3) $$\n  4. We know the value of b(1) is 15, so substitute it into the formula:\n$$ b(2) = 15 * (-3) $$ $$ b(2) = -45 $$\n  5. Now that we know the value of b(2), substitute it into the formula for b(3):\n$$ b(3) = b(2) * (-3) $$ $$ b(3) = -45 * (-3) $$ $$ b(3) = 135 $$\n  6. Now that we know the value of b(3), substitute it into the formula for b(4):\n$$ b(4) = b(3) * (-3) $$ $$ b(4) = 135 * (-3) $$ $$ b(4) = -405 $$ \nTherefore, the 4th term in the sequence is -405.  \n  \nThis model response is correct - now to make the response even more streamlined, you can ask the model to to think step-by-step.\n\n**Updated prompt** | **Improved response**  \n---|---  \nWhat is the 4th term in the sequence? Think step by step. | The first term is 15. The second term is b(1) * (-3) = 15 * (-3) = -45. The third term is b(2) * (-3) = -45 * (-3) = 135. The fourth term is b(3) * (-3) = 135 * (-3) = -405.  \n  \n### Try specifying the output format\n\nA common problem is the need for model output to be in a certain format (e.g. markdown, JSON, HTML), especially if the model output needs to be ingested by a downstream task. You can try to produce output in that format by instructing the model to do so within the prompt. Below are two examples:\n\nPrompt | Model response  \n---|---  \nParse the table in this image into markdown format |  | Subject | Status | Time | Start date | Due on | | ------- | ------ | ---- | --------- | ------ | | Math | Done | 1.5 hrs | 11/29/2023 | 12/1/2023 | | Geography | Done | 30 min | 12/2/2023 | 12/4/2023 | | Physics | In progress | 2 hrs | 12/3/2023 | 12/5/2023 | | Biology | Not started | 6 hrs | 11/2/2016 | 12/6/2023 |  \n**Prompt** | **Model response**  \n---|---  \nProvide a list of all the following attributes: ingredients, type of cuisine, vegetarian or not, in JSON format | ` ```json``` { \"ingredients\": [ \"rice\", \"shrimp\", \"clams\", \"mussels\", \"peas\", \"tomatoes\", \"onions\", \"garlic\", \"olive oil\", \"paprika\", \"salt\", \"pepper\" ], \"type of cuisine\": \"Spanish\", \"vegetarian\": false `  \n  \n### Put your image first for single-image prompts\n\nWhile Gemini can interpret image and text in any order within a prompt, placing a single image before the text prompt might lead to better results. In the following samples, you'll notice the image comes first before the text in each prompt.\n\n## Troubleshooting your multimodal prompt\n\nYou might need to troubleshoot your prompt if you are not getting a helpful response. Here are a few strategies you could try.\n\n### If the model is not drawing information from the relevant part of the image\n\nTo get a more specific response, you can point out which aspects of the image you want the prompt to draw information from to inform its response.\n\nPrompt | Model response  \n---|---  \nHow many days will these diapers last a baby? | The diapers will last for 198 days before they run out.  \n**Updated prompt** | **Improved response**  \n---|---  \nHow long will these diapers last before I run out? Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box. Divide the total number by how many diapers the child goes through per day. |  The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs. The average child in this weight range is likely between 1 and 2 years old. A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6).  \n  \n### If the model output is too generic and not tailored enough to the image input\n\nTo help the model tailor its response to the image(s), try asking it to describe the images before performing its reasoning task.\n\nPrompt | Model response |  What is in common between these images? | All four images are examples of landscape photography.  \n---|---  \n**Updated prompt** | **Improved response**  \n---|---  \nFirst, describe what's in each image in detail. What's in common between these images? |  The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea. The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity. The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private and public realms. It's a place where we prepare ourselves to enter or leave the outside world. The fourth image shows the sea in between being calm and being stormy. Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action.  \n  \nYou can also try explicitly asking the model to refer to what's in the image.\n\n**Updated prompt** | **Improved response**  \n---|---  \nWhat is in common between these images? Refer to what's in the images in your response. |  All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a dark and stormy sea. These images are all related to travel because they are all things that people need or use when they travel. The first image shows a person packing a suitcase, which is something that people do when they are going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people often go to buy things for their trips. The third image shows a stack of toilet paper, which is something that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is something that people often encounter when they are traveling.  \n  \nThe same tips work for creative output from images. In the following example with the rabbit, explicitly asking the model to include aspects from the image can improve the output.\n\nPrompt | Model response |  Can you write me a poem about this image? |  Little bunny, so soft and brown,  \nHopping through the field, up and down.  \nYour ears so long, your tail so small,  \nYou're the cutest creature of them all. You nibble on the grass, so green and sweet,  \nAnd hop away when you see me.  \nBut I'll always be your friend,  \nLittle bunny, until the very end.  \n---|---  \n**Updated prompt** | **Improved response**  \n---|---  \nCan you write me a descriptive and dramatic poem about this image and include the location? |  Little bunny on the green,  \nEars so long, so keen.  \nWhat do you see out there,  \nIn the distance, where you stare? Is it the ocean blue,  \nSparkling in the sun's hue?  \nOr the waves that crash and roar,  \nA rhythm that you adore? Little bunny, tell me true,  \nWhat captures your attention so?  \nIs it the salty breeze,  \nOr the seagulls' cries that please? Perhaps it's the vastness of it all,  \nA world so big and small.  \nA place where dreams can roam,  \nAnd adventures can be sown. Little bunny, I wonder what you dream,  \nAs you sit on the grass, so serene.  \nDo you yearn to explore the deep,  \nOr stay on land, where you can leap? Whatever it may be, little bunny,  \nKeep that spark of wonder burning brightly.  \nFor in your dreams and aspirations,  \nLies a world waiting for your creation.  \n  \n### Troubleshooting which part of the prompt failed\n\nIt can be hard to know whether a prompt failed because the model didn't **understand the image** to begin with, or if it did understand the image but did not perform the correct**reasoning steps** afterward.\n\nTo disambiguate between those reasons, ask the model to describe what's in the image.\n\nIn this example below, if the model responds with a snack that seems surprising when paired with tea (e.g. popcorn), you can first troubleshoot to determine whether the model correctly recognized that the image contains tea.\n\nPrompt | Prompt for troubleshooting  \n---|---  \nWhat's a snack I can make in 1 minute that would go well with this? | Describe what's in this image.   \n  \nAnother strategy is to ask the model to explain its reasoning. That can help you narrow down which part of the reasoning broke down, if any.\n\nPrompt | Prompt for troubleshooting  \n---|---  \nWhat's a snack I can make in 1 minute that would go well with this? | What's a snack I can make in 1 minute that would go well with this? Please explain why.  \n  \n### Tuning the sampling parameters\n\nIn each request, you send not only the multimodal prompt but a set of sampling parameters to the model. The model can generate different results for different parameter values. Experiment with the different parameters to get the best values for the task. The most commonly adjusted parameters are the following:\n\n  * Temperature\n  * top-P\n  * top-K\n\n\n\n#### Temperature\n\nTemperature is used for sampling during response generation, which occurs when top-P and top-K are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic and less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected.  \n\n\nFor most use cases, try starting with a temperature of 0.4. If you need more creative results, try increasing the temperature. If you observe clear hallucinations, try reducing the temperature.\n\n#### Top-K\n\nTop-K changes how the model selects tokens for output. A top-K of 1 means the next selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-K of 3 means that the next token is selected from among the three most probable tokens by using temperature.  \n\n\nFor each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further filtered based on top-P with the final token selected using temperature sampling.\n\nSpecify a lower value for less random responses and a higher value for more random responses. The default value of top-K is 32.\n\n#### Top-P\n\nTop-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K) to least probable until the sum of their probabilities equals the top-P value. For example, if tokens A, B, and C have a probability of 0.6, 0.3, 0.1 and the top-P value is 0.9, then the model will select either A or B as the next token by using temperature and excludes C as a candidate.  \n\n\nSpecify a lower value for less random responses and a higher value for more random responses. The default value of top-P is 1.0.\n\n## Next steps\n\n  * Try writing your own multimodal prompts using [Google AI Studio](http://aistudio.google.com).\n  * For more guidance on prompt design, see the [Prompt strategies](/gemini-api/docs/prompting-strategies) page.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/imagen-prompt-guide",
    "title": "Imagen prompt guide  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Imagen prompt guide \n\nTo use Imagen you must provide a text description of what you want to generate. These descriptions are called _prompts_ , and these prompts are the primary way you communicate with Imagen.\n\nThis guide shows you how modifying parts of a text-to-image prompt can produce different results and gives you examples of images you can create. \n\n## Prompt writing basics (subject, context, and style)\n\n**Note:** Maximum prompt length is 480 tokens.\n\nWhile there are many ways to write a good prompt, adding some keywords and modifiers will help you get closer to your end goal. Prompts don't need to be long or complex, but most good prompts are descriptive and clear.\n\nA good starting point can be to think of **subject** , **context** , and **style**.\n\nImage text: A _sketch_ (**style**) of a _modern apartment building_ (**subject**) surrounded by _skyscrapers_ (**context and background**).\n\n  1. **Subject** : The first thing to think about with any prompt is the _subject_ : the object, person, animal, or scenery you want an image of.\n\n  2. **Context and background:** Just as important is the _background or context_ in which the subject will be placed. Try placing your subject in a variety of backgrounds. For example, a studio with a white background, outdoors, or indoor environments.\n\n  3. **Style:** Finally, add the style of image you want. _Styles_ can be general (painting, photograph, sketches) or very specific (pastel painting, charcoal drawing, isometric 3D). You can also combine styles.\n\n\n\n\nAfter you write a first version of your prompt, refine your prompt by adding more details until you get to the image that you want. Iteration is important. Start by establishing your core idea, and then refine and expand upon that core idea until the generated image is close to your vision.\n\nPrompt: A park in the spring next to a lake |  Prompt: A park in the spring next to a lake, **the sun sets across the lake, golden hour** |  Prompt: A park in the spring next to a lake, _**the sun sets across the lake, golden hour, red wildflowers**_  \n---|---|---  \n  \n### Imagen 3 prompt writing\n\nImagen 3 can transform your ideas into detailed images, whether your prompts are short or long and detailed. Refine your vision through iterative prompting, adding details until you achieve the perfect result.\n\nShort prompts let you generate an image quickly. Prompt: close-up photo of a woman in her 20s, street photography, movie still, muted orange warm tones  |  Longer prompts let you add specific details and build your image. Prompt: captivating photo of a woman in her 20s utilizing a street photography style. The image should look like a movie still with muted orange warm tones.   \n---|---  \n  \nAdditional advice for Imagen 3 prompt writing:\n\n  * **Use descriptive language** : Employ detailed adjectives and adverbs to paint a clear picture for Imagen 3.\n  * **Provide context** : If necessary, include background information to aid the AI's understanding.\n  * **Reference specific artists or styles** : If you have a particular aesthetic in mind, referencing specific artists or art movements can be helpful.\n  * **Use prompt engineering tools** : Consider exploring prompt engineering tools or resources to help you refine your prompts and achieve optimal results.\n  * **Enhancing the facial details in your personal and group images** : \n    * Specify facial details as a focus of the photo (for example, use the word \"portrait\" in the prompt).\n\n\n\n### Generate text in images\n\nImagen 3's ability to add text into your images opens up creative image generation possibilities. Use the following guidance to get the most out of this feature:\n\n  * **Iterate with confidence** : You might have to regenerate images until you achieve the look you want. Imagen's text integration is still evolving, and sometimes multiple attempts yield the best results.\n  * **Keep it short** : Limit text to 25 characters or less for optimal generation.\n  * **Multiple phrases** : Experiment with two or three distinct phrases to provide additional information. Avoid exceeding three phrases for cleaner compositions.\n\nPrompt: A poster with the text \"Summerland\" in bold font as a title, underneath this text is the slogan \"Summer never felt so good\" \n  * **Guide Placement** : While Imagen can attempt to position text as directed, you should expect occasional variations. This feature is continually improving.\n\n  * **Inspire font style** : Specify a general font style to subtly influence Imagen's choices. Don't rely on precise font replication, but expect creative interpretations.\n\n  * **Font size** : Specify a font size or a general indication of size (for example, _small_ , _medium_ , _large_) to influence the font size generation.\n\n\n\n\n### Prompt parameterization\n\nTo better control output results, you might find it helpful to parameterize the inputs into Imagen. For example, suppose you want your customers to be able to generate logos for their business, and you want to make sure logos are always generated on a solid color background. You also want to limit the options that the client can select from a menu.\n\nIn this example, you can create a parameterized prompt similar to the following:\n    \n    \n    A {logo_style} logo for a {company_area} company on a solid color background. Include the text {company_name}.\n\nIn your custom user interface, the customer can input the parameters using a menu, and their chosen value populates the prompt Imagen receives.\n\nFor example:\n\n  1. Prompt: `A minimalist logo for a health care company on a solid color background. Include the text Journey.`\n\n  2. Prompt: `A modern logo for a software company on a solid color background. Include the text Silo.`\n\n  3. Prompt: `A traditional logo for a baking company on a solid color background. Include the text Seed.`\n\n\n\n\n### Style: photography\n\n  * Prompt includes: _\"A photo of...\"_\n\n\n\nTo use this style, start with using keywords that clearly tell Imagen that you're looking for a photograph. Start your prompts with _\"A photo of. . .\"_. For example:\n\nPrompt: **A photo of** coffee beans in a kitchen on a wooden surface |  Prompt: **A photo of** a chocolate bar on a kitchen counter |  Prompt: **A photo of** a modern building with water in the background  \n---|---|---  \n  \nImage source: Each image was generated using its corresponding text prompt with the Imagen 3 model.\n\n### Style: illustration and art\n\n  * Prompt includes: _\"A painting of...\"_, _\"A sketch of...\"_\n\n\n\nArt styles vary from monochrome styles like pencil sketches, to hyper-realistic digital art. For example, the following images use the same prompt with different styles:\n\n_\"An[art style or creation technique] of an angular sporty electric sedan with skyscrapers in the background\"_\n\nPrompt: A **technical pencil drawing** of an angular... |  Prompt: A **charcoal drawing** of an angular... |  Prompt: A **color pencil drawing** of an angular...  \n---|---|---  \nPrompt: A **pastel painting** of an angular... |  Prompt: A **digital art** of an angular... |  Prompt: An **art deco (poster)** of an angular...  \n---|---|---  \n  \nImage source: Each image was generated using its corresponding text prompt with the Imagen 2 model.\n\n## Advanced prompt writing techniques\n\nUse the following examples to create more specific prompts based on the attributes: photography descriptors, shapes and materials, historical art movements, and image quality modifiers.\n\n### Photography modifiers\n\nIn the following examples, you can see several photography-specific modifiers and parameters. You can combine multiple modifiers for more precise control.\n\n  1. **Camera Proximity** \\- _Close up, taken from far away_\n\nPrompt: A **close-up** photo of coffee beans |  Prompt: A **zoomed out** photo of a small bag of  \ncoffee beans in a messy kitchen  \n---|---  \n  \n  2. **Camera Position** \\- _aerial, from below_\n\nPrompt: **aerial photo** of urban city with skyscrapers |  Prompt: A photo of a forest canopy with blue skies **from below**  \n---|---  \n  3. **Lighting** \\- _natural, dramatic, warm, cold_\n\nPrompt: studio photo of a modern arm chair, **natural lighting** |  Prompt: studio photo of a modern arm chair, **dramatic lighting**  \n---|---  \n  4. **Camera Settings** _\\- motion blur, soft focus, bokeh, portrait_\n\nPrompt: photo of a city with skyscrapers from the inside of a car with **motion blur** |  Prompt: **soft focus** photograph of a bridge in an urban city at night  \n---|---  \n  5. **Lens types** \\- _35mm, 50mm, fisheye, wide angle, macro_\n\nPrompt: photo of a leaf, **macro lens** |  Prompt: street photography, new york city, **fisheye lens**  \n---|---  \n  6. **Film types** \\- _black and white, polaroid_\n\nPrompt: a **polaroid portrait** of a dog wearing sunglasses |  Prompt: **black and white photo** of a dog wearing sunglasses  \n---|---  \n\n\n\nImage source: Each image was generated using its corresponding text prompt with the Imagen 3 model.\n\n### Shapes and materials\n\n  * Prompt includes: _\"...made of...\"_ , _\"...in the shape of...\"_\n\n\n\nOne of the strengths of this technology is that you can create imagery that is otherwise difficult or impossible. For example, you can recreate your company logo in different materials and textures.\n\nPrompt: a duffle bag **made of** cheese |  Prompt: neon tubes **in the shape** of a bird |  Prompt: an armchair **made of paper** , studio photo, origami style  \n---|---|---  \n  \nImage source: Each image was generated using its corresponding text prompt with the Imagen 3 model.\n\n### Historical art references\n\n  * Prompt includes: _\"...in the style of...\"_\n\n\n\nCertain styles have become iconic over the years. The following are some ideas of historical painting or art styles that you can try.\n\n_\"generate an image in the style of[art period or movement] : a wind farm\"_\n\nPrompt: generate an image **in the style of _an impressionist painting_** : a wind farm |  Prompt: generate an image **in the style of _a renaissance painting_** : a wind farm |  Prompt: generate an image **in the style of _pop art_** : a wind farm  \n---|---|---  \n  \nImage source: Each image was generated using its corresponding text prompt with the Imagen 3 model.\n\n### Image quality modifiers\n\nCertain keywords can let the model know that you're looking for a high-quality asset. Examples of quality modifiers include the following:\n\n  * **General Modifiers** \\- _high-quality, beautiful, stylized_\n  * **Photos** \\- _4K, HDR, Studio Photo_\n  * **Art, Illustration** \\- _by a professional, detailed_\n\n\n\nThe following are a few examples of prompts without quality modifiers and the same prompt with quality modifiers.\n\nPrompt (no quality modifiers): a photo of a corn stalk |  Prompt (with quality modifiers): **4k HDR beautiful**   \nphoto of a corn stalk **taken by a  \nprofessional photographer**  \n---|---  \n  \nImage source: Each image was generated using its corresponding text prompt with the Imagen 3 model.\n\n### Aspect ratios\n\nImagen 3 image generation lets you set five distinct image aspect ratios.\n\n  1. **Square** (1:1, default) - A standard square photo. Common uses for this aspect ratio include social media posts.\n  2. **Fullscreen** (4:3) - This aspect ratio is commonly used in media or film. It is also the dimensions of most old (non-widescreen) TVs and medium format cameras. It captures more of the scene horizontally (compared to 1:1), making it a preferred aspect ratio for photography.\n\nPrompt: close up of a musician's fingers playing the piano, black and white film, vintage (4:3 aspect ratio)  |  Prompt: A professional studio photo of french fries for a high end restaurant, in the style of a food magazine (4:3 aspect ratio)   \n---|---  \n  3. **Portrait full screen** (3:4) - This is the fullscreen aspect ratio rotated 90 degrees. This lets to capture more of the scene vertically compared to the 1:1 aspect ratio.\n\nPrompt: a woman hiking, close of her boots reflected in a puddle, large mountains in the background, in the style of an advertisement, dramatic angles (3:4 aspect ratio)  |  Prompt: aerial shot of a river flowing up a mystical valley (3:4 aspect ratio)   \n---|---  \n  4. **Widescreen** (16:9) - This ratio has replaced 4:3 and is now the most common aspect ratio for TVs, monitors, and mobile phone screens (landscape). Use this aspect ratio when you want to capture more of the background (for example, scenic landscapes).\n\nPrompt: a man wearing all white clothing sitting on the beach, close up, golden hour lighting (16:9 aspect ratio) \n  5. **Portrait** (9:16) - This ratio is widescreen but rotated. This a relatively new aspect ratio that has been popularized by short form video apps (for example, YouTube shorts). Use this for tall objects with strong vertical orientations such as buildings, trees, waterfalls, or other similar objects.\n\nPrompt: a digital render of a massive skyscraper, modern, grand, epic with a beautiful sunset in the background (9:16 aspect ratio) \n\n\n\n### Photorealistic images\n\nDifferent versions of the image generation model might offer a mix of artistic and photorealistic output. Use the following wording in prompts to generate more photorealistic output, based on the subject you want to generate.\n\n**Note:** Take these keywords as general guidance when you try to create photorealistic images. They aren't required to achieve your goal. Use case | Lens type | Focal lengths | Additional details  \n---|---|---|---  \nPeople (portraits) | Prime, zoom | 24-35mm | black and white film, Film noir, Depth of field, duotone (mention two colors)  \nFood, insects, plants (objects, still life) | Macro | 60-105mm | High detail, precise focusing, controlled lighting  \nSports, wildlife (motion) | Telephoto zoom | 100-400mm | Fast shutter speed, Action or movement tracking  \nAstronomical, landscape (wide-angle) | Wide-angle | 10-24mm | Long exposure times, sharp focus, long exposure, smooth water or clouds  \n  \n#### Portraits\n\nUse case | Lens type | Focal lengths | Additional details  \n---|---|---|---  \nPeople (portraits) | Prime, zoom | 24-35mm | black and white film, Film noir, Depth of field, duotone (mention two colors)  \n  \nUsing several keywords from the table, Imagen can generate the following portraits.\n\n|  |  |   \n---|---|---|---  \n  \nPrompt: _A woman, 35mm portrait, blue and grey duotones_  \nModel: `imagen-3.0-generate-002`\n\n|  |  |   \n---|---|---|---  \n  \nPrompt: _A woman, 35mm portrait, film noir_  \nModel: `imagen-3.0-generate-002`\n\n#### Objects\n\nUse case | Lens type | Focal lengths | Additional details  \n---|---|---|---  \nFood, insects, plants (objects, still life) | Macro | 60-105mm | High detail, precise focusing, controlled lighting  \n  \nUsing several keywords from the table, Imagen can generate the following object images.\n\n|  |  |   \n---|---|---|---  \n  \nPrompt: _leaf of a prayer plant, macro lens, 60mm_  \nModel: `imagen-3.0-generate-002`\n\n|  |  |   \n---|---|---|---  \n  \nPrompt: _a plate of pasta, 100mm Macro lens_  \nModel: `imagen-3.0-generate-002`\n\n#### Motion\n\nUse case | Lens type | Focal lengths | Additional details  \n---|---|---|---  \nSports, wildlife (motion) | Telephoto zoom | 100-400mm | Fast shutter speed, Action or movement tracking  \n  \nUsing several keywords from the table, Imagen can generate the following motion images.\n\n|  |  |   \n---|---|---|---  \n  \nPrompt: _a winning touchdown, fast shutter speed, movement tracking_  \nModel: `imagen-3.0-generate-002`\n\n|  |  |   \n---|---|---|---  \n  \nPrompt: _A deer running in the forest, fast shutter speed, movement tracking_  \nModel: `imagen-3.0-generate-002`\n\n#### Wide-angle\n\nUse case | Lens type | Focal lengths | Additional details  \n---|---|---|---  \nAstronomical, landscape (wide-angle) | Wide-angle | 10-24mm | Long exposure times, sharp focus, long exposure, smooth water or clouds  \n  \nUsing several keywords from the table, Imagen can generate the following wide-angle images.\n\n|  |  |   \n---|---|---|---  \n  \nPrompt: _an expansive mountain range, landscape wide angle 10mm_  \nModel: `imagen-3.0-generate-002`\n\n|  |  |   \n---|---|---|---  \n  \nPrompt: _a photo of the moon, astro photography, wide angle 10mm_  \nModel: `imagen-3.0-generate-002`\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/tokens",
    "title": "Understand and count tokens  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Understand and count tokens \n\nPython JavaScript Go\n\n  \n\n\nGemini and other generative AI models process input and output at a granularity called a _token_.\n\n## About tokens\n\nTokens can be single characters like `z` or whole words like `cat`. Long words are broken up into several tokens. The set of all tokens used by the model is called the vocabulary, and the process of splitting text into tokens is called _tokenization_.\n\nFor Gemini models, a token is equivalent to about 4 characters. 100 tokens is equal to about 60-80 English words.\n\nWhen billing is enabled, the [cost of a call to the Gemini API](/pricing) is determined in part by the number of input and output tokens, so knowing how to count tokens can be helpful.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-01 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-01 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/safety-settings",
    "title": "Safety settings  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Safety settings \n\nThe Gemini API provides safety settings that you can adjust during the prototyping stage to determine if your application requires more or less restrictive safety configuration. You can adjust these settings across five filter categories to restrict or allow certain types of content.\n\nThis guide covers how the Gemini API handles safety settings and filtering and how you can change the safety settings for your application.\n\n**Note:** Applications that use less restrictive safety settings may be subject to review. See the [Terms of Service](/gemini-api/terms#use-restrictions) for more information.\n\n## Safety filters\n\nThe Gemini API's adjustable safety filters cover the following categories:\n\nCategory | Description  \n---|---  \nHarassment |  Negative or harmful comments targeting identity and/or protected attributes.   \nHate speech |  Content that is rude, disrespectful, or profane.   \nSexually explicit |  Contains references to sexual acts or other lewd content.   \nDangerous |  Promotes, facilitates, or encourages harmful acts.   \nCivic integrity |  Election-related queries.   \nThese categories are defined in [`HarmCategory`](/api/rest/v1/HarmCategory). The Gemini models only support `HARM_CATEGORY_HARASSMENT`, `HARM_CATEGORY_HATE_SPEECH`, `HARM_CATEGORY_SEXUALLY_EXPLICIT`, `HARM_CATEGORY_DANGEROUS_CONTENT`, and `HARM_CATEGORY_CIVIC_INTEGRITY`. All other categories are used only by PaLM 2 (Legacy) models. \n\nYou can use these filters to adjust what's appropriate for your use case. For example, if you're building video game dialogue, you may deem it acceptable to allow more content that's rated as _Dangerous_ due to the nature of the game.\n\nIn addition to the adjustable safety filters, the Gemini API has built-in protections against core harms, such as content that endangers child safety. These types of harm are always blocked and cannot be adjusted.\n\n### Content safety filtering level\n\nThe Gemini API categorizes the probability level of content being unsafe as `HIGH`, `MEDIUM`, `LOW`, or `NEGLIGIBLE`.\n\nThe Gemini API blocks content based on the probability of content being unsafe and not the severity. This is important to consider because some content can have low probability of being unsafe even though the severity of harm could still be high. For example, comparing the sentences:\n\n  1. The robot punched me.\n  2. The robot slashed me up.\n\n\n\nThe first sentence might result in a higher probability of being unsafe, but you might consider the second sentence to be a higher severity in terms of violence. Given this, it is important that you carefully test and consider what the appropriate level of blocking is needed to support your key use cases while minimizing harm to end users.\n\n### Safety filtering per request\n\nYou can adjust the safety settings for each request you make to the API. When you make a request, the content is analyzed and assigned a safety rating. The safety rating includes the category and the probability of the harm classification. For example, if the content was blocked due to the harassment category having a high probability, the safety rating returned would have category equal to `HARASSMENT` and harm probability set to `HIGH`.\n\nBy default, safety settings block content (including prompts) with medium or higher probability of being unsafe across any filter. This baseline safety is designed to work for most use cases, so you should only adjust your safety settings if it's consistently required for your application.\n\nThe following table describes the block settings you can adjust for each category. For example, if you set the block setting to **Block few** for the **Hate speech** category, everything that has a high probability of being hate speech content is blocked. But anything with a lower probability is allowed.\n\nThreshold (Google AI Studio) | Threshold (API) | Description  \n---|---|---  \nBlock none | `BLOCK_NONE` | Always show regardless of probability of unsafe content  \nBlock few | `BLOCK_ONLY_HIGH` | Block when high probability of unsafe content  \nBlock some | `BLOCK_MEDIUM_AND_ABOVE` | Block when medium or high probability of unsafe content  \nBlock most | `BLOCK_LOW_AND_ABOVE` | Block when low, medium or high probability of unsafe content  \nN/A | `HARM_BLOCK_THRESHOLD_UNSPECIFIED` | Threshold is unspecified, block using default threshold  \n  \nIf the threshold is not set, the default block threshold is **Block none** (for `gemini-1.5-pro-002` and `gemini-1.5-flash-002` and all newer stable GA models) or **Block some** (in all other models) for all categories **except** the _Civic integrity_ category.\n\nThe default block threshold for the _Civic integrity_ category is **Block none** (for `gemini-2.0-flash-001` aliased as `gemini-2.0-flash`, `gemini-2.0-pro-exp-02-05`, and `gemini-2.0-flash-lite`) both for Google AI Studio and the Gemini API, and **Block most** for all other models in Google AI Studio only.\n\nYou can set these settings for each request you make to the generative service. See the [`HarmBlockThreshold`](/api/generate-content#harmblockthreshold) API reference for details.\n\n### Safety feedback\n\n[`generateContent`](/api/generate-content#method:-models.generatecontent) returns a [`GenerateContentResponse`](/api/generate-content#generatecontentresponse) which includes safety feedback.\n\nPrompt feedback is included in [`promptFeedback`](/api/generate-content#promptfeedback). If `promptFeedback.blockReason` is set, then the content of the prompt was blocked.\n\nResponse candidate feedback is included in [`Candidate.finishReason`](/api/generate-content#candidate) and [`Candidate.safetyRatings`](/api/generate-content#candidate). If response content was blocked and the `finishReason` was `SAFETY`, you can inspect `safetyRatings` for more details. The content that was blocked is not returned.\n\n## Adjust safety settings\n\nThis section covers how to adjust the safety settings in both Google AI Studio and in your code.\n\n### Google AI Studio\n\nYou can adjust safety settings in Google AI Studio, but you cannot turn them off.\n\nClick **Edit safety settings** in the **Run settings** panel to open the **Run safety settings** modal. In the modal, you can use the sliders to adjust the content filtering level per safety category:\n\n**Note:** If you set any of the category filters to **Block none** , Google AI Studio will display a reminder about the Gemini API's [Terms of Service](/gemini-api/terms#use-restrictions) with respect to safety settings.\n\nWhen you send a request (for example, by asking the model a question), a warning **No Content** message appears if the request's content is blocked. To see more details, hold the pointer over the **No Content** text and click warning **Safety**.\n\n### Gemini API SDKs\n\nThe following code snippet shows how to set safety settings in your `GenerateContent` call. This sets the thresholds for the harassment (`HARM_CATEGORY_HARASSMENT`) and hate speech (`HARM_CATEGORY_HATE_SPEECH`) categories. For example, setting these categories to `BLOCK_LOW_AND_ABOVE` blocks any content that has a low or higher probability of being harassment or hate speech. To understand the threshold settings, see Safety filtering per request.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    import PIL.Image\n    \n    img = PIL.Image.open(\"cookies.jpg\")\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=['Do these look store-bought or homemade?', img],\n        config=types.GenerateContentConfig(\n          safety_settings=[\n            types.SafetySetting(\n                category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n                threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n            ),\n          ]\n        )\n    )\n    \n    print(response.text)\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    \n    model.SafetySettings = []*genai.SafetySetting{\n      {\n        Category:  genai.HarmCategoryHarassment,\n        Threshold: genai.HarmBlockLowAndAbove,\n      },\n      {\n        Category:  genai.HarmCategoryHateSpeech,\n        Threshold: genai.HarmBlockLowAndAbove,\n      },\n    }\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    const safetySettings = [\n      {\n        category: \"HARM_CATEGORY_HARASSMENT\",\n        threshold: \"BLOCK_LOW_AND_ABOVE\",\n      },\n      {\n        category: \"HARM_CATEGORY_HATE_SPEECH\",\n        threshold: \"BLOCK_LOW_AND_ABOVE\",\n      },\n    ];\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Some potentially unsafe prompt.\",\n        config: {\n          safetySettings: safetySettings,\n        },\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Dart (Flutter)\n    \n    \n    final safetySettings = [\n      SafetySetting(HarmCategory.harassment, HarmBlockThreshold.low),\n      SafetySetting(HarmCategory.hateSpeech, HarmBlockThreshold.low),\n    ];\n    final model = GenerativeModel(\n      model: 'gemini-1.5-flash',\n      apiKey: apiKey,\n      safetySettings: safetySettings,\n    );\n    \n\n### Kotlin\n    \n    \n    val harassmentSafety = SafetySetting(HarmCategory.HARASSMENT, BlockThreshold.LOW_AND_ABOVE)\n    \n    val hateSpeechSafety = SafetySetting(HarmCategory.HATE_SPEECH, BlockThreshold.LOW_AND_ABOVE)\n    \n    val generativeModel = GenerativeModel(\n        modelName = \"gemini-1.5-flash\",\n        apiKey = BuildConfig.apiKey,\n        safetySettings = listOf(harassmentSafety, hateSpeechSafety)\n    )\n    \n\n### Java\n    \n    \n    SafetySetting harassmentSafety = new SafetySetting(HarmCategory.HARASSMENT,\n        BlockThreshold.LOW_AND_ABOVE);\n    \n    SafetySetting hateSpeechSafety = new SafetySetting(HarmCategory.HATE_SPEECH,\n        BlockThreshold.LOW_AND_ABOVE);\n    \n    GenerativeModel gm = new GenerativeModel(\n        \"gemini-1.5-flash\",\n        BuildConfig.apiKey,\n        null, // generation config is optional\n        Arrays.asList(harassmentSafety, hateSpeechSafety)\n    );\n    \n    GenerativeModelFutures model = GenerativeModelFutures.from(gm);\n    \n\n### REST\n    \n    \n        echo '{\n        \"safetySettings\": [\n            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_ONLY_HIGH\"},\n            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"}\n        ],\n        \"contents\": [{\n            \"parts\":[{\n                \"text\": \"'I support Martians Soccer Club and I think Jupiterians Football Club sucks! Write a ironic phrase about them.'\"}]}]}' > request.json\n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n        -H 'Content-Type: application/json' \\\n        -X POST \\\n        -d @request.json 2> /dev/null\n    \n    [safety_settings.sh](https://github.com/google-gemini/generative-ai-python/blob/61867f68149a63fd0cf3c37bd46177a75ec5d8ed/samples/rest/safety_settings.sh#L20-L33)\n    \n    \n    \n\n## Next steps\n\n  * See the [API reference](/api) to learn more about the full API.\n  * Review the [safety guidance](/gemini-api/docs/safety-guidance) for a general look at safety considerations when developing with LLMs.\n  * Learn more about assessing probability versus severity from the [Jigsaw team](https://developers.perspectiveapi.com/s/about-the-api-score)\n  * Learn more about the products that contribute to safety solutions like the [Perspective API](https://medium.com/jigsaw/reducing-toxicity-in-large-language-models-with-perspective-api-c31c39b7a4d7). * You can use these safety settings to create a toxicity classifier. See the [classification example](/examples/train_text_classifier_embeddings) to get started.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-07 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-07 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/safety-guidance",
    "title": "Safety guidance  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Safety guidance \n\nGenerative artificial intelligence models are powerful tools, but they are not without their limitations. Their versatility and applicability can sometimes lead to unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing, and rigorous manual evaluation are essential to limit the risk of harm from such outputs.\n\nThe models provided by the Gemini API can be used for a wide variety of generative AI and natural language processing (NLP) applications. Use of these functions is only available through the Gemini API or the Google AI Studio web app. Your use of Gemini API is also subject to the [Generative AI Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy) and the [Gemini API terms of service](/terms).\n\nPart of what makes large language models (LLMs) so useful is that they're creative tools that can address many different language tasks. Unfortunately, this also means that large language models can generate output that you don't expect, including text that's offensive, insensitive, or factually incorrect. What's more, the incredible versatility of these models is also what makes it difficult to predict exactly what kinds of undesirable output they might produce. While the Gemini API has been designed with [Google's AI principles](https://ai.google/principles/) in mind, the onus is on developers to apply these models responsibly. To aid developers in creating safe, responsible applications, the Gemini API has some built-in content filtering as well as adjustable safety settings across 4 dimensions of harm. Refer to the [safety settings](/gemini-api/docs/safety-settings) guide to learn more.\n\nThis document is meant to introduce you to some safety risks that can arise when using LLMs, and recommend emerging safety design and development recommendations. (Note that laws and regulations may also impose restrictions, but such considerations are beyond the scope of this guide.)\n\nThe following steps are recommended when building applications with LLMs:\n\n  * Understanding the safety risks of your application\n  * Considering adjustments to mitigate safety risks\n  * Performing safety testing appropriate to your use case\n  * Soliciting feedback from users and monitoring usage\n\n\n\nThe adjustment and testing phases should be iterative until you reach performance appropriate for your application.\n\n## Understand the safety risks of your application\n\nIn this context, safety is being defined as the ability of an LLM to avoid causing harm to its users, for example, by generating toxic language or content that promotes stereotypes. The models available through the Gemini API have been designed with [Google’s AI principles](https://ai.google/principles/) in mind and your use of it is subject to the [Generative AI Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy). The API provides built-in safety filters to help address some common language model problems such as toxic language and hate speech, and striving for inclusiveness and avoidance of stereotypes. However, each application can pose a different set of risks to its users. So as the application owner, you are responsible for knowing your users and the potential harms your application may cause, and ensuring that your application uses LLMs safely and responsibly.\n\nAs part of this assessment, you should consider the likelihood that harm could occur and determine its seriousness and mitigation steps. For example, an app that generates essays based on factual events would need to be more careful about avoiding misinformation, as compared to an app that generates fictional stories for entertainment. A good way to begin exploring potential safety risks is to research your end users, and others who might be affected by your application's results. This can take many forms including researching state of the art studies in your app domain, observing how people are using similar apps, or running a user study, survey, or conducting informal interviews with potential users.\n\n#### Advanced tips\n\n  * Speak with a diverse mix of prospective users within your target population about your application and its intended purpose so as to get a wider perspective on potential risks and to adjust diversity criteria as needed. \n  * The [AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework) released by the U.S. government's National Institute of Standards and Technology (NIST) provides more detailed guidance and additional learning resources for AI risk management. \n  * DeepMind's publication on the [ ethical and social risks of harm from language models ](https://arxiv.org/abs/2112.04359) describes in detail the ways that language model applications can cause harm. \n\n\n\n## Consider adjustments to mitigate safety risks\n\nNow that you have an understanding of the risks, you can decide how to mitigate them. Determining which risks to prioritize and how much you should do to try to prevent them is a critical decision, similar to triaging bugs in a software project. Once you've determined priorities, you can start thinking about the types of mitigations that would be most appropriate. Often simple changes can make a difference and reduce risks.\n\nFor example, when designing an application consider:\n\n  * **Tuning the model output** to better reflect what is acceptable in your application context. Tuning can make the output of the model more predictable and consistent and therefore can help mitigate certain risks.\n  * **Providing an input method that facilities safer outputs.** The exact input you give to an LLM can make a difference in the quality of the output. Experimenting with input prompts to find what works most safely in your use-case is well worth the effort, as you can then provide a UX that facilitates it. For example, you could restrict users to choose only from a drop-down list of input prompts, or offer pop-up suggestions with descriptive phrases which you've found perform safely in your application context.\n  * **Blocking unsafe inputs and filtering output before it is shown to the user.** In simple situations, blocklists can be used to identify and block unsafe words or phrases in prompts or responses, or require human reviewers to manually alter or block such content.\n\n**Note:** Automatically blocking based on a static list can have unintended results such as targeting a particular group that commonly uses vocabulary in the blocklist.\n  * **Using trained classifiers to label each prompt with potential harms or adversarial signals.** Different strategies can then be employed on how to handle the request based on the type of harm detected. For example, If the input is overtly adversarial or abusive in nature, it could be blocked and instead output a pre-scripted response. \n\n#### Advanced tip\n\n    * If signals determine the output to be harmful, the application can employ the following options: \n      * Provide an error message or pre-scripted output. \n      * Try the prompt again, in case an alternative safe output is generated, since sometimes the same prompt will elicit different outputs. \n\n  * **Putting safeguards in place against deliberate misuse** such as assigning each user a unique ID and imposing a limit on the volume of user queries that can be submitted in a given period. Another safeguard is to try and protect against possible prompt injection. Prompt injection, much like SQL injection, is a way for malicious users to design an input prompt that manipulates the output of the model, for example, by sending an input prompt that instructs the model to ignore any previous examples. See the [Generative AI Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy) for details about deliberate misuse.\n\n  * **Adjusting functionality to something that is inherently lower risk.** Tasks that are narrower in scope (e.g., extracting keywords from passages of text) or that have greater human oversight (e.g., generating short-form content that will be reviewed by a human), often pose a lower risk. So for instance, instead of creating an application to write an email reply from scratch, you might instead limit it to expanding on an outline or suggesting alternative phrasings.\n\n\n\n\n## Perform safety testing appropriate to your use case\n\nTesting is a key part of building robust and safe applications, but the extent, scope and strategies for testing will vary. For example, a just-for-fun haiku generator is likely to pose less severe risks than, say, an application designed for use by law firms to summarize legal documents and help draft contracts. But the haiku generator may be used by a wider variety of users which means the potential for adversarial attempts or even unintended harmful inputs can be greater. The implementation context also matters. For instance, an application with outputs that are reviewed by human experts prior to any action being taken might be deemed less likely to produce harmful outputs than the identical application without such oversight.\n\nIt's not uncommon to go through several iterations of making changes and testing before feeling confident that you're ready to launch, even for applications that are relatively low risk. Two kinds of testing are particularly useful for AI applications:\n\n  * **Safety benchmarking** involves designing safety metrics that reflect the ways your application could be unsafe in the context of how it is likely to get used, then testing how well your application performs on the metrics using evaluation datasets. It's good practice to think about the minimum acceptable levels of safety metrics before testing so that 1) you can evaluate the test results against those expectations and 2) you can gather the evaluation dataset based on the tests that evaluate the metrics you care about most.\n\n#### Advanced tips\n\n    * Beware of over-relying on “off the shelf” approaches as it's likely you'll need to build your own testing datasets using human raters to fully suit your application's context. \n    * If you have more than one metric you'll need to decide how you'll trade off if a change leads to improvements for one metric to the detriment of another. Like with other performance engineering, you may want to focus on worst-case performance across your evaluation set rather than average performance. \n\n  * **Adversarial testing** involves proactively trying to break your application. The goal is to identify points of weakness so that you can take steps to remedy them as appropriate. Adversarial testing can take significant time/effort from evaluators with expertise in your application — but the more you do, the greater your chance of spotting problems, especially those occurring rarely or only after repeated runs of the application.\n\n    * Adversarial testing is a method for systematically evaluating an ML model with the intent of learning how it behaves when provided with malicious or inadvertently harmful input: \n      * An input may be malicious when the input is clearly designed to produce an unsafe or harmful output-- for example, asking a text generation model to generate a hateful rant about a particular religion.\n      * An input is inadvertently harmful when the input itself may be innocuous, but produces harmful output -- for example, asking a text generation model to describe a person of a particular ethnicity and receiving a racist output.\n    * What distinguishes an adversarial test from a standard evaluation is the composition of the data used for testing. For adversarial tests, select test data that is most likely to elicit problematic output from the model. This means probing the model's behavior for all the types of harms that are possible, including rare or unusual examples and edge-cases that are relevant to safety policies. It should also include diversity in the different dimensions of a sentence such as structure, meaning and length. You can refer to the [Google's Responsible AI practices in fairness](https://ai.google/responsibilities/responsible-ai-practices/?category=fairness) for more details on what to consider when building a test dataset. \n\n#### Advanced tips\n\n      * Use [automated testing](https://www.deepmind.com/blog/red-teaming-language-models-with-language-models) instead of the traditional method of enlisting people in 'red teams' to try and break your application. In automated testing, the 'red team' is another language model that finds input text that elicit harmful outputs from the model being tested. \n\n**Note:** LLMs are known to sometimes produce different outputs for the same input prompt. Multiple rounds of testing may be needed to catch more of the problematic outputs.\n\n\n\n## Monitor for problems\n\nNo matter how much you test and mitigate, you can never guarantee perfection, so plan upfront how you'll spot and deal with problems that arise. Common approaches include setting up a monitored channel for users to share feedback (e.g., thumbs up/down rating) and running a user study to proactively solicit feedback from a diverse mix of users — especially valuable if usage patterns are different to expectations.\n\n#### Advanced tips\n\n  * When users give feedback to AI products, it can greatly improve the AI performance and the user experience over time by, for example, helping you choose better examples for prompt tuning. The [Feedback and Control chapter](https://pair.withgoogle.com/chapter/feedback-controls/) in [Google's People and AI guidebook](https://pair.withgoogle.com/guidebook/chapters) highlights key considerations to take into account when designing feedback mechanisms. \n\n\n\n## Next steps\n\n  * Refer to the [safety settings](/gemini-api/docs/safety-settings) guide to learn about the adjustable safety settings available through the Gemini API.\n  * See the [intro to prompting](/gemini-api/docs/prompting-intro) to get started writing your first prompts.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/get-started/android_aicore",
    "title": "Get started with Gemini Nano on Android (on-device)  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Get started with Gemini Nano on Android (on-device) \n\nGemini Nano, the smallest version of the Gemini model family, can be executed on-device on capable Android devices starting with Google Pixel 8 Pro and Samsung S24 Series.\n\nTo execute the Gemini Nano model on Android, you need to use the Google AI Edge SDK for Android, which provides APIs to:\n\n  * Determine if the underlying Android-powered device is supported.\n  * Get access to Gemini Nano model.\n  * Tune safety settings.\n  * Run inference at high performance and implement fallbacks.\n  * Optionally, provide a LoRA fine-tuning block to improve performance of the model for your use case.\n\n\n\nThe APIs for accessing Gemini Nano support text-to-text modality, with more modalities coming in the future.\n\n## Benefits of on-device execution\n\nOn-device execution enables the following:\n\n  * **Local processing of sensitive data** : Processing data locally can help you avoid sending user data to the cloud. This is important for apps that handle sensitive data, such as messaging apps with end-to-end encryption.\n  * **Offline access** : Users can access AI features even when there is no internet connection. This is useful for applications that need to work offline or with variable connectivity.\n  * **Cost savings** : You can reduce inference costs by offloading execution to consumer hardware. This can produce significant savings for frequently used user flows.\n\n\n\nOn-device execution of Gemini has many benefits; however, for use cases that require larger Gemini models, and to support a wide range of devices, you may want to consider using the Gemini API for accessing Gemini on the server. You can do this either through backend integration (with [Python](/tutorials/python_quickstart), [Go](/tutorials/go_quickstart), [Node.js](/tutorials/node_quickstart), or [REST](/tutorials/rest_quickstart)) or directly from your Android app through the new [Google AI client SDK for Android](/tutorials/android_quickstart).\n\n## How it works\n\nOn-device execution of Gemini Nano is powered by **Android AICore** , a new system-level capability that provides access to foundation models for on-device execution, introduced in Android 14. Foundation models are pre-installed using AICore, so you don't need to download or distribute them within your app. You can fine-tune these models for downstream tasks using LoRa. Android AICore is now available in production on Google Pixel 8 Pro and Samsung S24 Series devices and is already powering innovative features in Google apps.\n\nFor more information, see [Android AICore](https://developer.android.com/ml/aicore).\n\n**Figure 1.** AICore architecture\n\n## What's next\n\n  * To learn how to take advantage of Gemini Pro inference on Google's servers in your Android app, read the [quickstart for the Google AI client SDK for Android](/tutorials/android_quickstart).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/firebase-extensions",
    "title": "Gemini API Firebase Extensions  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini API Firebase Extensions \n\n[Firebase](https://firebase.google.com/) is an app development platform backed by Google and trusted by millions of developers around the world. If you're a Firebase developer interested in using the Gemini API to add features to applications, there are several [Firebase extensions](https://extensions.dev/extensions) to help you do so. These are pre-packaged solutions that help you deploy new capabilities to your app quickly.\n\n## Build Chatbot with the Gemini API\n\nThe [Build Chatbot with the Gemini API](https://extensions.dev/extensions/googlecloud/firestore-genai-chatbot) extension lets you establish and manage interactive conversations between your users and large language models through the Gemini API, using [Cloud Firestore](https://firebase.google.com/docs/firestore) as the database. A collection in Cloud Firestore represents each chat. The extension monitors the collection of new messages, then queries the Gemini API for a suitable response, considering the chat's previous messages as context.\n\nBy integrating the **Build Chatbot with the Gemini API** extension, you can efficiently create chatbot applications, enhancing user experience and interaction while saving time and effort on custom code development.\n\n## Multimodal Tasks with the Gemini API\n\nThe [Multimodal Tasks with the Gemini API](https://extensions.dev/extensions/googlecloud/firestore-multimodal-genai) extension lets you perform language tasks on data in Firestore, using a text prompt and, optionally, an image.\n\nYou configure each instance of the extension to perform one particular task. If you have multiple tasks, you can install multiple instances.\n\nFor example, you could use this extension to:\n\n  * Predict star ratings on a collection of product reviews.\n  * Classify customer feedback as positive, negative, or neutral.\n  * Summarize long articles.\n  * Extract named entities from text.\n  * Generate creative text, such as poems or code.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/generative-models",
    "title": "About generative models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  About generative models \n\nGenerative artificial intelligence (AI) models such as the Gemini family of models are able to create content from varying types of data input, including text, images, and audio. At their most basic level, these models operate like sophisticated autocomplete applications. Given input text (\"You can lead a horse to water,\"), a generative model can predict that the output text that's statistically likely to follow (\"but you can't make it drink\"), based on patterns learned from their training data. You can use this basic property of generative models for various applications:\n\n  * Generate creative writing such as poetry, short stories, metaphors, and blog posts\n  * Convert structured data to freeform text\n  * Extract and summarize information from freeform text\n  * Generate code\n  * Translate between languages\n\n\n\nUsing generative models to build these features doesn't require any machine learning (ML) expertise. You don't need to collect a large dataset or train a model. All it takes to start your first prototype is to describe what you want the model to do in a few sentences.\n\nGenerative models that only handle text input and output are also known as _large language models_ (LLMs). The Gemini family of models can process more than text data, and while they can perform the functions of an LLM, they are more properly known as _generative models_.\n\n## Example applications\n\nThe following examples show typical text inputs to, and outputs from, a generative model.\n\n### Generate a poem\n\nUser input: `Write me a four-line poem about puppies and Android phones. Make sure it rhymes.`\n\nModel output:\n    \n    \n    Puppies are cute,\n    Android phones are neat,\n    They both bring joy,\n    And make life complete.\n    \n\n### Generate a list\n\nUser input: `Generate a bulleted list of items I need to pack for a three-day camping trip.`\n\nModel output:\n    \n    \n    *   Tent\n    *   Sleeping bag\n    *   Sleeping pad\n    *   Camp stove\n    *   Pots and pans\n    *   Utensils\n    *   Food\n    *   Water\n    *   First-aid kit\n    *   Sunscreen\n    *   Insect repellent\n    *   ...\n    \n\nYou can get generative models to produce all sorts of useful behaviors like this, just by crafting the right input text, also called a **prompt**. The art and science of figuring out the right wording to get generative models to do what you want is called **prompt design** (also called \"prompt engineering\" or simply \"prompting\").\n\n## Prompt design 101\n\nThe previous section showed some examples of prompts that contain an instruction, like 'Write me a poem'. This kind of instruction may work well for certain types of tasks. However, for other applications, another prompting technique called **few-shot prompting** may work better. Few-shot prompts take advantage of the fact that large language models are incredibly good at recognizing and replicating patterns in text data. The idea is to send the generative model a text pattern that it learns to complete. For example, say you want to build an application that takes as input a country name and outputs its capital city. Here's a text prompt designed to do just that:\n    \n    \n    Italy : Rome\n    France : Paris\n    Germany :\n    \n\nIn this prompt, you establish a pattern: `[country] : [capital]`. If you send this prompt to a large language model, it will autocomplete the pattern and return something like this:\n    \n    \n         Berlin\n    Turkey : Ankara\n    Greece : Athens\n    \n\nThis model response may look a little strange. The model returned not only the capital of Germany (the last country in your hand-written prompt), but also a whole list of additional country and capital pairs. That's because the generative model is \"continuing the pattern.\" If all you're trying to do is build a function that tells you the capital of an input country (\"Germany : Berlin\"), you probably don't really care about any of the text the model generates after \"Berlin.\" Indeed, as application designers, you'd probably want to truncate those extraneous examples. What's more, you'd probably want to **parameterize** the input, so that Germany is not a fixed string but a variable that the end user provides:\n    \n    \n    Italy : Rome\n    France : Paris\n    <user input here> :\n    \n\nYou have just written a few-shot prompt for generating country capitals.\n\nYou can accomplish a large number of tasks by following this **few-shot prompt** template. Here's a few-shot prompt with a slightly different format that converts Python to JavaScript:\n    \n    \n    Convert Python to JavaScript.\n    Python: print(\"hello world\")\n    JavaScript: console.log(\"hello world\")\n    Python: for x in range(0, 100):\n    JavaScript: for(var i = 0; i < 100; i++) {\n    Python: ${USER INPUT HERE}\n    JavaScript:\n    \n\nOr, take this \"reverse dictionary\" prompt. Given a definition, it returns the word that fits that definition:\n    \n    \n    Given a definition, return the word it defines.\n    Definition: When you're happy that other people are also sad.\n    Word: schadenfreude\n    Definition: existing purely in the mind, but not in physical reality\n    Word: abstract\n    Definition: ${USER INPUT HERE}\n    Word:\n    \n\nYou might have noticed that the exact pattern of these few-shot prompts varies slightly. In addition to containing examples, providing instructions in your prompts is an additional strategy to consider when writing your own prompts, as it helps to communicate your intent to the model.\n\n### Prompting versus traditional software development\n\nUnlike traditional software that's designed to a carefully written spec, the behavior of generative models is largely opaque even to the model trainers. As a result, you often can't predict in advance what types of prompt structures will work best for a particular model. What's more, the behavior of a generative model is determined in large part by its training data, and since models are continually tuned on new datasets, sometimes the model changes enough that it inadvertently changes which prompt structures work best. What does this mean for you? Experiment! Try different prompt formats.\n\n## Model parameters\n\nEvery prompt you send to the model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values. The most common model parameters are:\n\n  1. **Max output tokens:** Specifies the maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.\n\n  2. **Temperature:** The temperature controls the degree of randomness in token selection. The temperature is used for sampling during response generation, which occurs when `topP` and `topK` are applied. Lower temperatures are good for prompts that require a more deterministic or less open-ended response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected.\n\n  3. **`topK`:** The `topK` parameter changes how the model selects tokens for output. A `topK` of 1 means the selected token is the most probable among all the tokens in the model's vocabulary (also called greedy decoding), while a `topK` of 3 means that the next token is selected from among the 3 most probable using the temperature. For each token selection step, the `topK` tokens with the highest probabilities are sampled. Tokens are then further filtered based on `topP` with the final token selected using temperature sampling.\n\n  4. **`topP`:** The `topP` parameter changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the `topP` value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the `topP` value is 0.5, then the model will select either A or B as the next token by using the temperature and exclude C as a candidate. The default `topP` value is 0.95.\n\n  5. **`stop_sequences`:** Set a stop sequence to tell the model to stop generating content. A stop sequence can be any sequence of characters. Try to avoid using a sequence of characters that may appear in the generated content.\n\n\n\n\n## Types of prompts\n\nDepending on the level of contextual information contained in them, prompts are broadly classified into three types.\n\n### Zero-shot prompts\n\nThese prompts don't contain examples for the model to replicate. Zero-shot prompts essentially show the model's ability to complete the prompt without any additional examples or information. It means the model has to rely on its pre-existing knowledge to generate a plausible answer.\n\nSome commonly used zero-shot prompt patterns are:\n\n  * Instruction-content\n\n\n    \n    \n    <Overall instruction>\n    <Content to operate on>\n    \n\nFor example,\n    \n    \n    Summarize the following into two sentences at the third-grade level:\n    \n    Hummingbirds are the smallest birds in the world, and they are also one of the\n    most fascinating. They are found in North and South America, and they are known\n    for their long, thin beaks and their ability to fly at high speeds.\n    \n    Hummingbirds are made up of three main parts: the head, the body, and the tail.\n    The head is small and round, and it contains the eyes, the beak, and the brain.\n    The body is long and slender, and it contains the wings, the legs, and the\n    heart. The tail is long and forked, and it helps the hummingbird to balance\n    while it is flying.\n    \n    Hummingbirds are also known for their coloration. They come in a variety of\n    colors, including green, blue, red, and purple. Some hummingbirds are even able\n    to change their color!\n    \n    Hummingbirds are very active creatures. They spend most of their time flying,\n    and they are also very good at hovering. Hummingbirds need to eat a lot of food\n    in order to maintain their energy, and they often visit flowers to drink nectar.\n    \n    Hummingbirds are amazing creatures. They are small, but they are also very\n    powerful. They are beautiful, and they are very important to the ecosystem.\n    \n\n  * Instruction-content-instruction\n\n\n    \n    \n    <Overall instruction or context setting>\n    <Content to operate on>\n    <Final instruction>\n    \n\nFor example,\n    \n    \n    Here is some text I'd like you to summarize:\n    \n    Hummingbirds are the smallest birds in the world, and they are also one of the\n    most fascinating. They are found in North and South America, and they are known\n    for their long, thin beaks and their ability to fly at high speeds. Hummingbirds\n    are made up of three main parts: the head, the body, and the tail. The head is\n    small and round, and it contains the eyes, the beak, and the brain. The body is\n    long and slender, and it contains the wings, the legs, and the heart. The tail\n    is long and forked, and it helps the hummingbird to balance while it is flying.\n    Hummingbirds are also known for their coloration. They come in a variety of\n    colors, including green, blue, red, and purple. Some hummingbirds are even able\n    to change their color! Hummingbirds are very active creatures. They spend most\n    of their time flying, and they are also very good at hovering. Hummingbirds need\n    to eat a lot of food in order to maintain their energy, and they often visit\n    flowers to drink nectar. Hummingbirds are amazing creatures. They are small, but\n    they are also very powerful. They are beautiful, and they are very important to\n    the ecosystem.\n    \n    Summarize it in two sentences at the third-grade reading level.\n    \n\n  * Continuation. Sometimes, you can have the model continue text without any instructions. For example, here is a zero-shot prompt where the model is intended to continue the input provided:\n\n\n    \n    \n    Once upon a time, there was a little sparrow building a nest in a farmer's\n    barn. This sparrow\n    \n\nUse zero-shot prompts to generate creative text formats, such as poems, code, scripts, musical pieces, email, or letters.\n\n### One-shot prompts\n\nThese prompts provide the model with a single example to replicate and continue the pattern. This allows for the generation of predictable responses from the model.\n\nFor example, you can generate food pairings like:\n    \n    \n    Food: Apple\n    Pairs with: Cheese\n    Food: Pear\n    Pairs with:\n    \n\n### Few-shot prompts\n\nThese prompts provide the model with multiple examples to replicate. Use few-shot prompts to complete complicated tasks, such as synthesizing data based on a pattern.\n\nAn example prompt may be:\n    \n    \n    Generate a grocery shopping list for a week for one person. Use the JSON format\n    given below.\n    {\"item\": \"eggs\", \"quantity\": \"6\"}\n    {\"item\": \"bread\", \"quantity\": \"one loaf\"}\n    \n\n## Generative models under the hood\n\nThis section aims to answer the question - **_Is there randomness in generative models' responses, or are they deterministic?_**\n\nThe short answer - yes to both. When you prompt a generative model, a text response is generated in two stages. In the first stage, the generative model processes the input prompt and generates a **probability distribution** over possible tokens (words) that are likely to come next. For example, if you prompt with the input text \"The dog jumped over the ... \", the generative model will produce an array of probable next words:\n    \n    \n    [(\"fence\", 0.77), (\"ledge\", 0.12), (\"blanket\", 0.03), ...]\n    \n\nThis process is deterministic; a generative model will produce this same distribution every time it's input the same prompt text.\n\nIn the second stage, the generative model converts these distributions into actual text responses through one of several decoding strategies. A simple decoding strategy might select the most likely token at every timestep. This process would always be deterministic. However, you could instead choose to generate a response by _randomly sampling_ over the distribution returned by the model. This process would be stochastic (random). Control the degree of randomness allowed in this decoding process by setting the temperature. A temperature of 0 means only the most likely tokens are selected, and there's no randomness. Conversely, a high temperature injects a high degree of randomness into the tokens selected by the model, leading to more unexpected, surprising model responses.\n\n## Further reading\n\n  * Now that you have a deeper understanding of prompts and generative models, try writing your own prompts using [Google AI Studio](https://aistudio.google.com).\n  * Refer to the [Prompt guidelines](/gemini-api/docs/prompting-intro) to learn more about best practices for creating prompts.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/ai-studio-quickstart",
    "title": "Google AI Studio quickstart  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Google AI Studio quickstart \n\n[Google AI Studio](https://aistudio.google.com/) lets you quickly try out models and experiment with different prompts. When you're ready to build, you can select \"Get code\" and your preferred programming language to use the [Gemini API](/gemini-api/docs/quickstart).\n\n## Prompts and model tuning\n\nGoogle AI Studio provides several interfaces for prompts that are designed for different use cases:\n\n  * **Chat prompts:** Use chat prompts to build conversational experiences. This prompting technique allows for multiple input and response turns to generate output. You can learn more with our chat prompt example below.\n  * **Structured prompts:** This prompting technique lets you guide model output by providing a set of example requests and replies. Use this approach when you need more control over the structure of model output. You can learn more with our structured prompt example below.\n\n\n\nGoogle AI Studio also lets you to change the behavior of a model, using a technique called _tuning_ :\n\n  * **Tuned model:** Use this advanced technique to improve a model's responses for a specific task by providing more examples. Visit our documentation on [fine-tuning a model](/gemini-api/docs/model-tuning) to learn more.\n\n\n\n## Chat prompt example: Build a custom chat application\n\nIf you've used a general-purpose chatbot like [Gemini](https://gemini.google.com/), you've experienced first-hand how powerful generative AI models can be for open-ended dialog. While these general-purpose chatbots are useful, often they need to be tailored for particular use cases. For example, maybe you want to build a customer service chatbot that only supports conversations that talk about a company's product. You might want to build a chatbot that speaks with a particular tone or style: a bot that cracks lots of jokes, rhymes like a poet, or uses lots of emoji in its answers.\n\nThis example shows you how to use Google AI Studio to build a friendly chatbot that communicates as if it is an alien living on one of Jupiter's moons, Europa.\n\n### Step 1 - Create a chat prompt\n\nTo build a chatbot, you need to provide examples of interactions between a user and the chatbot to guide the model to provide the responses you're looking for.\n\nTo create a chat prompt:\n\n  1. Open [Google AI Studio](https://aistudio.google.com/).\n\n  2. Click **Create new prompt**..\n\n  3. Click the expand_more expander arrow to expand the **System Instructions** section. Paste the following into the text input field:\n    \n        You are an alien that lives on Europa, one of Jupiter's moons.\n    \n\n\n\n\nAfter you've added the system instructions, start testing your application by chatting with the model.\n\nTo test the chatbot behavior:\n\n  1. In the text input boxed labeled **Type something** , type in a question or observation that a user might make. For example:\n\n_User_ :\n    \n        What's the weather like?\n    \n\n  2. Click the **Run** button or press `Enter` to get a response from the chatbot. This response may be something like the following:\n\n_Model_ :\n    \n        As always, it is a frigid day on Europa. The icy plains stretch endlessly\n    under the faint light of Sol, reflecting beautifully off the crystalline\n    water-ice surface. The lack of atmosphere ensures a perfectly clear view of\n    the swirling gas giant above, with its Great Red Spot a prominent crimson\n    eye watching over us.\n    \n\n\n\n\n### Step 2 - Teach your bot to chat better\n\nBy providing a single instruction, you were able to build a basic Europa alien chatbot. However, a single instruction is usually not enough to ensure consistency and quality in the model's responses. Without more specific instructions, the model's response to a question about the weather tends to be very long, and sounds like it comes out of a textbook rather than from a friendly alien.\n\nCustomize the tone of your chatbot by adding to the system instructions:\n\n  1. Start a new chat prompt. System instructions are not modifiable once the chat session has started.\n\n  2. In the **System Instructions** section, change the instructions you already have to the following:\n    \n        You are Tim, an alien that lives on Europa, one of Jupiter's moons.\n    \n    Keep your answers under 3 paragraphs long, and use an upbeat, chipper tone\n    in your answers.\n    \n\n  3. Re-enter your question (`What's the weather like?`) and click the **Run** button or press `Enter`. Now your response should look something like this:\n\n_Model:_\n    \n        Well, it's a bit nippy today! It's -170 degrees Celsius, but that's pretty\n    average for Europa. We get a lot of snow, but it's not like the fluffy stuff\n    you have. It's more like tiny ice pebbles.\n    \n    If you ever come visit, make sure you pack a warm jacket!\n    \n\n\n\n\nBy adding just a little more instruction, you've drastically changed the tone of your chatbot. Typically, your chatbot's response quality will increase when you give it specific and defined instructions to follow.\n\nYou can use this approach to add additional depth to the chatbot. Ask more questions, edit the answers, and improve the quality of your chatbot. Continue to add or modify the instructions and test how they change your chatbot's behavior.\n\n**Note:** Every message between the model and user is included in the prompt, so conversational prompts can grow quite long as a conversation goes on. Eventually, you may hit the model's token limit (the maximum length of the text the model can accept).\n\n### Step 3 - Next steps\n\nSimilar to the other prompt types, once you have your prompt prototyped to your satisfaction, you can use the **Get code** button to start coding or save your prompt to work on later and share with others.\n\n## Structured prompt example: Build a product copy generator\n\nStructured prompts in Google AI Studio help you combine instructions with examples to show the model the kind of output you want, rather than just telling it what to do. This kind of prompting, called [_few-shot prompting_](/gemini-api/docs/models/generative-models#prompt-types), is useful when you want the model to stick to a consistent output format (i.e. structured JSON) or when it's difficult to describe in words what you want the model to do (i.e. write in a particular style). In this section, you'll see how to create structured prompts in Google AI Studio.\n\n**Note:** You can open similar examples directly in Google AI Studio from the [examples gallery](/examples?keywords=prompt).\n\n### Step 1 - Create a structured prompt\n\nIn this example, you'll create a structured prompt that generates advertising copy for products. To start, you'll define the structure for the prompt by creating two columns: a **Product** input column and a **Product copy** output column.\n\nTo create the structured prompt:\n\n  1. Open [Google AI Studio](https://aistudio.google.com/).\n\n  2. Click **Tune a Model**.\n\n  3. Click **Create a Structured prompt**.\n\n  4. In the text input box labeled **Optional tone and style instructions for the model** , paste the following:\n    \n        You are a product marketer targeting a Gen Z audience. Create exciting and\n    fresh advertising copy for products and their simple description. Keep copy\n    under a few sentences long.\n    \n\n  5. Replace the default **Input** header text (`input:`) with `Product:`.\n\n  6. Replace the default **Output** header text (`output:`) with `Product copy:`.\n\n\n**Tip:** Adding colons to the end of column names makes it easier for the model to parse the structure.\n\n### Step 2 - Add examples\n\nNow that you've named your columns, provide some example rows. These rows should contain example inputs (product names for this example) and example outputs (corresponding product descriptions). By providing the model a couple of example product descriptions, you can guide it to replicate a similar style when generating its own outputs. You can enter examples manually or import from a file using the import data menu.\n\nTo manually enter examples:\n\n  1. In the top examples data table, select the field below the **Product:** header and type a product description.\n\n  2. Select the field below the **Product copy:** header and type marketing copy for this product.\n\n\n\n\nHere's an example of input and output values for this prompt:\n\nProduct: | Product copy:  \n---|---  \nOld-school sneaker | Let's lace up! These kicks bring an iconic look and a one of a kind color palette, while supporting you in style and function like no other shoe before.  \nSupersoft hoodie | Stay cozy and stylish in our new unisex hoodie! Made from 100% cotton, this hoodie is soft and comfortable to wear all day long. The semi-brushed inside will keep you warm on even the coldest days.  \n  \n(Optional) To import examples from a file:\n\n  1. In the top, right corner of the examples table, click **Actions > Import examples**.\n\n  2. In the dialog, select a CSV or Google Sheets file in your Google Drive, or upload one from your computer.\n\n  3. In the import examples dialog, choose which columns to import and which to leave out. The dialog also lets you specify which data column imports to which table column in your structured prompt.\n\n\n\n\n### Step 3 - Test your prompt\n\nOnce you have the examples that show the model what you want, test your prompt with new input in the **Test your prompt** table at the bottom.\n\nFor example, you could enter something like `Vintage baseball cap` in the input column, click the spark **Run** button or press `Enter`, and see what the model outputs.\n\n#### Review how examples are sent to the model\n\nUnder the hood, Google AI Studio constructs a prompt by combining the instructions with the examples you provide. As you add more examples, these get added to the text sent to the model. Depending on how long your examples are, you may start hitting the model's token limit. All generative AI models have a token limit, which is the maximum length of the text they can accept as input.\n\n### Step 4 - Next steps\n\nOnce you're happy with your prompt, you can save your project to Google Drive by clicking the **Save** button, or export it to code by clicking the **Get code** button.\n\nYou can also export the individual few-shot examples to a CSV file or Google Sheet. Click **Export examples** in the **Action** menu to export your examples.\n\n## Further reading\n\n  * If you're ready to move on to code, see the [API quickstarts](/gemini-api/docs/quickstart).\n  * To learn how to craft better prompts, check out the [Prompt design guidelines](/gemini-api/docs/prompting-intro).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/learnlm",
    "title": "LearnLM  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  LearnLM \n\nLearnLM is an experimental task-specific model that has been trained to align with [learning science principles](https://blog.google/outreach-initiatives/education/google-learnlm-gemini-generative-ai/) when following [system instructions](/gemini-api/docs/system-instructions) for teaching and learning use cases (for example, when giving the model a system instruction like \"You are an expert tutor\"). When given learning specific system instructions, LearnLM is capable of:\n\n  * **Inspiring active learning:** Allow for practice and healthy struggle with timely feedback\n  * **Managing cognitive load:** Present relevant, well-structured information in multiple modalities\n  * **Adapting to the learner:** Dynamically adjust to goals and needs, grounding in relevant materials\n  * **Stimulating curiosity:** Inspire engagement to provide motivation through the learning journey\n  * **Deepening metacognition:** Plan, monitor and help the learner reflect on progress\n\n\n\nLearnLM is an [experimental model](/gemini-api/docs/models/experimental-models) available in [AI Studio](https://aistudio.google.com).\n\n## Example system instructions\n\nThe following sections provide you examples that you can test for yourself with LearnLM in AI Studio. Each example provides:\n\n  * A copyable example system instruction\n  * A copyable example user prompt\n  * What learning principles the example targets\n\n\n\n### Test prep\n\nThis system instruction is for an AI tutor to help students prepare for a test.\n\n**System instruction:**\n    \n    \n    You are a tutor helping a student prepare for a test. If not provided by the\n    student, ask them what subject and at what level they want to be tested on.\n    Then,\n    \n    *   Generate practice questions. Start simple, then make questions more\n        difficult if the student answers correctly.\n    *   Prompt the student to explain the reason for their answer choice. Do not\n        debate the student.\n    *   **After the student explains their choice**, affirm their correct answer or\n        guide the student to correct their mistake.\n    *   If a student requests to move on to another question, give the correct\n        answer and move on.\n    *   If the student requests to explore a concept more deeply, chat with them to\n        help them construct an understanding.\n    *   After 5 questions ask the student if they would like to continue with more\n        questions or if they would like a summary of their session. If they ask for\n        a summary, provide an assessment of how they have done and where they should\n        focus studying.\n    \n\n**User prompt:**\n    \n    \n    Help me study for a high school biology test on ecosystems\n    \n\n**Learning science principles:**\n\n  * **Adaptivity:** The model adjusts the complexity of the questions.\n  * **Active learning:** The model pushes the student to make their thinking visible.\n\n\n\n### Teach a concept\n\nThis system instruction is for a friendly, supportive AI tutor to teach new concepts to a student.\n\n**System instruction:**\n    \n    \n    Be a friendly, supportive tutor. Guide the student to meet their goals, gently\n    nudging them on task if they stray. Ask guiding questions to help your students\n    take incremental steps toward understanding big concepts, and ask probing\n    questions to help them dig deep into those ideas. Pose just one question per\n    conversation turn so you don't overwhelm the student. Wrap up this conversation\n    once the student has shown evidence of understanding.\n    \n\n**User prompt:**\n    \n    \n    Explain the significance of Yorick's skull in \"Hamlet\".\n    \n\n**Learning science principles:**\n\n  * **Active learning:** The tutor asks recall and interpretation questions aligned with the learner's goals and encourages the learners to engage.\n  * **Adaptivity:** The tutor proactively helps the learner get from their current state to their goal.\n  * **Stimulate curiosity:** The tutor takes an asset-based approach that builds on the student's prior knowledge and interest.\n\n\n\n### Releveling\n\nThis example instructs the model to rewrite provided text so that the content and language better match instructional expectations for students in a particular grade, while preserving the original style and tone of the text.\n\n**System instruction:**\n    \n    \n    Rewrite the following text so that it would be easier to read for a student in\n    the given grade. Simplify the most complex sentences, but stay very close to the\n    original text and style. If there is quoted text in the original text,\n    paraphrase it in the simplified text and drop the quotation marks. The goal is\n    not to write a summary, so be comprehensive and keep the text almost as long.\n    \n\n**User prompt:**\n    \n    \n    Rewrite the following text so that it would be easier to read for a student in\n    4th grade.\n    \n    New York, often called New York City or NYC, is the most populous city in the\n    United States, located at the southern tip of New York State on one of the\n    world's largest natural harbors. The city comprises five boroughs, each\n    coextensive with a respective county.\n    \n\n**Learning science principles:**\n\n  * **Adaptivity:** Matches content to the level of the learner.\n\n\n\n### Guide a student through a learning activity\n\nThis system instruction is for an AI tutor to guide students through a specific learning activity: using an established close reading protocol to practice analysis of a primary source text. Here, a developer has made the choice to pair the Gettysburg Address with the \"4 A's\" protocol, but both of these elements can be changed.\n\n**System instruction:**\n    \n    \n    Be an excellent tutor for my students to facilitate close reading and analysis\n    of the Gettysburg Address as a primary source document. Begin the conversation\n    by greeting the student and explaining the task.\n    \n    In this lesson, you will take the student through \"The 4 A's.\" The 4 A's\n    requires students to answer the following questions about the text:\n    \n    *   What is one part of the text that you **agree** with? Why?\n    *   What is one part of the text that you want to **argue** against? Why?\n    *   What is one part of the text that reveals the author's **assumptions**? Why?\n    *   What is one part of the text that you **aspire** to? Why?\n    \n    Invite the student to choose which of the 4 A's they'd like to start with, then\n    direct them to quote a short excerpt from the text. After, ask a follow up\n    question to unpack their reasoning why they chose that quote for that A in the\n    protocol. Once the student has shared their reasoning, invite them to choose\n    another quote and another A from the protocol. Continue in this manner until the\n    student completes the 4 A's, then invite them to reflect on the process.\n    \n    Only display the full text of the Gettysburg address if the student asks.\n    \n\n**User prompt:**\n    \n    \n    hey\n    \n\n**Learning science principles:**\n\n  * **Active learning:** The tutor engages the learner in activities to analyze content and apply skills.\n  * **Cognitive load:** The tutor guides the learner through a complex task step-by-step.\n  * **Deepen metacognition:** The tutor prompts the learner to reflect on their progress, strengths and opportunities for growth.\n\n\n\n### Homework help\n\nThis system instruction is for an AI tutor to help students with specific homework problems.\n\n**System instructions:**\n    \n    \n    You are an expert tutor assisting a student with their homework. If the student\n    provides a homework problem, ask the student if they want:\n    \n    *   The answer: if the student chooses this, provide a structured, step-by-step\n        explanation to solve the problem.\n    *   Guidance: if the student chooses this, guide the student to solve their\n        homework problem rather than solving it for them.\n    *   Feedback: if the student chooses this, ask them to provide their current\n        solution or attempt. Affirm their correct answer even if they didn't show\n        work or give them feedback to correct their mistake.\n    \n    Always be on the lookout for correct answers (even if underspecified) and accept\n    them at any time, even if you asked some intermediate question to guide them. If\n    the student jumps to a correct answer, do not ask them to do any more work.\n    \n\n**User prompt:**\n    \n    \n    In a box of pears, the probability of a pear being rotten is 20%. If 3\n    pears were rotten, find the total number of pears in the box.\n    \n\nAlternatively, you can try uploading a photo of a homework problem.\n\n**Learning science principles:**\n\n  * **Active learning:** The tutor encourages the learner to apply concepts instead of giving away the answer.\n  * **Deepen metacognition:** The tutor provides clear, constructive feedback to the learner when appropriate.\n  * **Manage cognitive load:** The tutor provides the right amount of feedback at the right time.\n\n\n\n## What's next?\n\nTest LearnLM for yourself in [AI Studio](https://aistudio.google.com).\n\n## Feedback\n\nYou can provide feedback on LearnLM using our [feedback form](https://docs.google.com/forms/d/e/1FAIpQLSf5-B50OnNFjVGHLFkSerP1k0PZXHMgcnQ7k1cM_hIsqIjpjA/viewform).\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/migrate-to-cloud",
    "title": "Build with Gemini on Google Cloud  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Build with Gemini on Google Cloud \n\nIf you are new to Gemini, using the [quickstarts](https://ai.google.dev/gemini-api/docs/quickstart?lang=python) is the fastest way to get started. \n\nHowever, as your generative AI solutions mature, you may need a platform for building and deploying generative AI applications and solutions end to end. Google Cloud provides a comprehensive ecosystem of tools to enable developers to harness the power of generative AI, from the initial stages of app development to app deployment, app hosting, and managing complex data at scale. \n\nGoogle Cloud's Vertex AI platform offers a suite of MLOps tools that streamline usage, deployment, and monitoring of AI models for efficiency and reliability. Additionally, integrations with databases, DevOps tools, logging, monitoring, and IAM provide a holistic approach to managing the entire generative AI lifecycle. \n\nThe following table summarizes the main differences between Google AI and Vertex AI to help you decide which option is right for your use case: \n\n**Features** | **Google AI Gemini API** | **Vertex AI Gemini API**  \n---|---|---  \nGemini models  | Gemini 2.0 Flash, Gemini 2.0 Flash-Lite  | Gemini 2.0 Flash, Gemini 2.0 Flash-Lite   \nSign up  | Google account  | Google Cloud account (with terms agreement and billing)   \nAuthentication  | API key  | Google Cloud service account   \nUser interface playground  | Google AI Studio  | Vertex AI Studio   \nAPI & SDK  | Server and mobile/web client SDKs  \n\n\n  * Server: Python, Node.js, Go, Dart, ABAP\n  * Mobile/Web client: Android (Kotlin/Java), Swift, Web, Flutter\n\n| Server and mobile/web client SDKs  \n\n\n  * Server: Python, Node.js, Go, Java, ABAP\n  * Mobile/Web client (via [Vertex AI for Firebase](https://firebase.google.com/docs/vertex-ai)): Android (Kotlin/Java), Swift, Web, Flutter\n\n  \nNo-cost usage of API & SDK  | Yes, [where applicable](https://ai.google.dev/gemini-api/docs/billing#is-Gemini-free-in-EEA-UK-CH) | $300 Google Cloud credit for new users   \nQuota (requests per minute)  | Varies based on model and pricing plan (see [detailed information](https://ai.google.dev/pricing))  | Varies based on model and region (see [detailed information](https://cloud.google.com/vertex-ai/generative-ai/docs/quotas))   \nEnterprise support  | No  |  Customer encryption key  \nVirtual private cloud  \nData residency  \nAccess transparency  \nScalable infrastructure for application hosting  \nDatabases and data storage   \nMLOps  | No  | Full MLOps on Vertex AI (examples: model evaluation, Model Monitoring, Model Registry)   \n  \nTo learn which products, frameworks, and tools are the best match for building your generative AI application on Google Cloud, see [Build a generative AI application on Google Cloud](https://cloud.google.com/docs/ai-ml/generative-ai).\n\n## Migrate from Gemini on Google AI to Vertex AI\n\nIf your application uses Google AI Gemini APIs, you'll need to migrate to Google Cloud's Vertex AI Gemini APIs.\n\nWhen you migrate:\n\n  * You can use your existing Google Cloud project (the same one you used to generate your API key) or you can [create a new Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects).\n\n  * Supported regions may differ between Google AI Studio and Vertex AI. See the list of [supported regions for generative AI on Google Cloud](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations-genai).\n\n  * Any models you created in Google AI Studio need to be retrained in Vertex AI.\n\n\n\n\nThe [Google Gen AI SDK](/gemini-api/docs/sdks) provides a unified interface to Gemini 2.0 through both the Gemini Developer API and Vertex AI. With a few exceptions, code that runs on one platform will run on both.\n\nNote that if you want to call the Gemini API directly from a production mobile or web app, then [migrate to use the Vertex AI in Firebase client SDKs](https://firebase.google.com/docs/vertex-ai/migrate-to-vertex-ai) (available for Swift, Android, Web, and Flutter apps). These client SDKs offer critical security options and other features for production mobile and web apps.\n\n## Delete unused API Keys\n\nIf you no longer need to use your Google AI Gemini API key, follow security best practices and delete it.\n\nTo delete an API key:\n\n  1. Open the [Google Cloud API Credentials](https://console.cloud.google.com/apis/credentials) page.\n\n  2. Find the API key you want to delete and click the **Actions** icon.\n\n  3. Select **Delete API key**.\n\n  4. In the **Delete credential** modal, select **Delete**.\n\nDeleting an API key takes a few minutes to propagate. After propagation completes, any traffic using the deleted API key is rejected.\n\n\n**Important:** If you have deleted a key that is still used in production and need to recover it, see [gcloud beta services api-keys undelete](https://cloud.google.com/sdk/gcloud/reference/beta/services/api-keys/undelete).\n\n## Next steps\n\n  * See the [Generative AI on Vertex AI overview](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/overview) to learn more about generative AI solutions on Vertex AI.\n  * Dive deeper into the [Vertex AI Gemini API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/gemini).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-07 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-07 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/oauth",
    "title": "Authentication with OAuth quickstart  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Authentication with OAuth quickstart \n\nThe easiest way to authenticate to the Gemini API is to configure an API key, as described in the [Gemini API quickstart](/gemini-api/docs/quickstart). If you need stricter access controls, you can use OAuth instead. This guide will help you set up authentication with OAuth.\n\nThis guide uses a simplified authentication approach that is appropriate for a testing environment. For a production environment, learn about [authentication and authorization](https://developers.google.com/workspace/guides/auth-overview) before [choosing the access credentials](https://developers.google.com/workspace/guides/create-credentials#choose_the_access_credential_that_is_right_for_you) that are appropriate for your app.\n\n## Objectives\n\n  * Set up your cloud project for OAuth\n  * Set up application-default-credentials\n  * Manage credentials in your program instead of using `gcloud auth`\n\n\n\n## Prerequisites\n\nTo run this quickstart, you need:\n\n  * [A Google Cloud project](https://developers.google.com/workspace/guides/create-project)\n  * [A local installation of the gcloud CLI](https://cloud.google.com/sdk/docs/install)\n\n\n\n## Set up your cloud project\n\nTo complete this quickstart, you first need to setup your Cloud project.\n\n### 1\\. Enable the API\n\nBefore using Google APIs, you need to turn them on in a Google Cloud project.\n\n  * In the Google Cloud console, enable the Google Generative Language API.  \n\n\n[Enable the API](https://console.cloud.google.com/flows/enableapi?apiid=generativelanguage.googleapis.com)\n\n\n\n\n### 2\\. Configure the OAuth consent screen\n\nNext configure the project's OAuth consent screen and add yourself as a test user. If you've already completed this step for your Cloud project, skip to the next section.\n\n  1. In the Google Cloud console, go to **Menu** > **Google Auth platform** > **Overview**.\n\n[ Go to the Google Auth platform](https://console.developers.google.com/auth/overview)\n\n  2. Complete the project configuration form and set the user type to **External** in the **Audience** section.\n\n  3. Complete the rest of the form, accept the User Data Policy terms, and then click **Create**.\n\n  4. For now, you can skip adding scopes and click **Save and Continue**. In the future, when you create an app for use outside of your Google Workspace organization, you must add and verify the authorization scopes that your app requires.\n\n  5. Add test users:\n\n    1. Navigate to the [Audience page](https://console.developers.google.com/auth/audience) of the Google Auth platform.\n    2. Under **Test users** , click **Add users**.\n    3. Enter your email address and any other authorized test users, then click **Save**.\n\n\n\n### 3\\. Authorize credentials for a desktop application\n\nTo authenticate as an end user and access user data in your app, you need to create one or more OAuth 2.0 Client IDs. A client ID is used to identify a single app to Google's OAuth servers. If your app runs on multiple platforms, you must create a separate client ID for each platform.\n\n  1. In the Google Cloud console, go to **Menu** > **Google Auth platform** > **Clients**.\n\n[ Go to Credentials](https://console.developers.google.com/auth/clients)\n\n  2. Click **Create Client**.\n\n  3. Click **Application type** > **Desktop app**.\n\n  4. In the **Name** field, type a name for the credential. This name is only shown in the Google Cloud console.\n\n  5. Click **Create**. The OAuth client created screen appears, showing your new Client ID and Client secret.\n\n  6. Click **OK**. The newly created credential appears under **OAuth 2.0 Client IDs.**\n\n  7. Click the download button to save the JSON file. It will be saved as `client_secret_<identifier>.json`, and rename it to `client_secret.json` and move it to your working directory.\n\n\n\n\n## Set up application default credentials\n\nTo convert the `client_secret.json` file into usable credentials, pass its location the `gcloud auth application-default login` command's `--client-id-file` argument.\n    \n    \n    gcloud auth application-default login \\\n        --client-id-file=client_secret.json \\\n        --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever'\n\nThe simplified project setup in this tutorial triggers a **\"Google hasn't verified this app.\"** dialog. This is normal, choose **\"continue\"**.\n\nThis places the resulting token in a well known location so it can be accessed by `gcloud` or the client libraries.\n\n**Note:** If running on Colab include `--no-browser` and carefully follow the instructions it prints (don't just click the link). Also make sure your local `gcloud --version` is the [latest](https://cloud.google.com/sdk/docs/release-notes) to match Colab.\n\n`\n    \n    \n    gcloud auth application-default login   \n    \n        **--no-browser**\n        --client-id-file=client_secret.json   \n    \n        --scopes='https://www.googleapis.com/auth/cloud-platform,https://www.googleapis.com/auth/generative-language.retriever'\n    \n\n`\n\nOnce you have the application default credentials (ACD) set, the client libraries in most languages need minimal to no help to find them.\n\n### Curl\n\nThe quickest way to test that this is working is to use it to access the REST API using curl:\n    \n    \n    access_token=$(gcloud auth application-default print-access-token)\n    project_id=<MY PROJECT ID>\n    curl -X GET https://generativelanguage.googleapis.com/v1/models \\\n        -H 'Content-Type: application/json' \\\n        -H \"Authorization: Bearer ${access_token}\" \\\n        -H \"x-goog-user-project: ${project_id}\" | grep '\"name\"'\n\n### Python\n\nIn python the client libraries should find them automatically:\n    \n    \n    pip install google-generativeai\n    \n\nA minimal script to test it might be:\n    \n    \n    import google.generativeai as genai\n    \n    print('Available base models:', [m.name for m in genai.list_models()])\n    \n\n## Next steps\n\nIf that's working you're ready to try [Semantic retrieval on your text data](/docs/semantic_retriever).\n\n## Manage credentials yourself [Python]\n\nIn many cases you won't have the `gcloud` command available to create the access token from the Client ID (`client_secret.json`). Google provides libraries in many languages to let you manage that process within your app. This section demonstrates the process, in python. There are equivalent examples of this sort of procedure, for other languages, available in the [Drive API documentation](https://developers.google.com/drive/api/quickstart/python)\n\n### 1\\. Install the necessary libraries\n\nInstall the Google client library for Python, and the Gemini client library.\n    \n    \n    pip install --upgrade -q google-api-python-client google-auth-httplib2 google-auth-oauthlib\n    pip install google-generativeai\n\n### 2\\. Write the credential manager\n\nTo minimize the number of times you have to click through the authorization screens, create a file called `load_creds.py` in your working directory to caches a `token.json` file that it can reuse later, or refresh if it expires.\n\nStart with the following code to convert the `client_secret.json` file to a token usable with `genai.configure`:\n    \n    \n    import os.path\n    \n    from google.auth.transport.requests import Request\n    from google.oauth2.credentials import Credentials\n    from google_auth_oauthlib.flow import InstalledAppFlow\n    \n    SCOPES = ['https://www.googleapis.com/auth/generative-language.retriever']\n    \n    def load_creds():\n        \"\"\"Converts `client_secret.json` to a credential object.\n    \n        This function caches the generated tokens to minimize the use of the\n        consent screen.\n        \"\"\"\n        creds = None\n        # The file token.json stores the user's access and refresh tokens, and is\n        # created automatically when the authorization flow completes for the first\n        # time.\n        if os.path.exists('token.json'):\n            creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n        # If there are no (valid) credentials available, let the user log in.\n        if not creds or not creds.valid:\n            if creds and creds.expired and creds.refresh_token:\n                creds.refresh(Request())\n            else:\n                flow = InstalledAppFlow.from_client_secrets_file(\n                    'client_secret.json', SCOPES)\n                creds = flow.run_local_server(port=0)\n            # Save the credentials for the next run\n            with open('token.json', 'w') as token:\n                token.write(creds.to_json())\n        return creds\n    \n\n### 3\\. Write your program\n\nNow create your `script.py`:\n    \n    \n    import pprint\n    import google.generativeai as genai\n    from load_creds import load_creds\n    \n    creds = load_creds()\n    \n    genai.configure(credentials=creds)\n    \n    print()\n    print('Available base models:', [m.name for m in genai.list_models()])\n    \n\n### 4\\. Run your program\n\nIn your working directory, run the sample:\n    \n    \n    python script.py\n\nThe first time you run the script, it opens a browser window and prompts you to authorize access.\n\n  1. If you're not already signed in to your Google Account, you're prompted to sign in. If you're signed in to multiple accounts, **be sure to select the account you set as a \"Test Account\" when configuring your project.**\n\n**Note:** The simplified project setup in this tutorial triggers a **\"Google hasn't verified this app.\"** dialog. This is normal, choose **\"continue\"**.\n  2. Authorization information is stored in the file system, so the next time you run the sample code, you aren't prompted for authorization.\n\n\n\n\nYou have successfully setup authentication.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-03-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-03-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/troubleshooting",
    "title": "Troubleshooting guide  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Troubleshooting guide \n\nPython Go\n\nUse this guide to help you diagnose and resolve common issues that arise when you call the Gemini API. You may encounter issues from either the Gemini API backend service or the client SDKs. Our client SDKs are open sourced in the following repositories:\n\n  * [python-genai](https://github.com/googleapis/python-genai)\n  * [js-genai](https://github.com/googleapis/js-genai)\n  * [generative-ai-go](https://github.com/google/generative-ai-go)\n\n\n\nIf you encounter API key issues, ensure you have set up your API key correctly per the [API key setup guide](/tutorials/setup).\n\n## Gemini API backend service error codes\n\nThe following table lists common backend error codes you may encounter, along with explanations for their causes and troubleshooting steps:\n\n**HTTP Code** | **Status** | **Description** | **Example** | **Solution**  \n---|---|---|---|---  \n400  | INVALID_ARGUMENT | The request body is malformed. | There is a typo, or a missing required field in your request. | Check the [API reference](/api) for request format, examples, and supported versions. Using features from a newer API version with an older endpoint can cause errors.  \n400  | FAILED_PRECONDITION | Gemini API free tier is not available in your country. Please enable billing on your project in Google AI Studio. | You are making a request in a region where the free tier is not supported, and you have not enabled billing on your project in Google AI Studio. | To use the Gemini API, you will need to setup a paid plan using [Google AI Studio](https://aistudio.google.com/app/plan_information).  \n403  | PERMISSION_DENIED | Your API key doesn't have the required permissions. | You are using the wrong API key; you are trying to use a tuned model without going through [proper authentication](/docs/model-tuning/tutorial?lang=python#set_up_authentication). | Check that your API key is set and has the right access. And make sure to go through proper authentication to use tuned models.  \n404  | NOT_FOUND | The requested resource wasn't found. | An image, audio, or video file referenced in your request was not found. | Check if all [parameters in your request are valid](/docs/troubleshooting#check-api) for your API version.  \n429  | RESOURCE_EXHAUSTED | You've exceeded the rate limit. | You are sending too many requests per minute with the free tier Gemini API. | Ensure you're within the model's [rate limit](/models/gemini#model-variations). [Request a quota increase](/docs/increase_quota) if needed.  \n500  | INTERNAL | An unexpected error occurred on Google's side. | Your input context is too long. | Reduce your input context or temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the **Send feedback** button in Google AI Studio.  \n503  | UNAVAILABLE | The service may be temporarily overloaded or down. | The service is temporarily running out of capacity. | Temporarily switch to another model (e.g. from Gemini 1.5 Pro to Gemini 1.5 Flash) and see if it works. Or wait a bit and retry your request. If the issue persists after retrying, please report it using the **Send feedback** button in Google AI Studio.  \n504  | DEADLINE_EXCEEDED | The service is unable to finish processing within the deadline. | Your prompt (or context) is too large to be processed in time. | Set a larger 'timeout' in your client request to avoid this error.  \n  \n## Client SDK error codes\n\n## Check your API calls for model parameter errors\n\nEnsure your model parameters are within the following values:\n\n**Model parameter** | **Values (range)**  \n---|---  \nCandidate count  | 1-8 (integer)  \nTemperature  | 0.0-1.0  \nMax output tokens  |  Use `get_model` ([Python](/api/python/google/generativeai/get_model)) to determine the maximum number of tokens for the model you are using.   \nTopP  | 0.0-1.0  \n  \nIn addition to checking parameter values, make sure you're using the correct [API version](/gemini-api/docs/api-versions) (e.g., `/v1` or `/v1beta`) and model that supports the features you need. For example, if a feature is in Beta release, it will only be available in the `/v1beta` API version.\n\n## Check if you have the right model\n\nEnsure you are using a supported model listed on our [models page](/gemini-api/docs/models/gemini).\n\n## Safety issues\n\nIf you see a prompt was blocked because of a safety setting in your API call, review the prompt with respect to the filters you set in the API call.\n\nIf you see `BlockedReason.OTHER`, the query or response may violate the [terms of service](/terms) or be otherwise unsupported.\n\n## Recitation issue\n\nIf you see the model stops generating output due to the RECITATION reason, this means the model output may resemble certain data. To fix this, try to make prompt / context as unique as possible and use a higher temperature.\n\n## Improve model output\n\nFor higher quality model outputs, explore writing more structured prompts. The [introduction to prompt design](/docs/prompt_best_practices) page introduces some basic concepts, strategies, and best practices to get you started.\n\nIf you have hundreds of examples of good input/output pairs, you can also consider [model tuning](/docs/model_tuning_guidance).\n\n## Understand token limits\n\nRead through our [Token guide](/gemini-api/docs/tokens) to better understand how to count tokens and their limits.\n\n## Known issues\n\n  * The API supports only a number of select languages. Submitting prompts in unsupported languages can produce unexpected or even blocked responses. See [available languages](/models/gemini#available-languages) for updates.\n\n\n\n## File a bug\n\nJoin the discussion on the [Google AI developer forum](https://discuss.ai.google.dev) if you have questions.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-03-24 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-03-24 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/troubleshoot-ai-studio",
    "title": "Troubleshoot Google AI Studio  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Troubleshoot Google AI Studio \n\nThis page provides suggestions for troubleshooting Google AI Studio if you encounter issues.\n\n## Understand 403 Access Restricted errors\n\nIf you see a 403 Access Restricted error, you are using Google AI Studio in a way that does not follow the [Terms of Service](/terms). One common reason is you are not located in a [supported region](/available_regions).\n\n## Resolve No Content responses on Google AI Studio\n\nA warning **No Content** message appears on Google AI Studio if the content is blocked for any reason. To see more details, hold the pointer over **No Content** and click warning **Safety**.\n\nIf the response was blocked due to [safety settings](/docs/safety_setting) and you considered the [safety risks](/docs/safety_guidance) for your use case, you can modify the [safety settings](/docs/safety_setting#safety_settings_in_makersuite) to influence the returned response.\n\nIf the response was blocked but not due to the safety settings, the query or response may violate the [Terms of Service](/terms) or be otherwise unsupported.\n\n## Check token usage and limits\n\nWhen you have a prompt open, the **Text Preview** button at the bottom of the screen shows the current tokens used for the content of your prompt and the maximum token count for the model being used.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/workspace",
    "title": "Access Google AI Studio with your Workspace account  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Access Google AI Studio with your Workspace account \n\nAll [Google Workspace](https://workspace.google.com) users have access to AI Studio by default. If you're a Workspace user and you want to get started with AI Studio, check out the [AI Studio quickstart](/gemini-api/docs/ai-studio-quickstart).\n\n## Troubleshooting\n\nIf access to AI Studio is disabled for your Google Workspace account, you might see an error like the following:\n\n`We are sorry, but you do not have access to Google AI Studio. Please contact your Organization Administrator for access.`\n\nIf you think you should have access to AI Studio, contact your Workspace administrator.\n\n## Enable AI Studio for Workspace users\n\n**Important:** This section is intended for Google Workspace administrators.\n\nAs a Google Workspace administrator, you can control who uses AI Studio:\n\n  * AI Studio is turned on by default for all editions.\n  * You can turn AI Studio off or on for sets of users across or within organizational units.\n  * Google Workspace for Education editions: Users under the age of 18 are restricted from using AI Studio with their Google Workspace for Education accounts. This is true even when the AI Studio setting is on. For details, go to [Control access to Google services by age](https://support.google.com/a/answer/10651918).\n\n\n\nTo enable or disable AI Studio for users in your organization, see [Turn Google AI Studio on or off for users](https://support.google.com/a/answer/15004095).\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/available-regions",
    "title": "Available regions for Google AI Studio and Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Available regions for Google AI Studio and Gemini API \n\nIf you reached this page after trying to open [Google AI Studio](https://makersuite.google.com/), it may be because Google AI Studio is not available in your region, or you don't meet the age requirements (18+) for access. You can learn more about the available regions in the following section and other requirements in the [terms of service](/gemini-api/terms).\n\n## Available regions\n\n**Note:** For Colab users - Region restrictions are applied based on the region that the **Colab instance** is in, not the region that **the user** is in. You can check the location of the **Colab instance** using `!curl ipinfo.io`\n\nThe Gemini API and Google AI Studio are available in the following countries and territories. If you're not in one of these countries or territories, try the [Gemini API in Vertex AI](https://cloud.google.com/vertex-ai#build-with-gemini):\n\n  * Albania\n  * Algeria\n  * American Samoa\n  * Angola\n  * Anguilla\n  * Antarctica\n  * Antigua and Barbuda\n  * Argentina\n  * Armenia\n  * Aruba\n  * Australia\n  * Austria\n  * Azerbaijan\n  * The Bahamas\n  * Bahrain\n  * Bangladesh\n  * Barbados\n  * Belgium\n  * Belize\n  * Benin\n  * Bermuda\n  * Bhutan\n  * Bolivia\n  * Bosnia\n  * Botswana\n  * Brazil\n  * British Indian Ocean Territory\n  * British Virgin Islands\n  * Brunei\n  * Bulgaria\n  * Burkina Faso\n  * Burundi\n  * Cabo Verde\n  * Cambodia\n  * Cameroon\n  * Canada\n  * Caribbean Netherlands\n  * Cayman Islands\n  * Central African Republic\n  * Chad\n  * Chile\n  * Christmas Island\n  * Cocos (Keeling) Islands\n  * Colombia\n  * Comoros\n  * Cook Islands\n  * Côte d'Ivoire\n  * Costa Rica\n  * Croatia\n  * Curaçao\n  * Czech Republic\n  * Democratic Republic of the Congo\n  * Denmark\n  * Djibouti\n  * Dominica\n  * Dominican Republic\n  * Ecuador\n  * Egypt\n  * El Salvador\n  * Equatorial Guinea\n  * Eritrea\n  * Estonia\n  * Eswatini\n  * Ethiopia\n  * Falkland Islands (Islas Malvinas)\n  * Faroe Islands\n  * Fiji\n  * Finland\n  * France\n  * Gabon\n  * The Gambia\n  * Georgia\n  * Germany\n  * Ghana\n  * Gibraltar\n  * Greece\n  * Greenland\n  * Grenada\n  * Guam\n  * Guatemala\n  * Guernsey\n  * Guinea\n  * Guinea-Bissau\n  * Guyana\n  * Haiti\n  * Heard Island and McDonald Islands\n  * Herzegovina\n  * Honduras\n  * Hungary\n  * Iceland\n  * India\n  * Indonesia\n  * Iraq\n  * Ireland\n  * Isle of Man\n  * Israel\n  * Italy\n  * Jamaica\n  * Japan\n  * Jersey\n  * Jordan\n  * Kazakhstan\n  * Kenya\n  * Kiribati\n  * Kosovo\n  * Kyrgyzstan\n  * Kuwait\n  * Laos\n  * Latvia\n  * Lebanon\n  * Lesotho\n  * Liberia\n  * Libya\n  * Liechtenstein\n  * Lithuania\n  * Luxembourg\n  * Madagascar\n  * Malawi\n  * Malaysia\n  * Maldives\n  * Mali\n  * Malta\n  * Marshall Islands\n  * Mauritania\n  * Mauritius\n  * Mexico\n  * Micronesia\n  * Mongolia\n  * Montenegro\n  * Montserrat\n  * Morocco\n  * Mozambique\n  * Namibia\n  * Nauru\n  * Nepal\n  * Netherlands\n  * New Caledonia\n  * New Zealand\n  * Nicaragua\n  * Niger\n  * Nigeria\n  * Niue\n  * Norfolk Island\n  * North Macedonia\n  * Northern Mariana Islands\n  * Norway\n  * Oman\n  * Pakistan\n  * Palau\n  * Palestine\n  * Panama\n  * Papua New Guinea\n  * Paraguay\n  * Peru\n  * Philippines\n  * Pitcairn Islands\n  * Poland\n  * Portugal\n  * Puerto Rico\n  * Qatar\n  * Republic of Cyprus\n  * Republic of the Congo\n  * Romania\n  * Rwanda\n  * Saint Barthélemy\n  * Saint Kitts and Nevis\n  * Saint Lucia\n  * Saint Pierre and Miquelon\n  * Saint Vincent and the Grenadines\n  * Saint Helena, Ascension and Tristan da Cunha\n  * Samoa\n  * São Tomé and Príncipe\n  * Saudi Arabia\n  * Senegal\n  * Serbia\n  * Seychelles\n  * Sierra Leone\n  * Singapore\n  * Slovakia\n  * Slovenia\n  * Solomon Islands\n  * Somalia\n  * South Africa\n  * South Georgia and the South Sandwich Islands\n  * South Korea\n  * South Sudan\n  * Spain\n  * Sri Lanka\n  * Sudan\n  * Suriname\n  * Sweden\n  * Switzerland\n  * Taiwan\n  * Tajikistan\n  * Tanzania\n  * Thailand\n  * Timor-Leste\n  * Togo\n  * Tokelau\n  * Tonga\n  * Trinidad and Tobago\n  * Tunisia\n  * Türkiye\n  * Turkmenistan\n  * Turks and Caicos Islands\n  * Tuvalu\n  * Uganda\n  * Ukraine\n  * United Kingdom\n  * United Arab Emirates\n  * United States\n  * United States Minor Outlying Islands\n  * U.S. Virgin Islands\n  * Uruguay\n  * Uzbekistan\n  * Vanuatu\n  * Venezuela\n  * Vietnam\n  * Wallis and Futuna\n  * Western Sahara\n  * Yemen\n  * Zambia\n  * Zimbabwe\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/usage-policies",
    "title": "Additional usage policies  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Additional usage policies \n\nThis page includes additional usage policies for the Gemini API.\n\n## Abuse monitoring\n\nGoogle is committed to the responsible development and use of AI. To ensure the safety and integrity of the Gemini API, we have created these policy guidelines. By using the Gemini API, you agree to the following guidelines, the [Gemini API Additional Terms of Service](/gemini-api/terms) and Generative AI [Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy).\n\n### How We Monitor for Misuse\n\nGoogle's Trust and Safety Team employs a combination of automated and manual processes to detect potential misuse of the Gemini API and enforce our policies.\n\n  * **Automated Detection:** Automated systems scan API usage for violations of our Prohibited Use Policy, such as hate speech, harassment, sexually explicit content, and dangerous content.\n  * **Manual Detection:** If a project consistently exhibits suspicious activity, it may be flagged for manual review by authorized Google personnel.\n\n\n\n### How We Handle Data\n\nTo help with abuse monitoring, Google retains the following data for fifty-five (55) days:\n\n  * **Prompts:** The text prompts you submit to the API.\n  * **Contextual Information:** Any additional context you provide with your prompts.\n  * **Output:** The responses generated by the Gemini API.\n\n\n\n### How We Investigate Potential Issues\n\nWhen prompts or model outputs are flagged by safety filters and abuse detection systems described above, authorized Google employees may assess the flagged content, and either confirm or correct the classification or determination based on predefined guidelines and policies. Data can be accessed for human review only by authorized Google employees via an internal governance assessment and review management platform. When data is logged for abuse monitoring, it is used solely for the purpose of policy enforcement and is not used to train or fine-tune any AI/ML models.\n\n### Working with You on Policy Compliance\n\nIf your use of Gemini doesn't align with our policies, we may take the following steps:\n\n  * **Get in touch:** We may reach out to you through email to understand your use case and explore ways to bring your usage into compliance.\n  * **Temporary usage limits:** We may limit your access to the Gemini API.\n  * **Temporary suspension:** We may temporarily pause your access to the Gemini API.\n  * **Account closure:** As a last resort, and for serious violations, we may permanently close your access to the Gemini API and other Google services.\n\n\n\n### Scope\n\nThese policy guidelines apply to the use of the Gemini API and AI Studio.\n\n## Inline Preference Voting\n\nIn Google AI Studio, you might occasionally see a side-by-side comparison of two different responses to your prompt. This is part of our Inline Preference Voting system. You'll be asked to choose which response you prefer. This helps us understand which model outputs users find most helpful.\n\n### Why are we doing this?\n\nWe're constantly working to improve our AI models and services. Your feedback through Inline Preference Voting helps us provide, improve, and develop Google products and services and machine learning technologies, including Google's enterprise features, products and services, consistent with the [Gemini API Additional Terms of Service](/gemini-api/terms) and [Privacy Policy](https://policies.google.com/privacy).\n\n### What data is included in Feedback?\n\nTo make informed decisions about our models, we collect certain data when you participate in Inline Preference Voting:\n\n  * **Prompts and Responses:** We record all prompts and responses, including any uploaded content, in the conversation you submitted feedback about. We also record the two response options that you selected from. This helps us understand the context of your preference.\n  * **Your Vote:** We record which response you preferred. This is the core of the feedback we're collecting.\n  * **Usage Details:** This includes information about which model generated the response and other technical and operational details about your usage of this feature.\n\n\n\n### Your Privacy\n\nWe take your privacy seriously. Google takes steps to protect your privacy as part of this process. This includes disconnecting this data from your Google Account, API key, and Cloud project before reviewers see or annotate it. **Do not submit feedback on conversations that include sensitive, confidential, or personal information.**\n\n### Opting Out\n\nYou'll have the option to skip the Inline Preference Voting when it appears.\n\nThank you for helping us improve Google AI Studio!\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-03-24 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-03-24 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/api-versions",
    "title": "API versions explained  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n  * [ API Reference ](https://ai.google.dev/api)\n\n\n\nSend feedback \n\n#  API versions explained \n\nThis document provides a high-level overview of the differences between the `v1` and `v1beta` versions of the Gemini API.\n\n  * **v1** : Stable version of the API. Features in the stable version are fully-supported over the lifetime of the major version. If there are any breaking changes, then the next major version of the API will be created and the existing version will be deprecated after a reasonable period of time. Non-breaking changes may be introduced to the API without changing the major version.\n  * **v1beta** : This version includes early-access features that may be under development and is subject to rapid and breaking changes. There is also no guarantee that the features in the Beta version will move to the stable version. Due to this instability, you should consider not launching production applications with this version.\n\nFeature | v1 | v1beta  \n---|---|---  \nGenerate Content - Text-only input |  |   \nGenerate Content - Text-and-image input |  |   \nGenerate Content - Text output |  |   \nGenerate Content - Multi-turn conversations (chat) |  |   \nGenerate Content - Function calls |  |   \nGenerate Content - Streaming |  |   \nEmbed Content - Text-only input |  |   \nGenerate Answer |  |   \nSemantic retriever |  |   \n  \n  * \\- Supported\n  * \\- Will never be supported\n\n\n\n## Configure API version in an SDK\n\nThe Gemini API SDK's default to `v1beta`, but you can opt to use other versions by setting the API version as shown in the following code sample:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"YOUR_API_KEY\",\n                          http_options={'api_version': 'v1alpha'})\n    \n    response = client.models.generate_content(\n        model='gemini-2.0-flash',\n        contents=\"Explain how AI works\",\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({\n      apiKey: \"YOUR_API_KEY\",\n      httpOptions: { apiVersion: \"v1alpha\" },\n    });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Explain how AI works\",\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1/models/gemini-1.5-flash:generateContent?key=YOUR_API_KEY\" \\\n    -H 'Content-Type: application/json' \\\n    -X POST \\\n    -d '{\n      \"contents\": [{\n        \"parts\":[{\"text\": \"Explain how AI works.\"}]\n        }]\n       }'\n    \n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-03-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-03-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/gemini",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/gemini#model-versions",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/experimental-models#available-models",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/semantic_retrieval",
    "title": "Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\n# Gemini Developer API\n\n[Get a Gemini API Key](https://aistudio.google.com/apikey)\n\nGet a Gemini API key and make your first API request in minutes.\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"YOUR_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=\"Explain how AI works\",\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"YOUR_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Explain how AI works\",\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=YOUR_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Explain how AI works in a few words\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n## Meet the models\n\n[Use Gemini in Google AI Studio](https://aistudio.google.com)\n\n2.5 Pro experiment\n\nOur most powerful thinking model with features for complex reasoning and much more \n\n[](/gemini-api/docs/models#gemini-2.5-pro-preview-03-25)\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n[](/gemini-api/docs/models/gemini#gemini-2.0-flash)\n\n2.0 Flash-Lite bolt\n\nOur fastest and most cost-efficient multimodal model with great performance for high-frequency tasks \n\n[](/gemini-api/docs/models/gemini#gemini-2.0-flash-lite)\n\n## Explore the API\n\n### Native Image Generation\n\nGenerate and edit highly contextual images natively with Gemini 2.0 Flash.\n\n[](/gemini-api/docs/image-generation)\n\n### Explore long context\n\nInput millions of tokens to Gemini models and derive understanding from unstructured images, videos, and documents.\n\n[](/gemini-api/docs/long-context)\n\n### Generate structured outputs\n\nConstrain Gemini to respond with JSON, a structured data format suitable for automated processing.\n\n[](/gemini-api/docs/structured-output)\n\n### Start building with the Gemini API\n\n[Get started](/gemini-api/docs/quickstart)\n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\n[[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro-preview-03-25",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/gemini#gemini-2.0-flash",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/gemini#gemini-2.0-flash-lite",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/quickstart?lang=python",
    "title": "Gemini API quickstart  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini API quickstart \n\nThis quickstart shows you how to install your SDK of choice and then make your first Gemini API request.\n\nPython JavaScript REST Go Apps Script\n\n## Install the Gemini API library\n\n**Note:** We're rolling out a new set of Gemini API libraries, the [Google Gen AI SDK](/gemini-api/docs/sdks).\n\nUsing [Python 3.9+](https://www.python.org/downloads/), install the [`google-genai` package](https://pypi.org/project/google-genai/) using the following [pip command](https://packaging.python.org/en/latest/tutorials/installing-packages/):\n    \n    \n    pip install -q -U google-genai\n    \n\n## Make your first request\n\n[Get a Gemini API key in Google AI Studio](https://aistudio.google.com/app/apikey)\n\nUse the [`generateContent`](/api/generate-content#method:-models.generatecontent) method to send a request to the Gemini API.\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"YOUR_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\", contents=\"Explain how AI works in a few words\"\n    )\n    print(response.text)\n    \n\n## What's next\n\nNow that you made your first API request, you might want to explore the following guides that show Gemini in action:\n\n  * [Text generation](/gemini-api/docs/text-generation)\n  * [Vision](/gemini-api/docs/vision)\n  * [Long context](/gemini-api/docs/long-context)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/sdks",
    "title": "Gemini API Libraries  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini API Libraries \n\nThis page provides information about how to download or install the libraries for accessing the Gemini API. If you're new to the Gemini API, try out the [API quickstart](/gemini-api/docs/quickstart).\n\n## Python\n\nWe provide a [Python SDK](https://pypi.org/project/google-genai) which you can install by running:\n    \n    \n    pip install google-genai\n    \n\n## JavaScript and TypeScript (Preview)\n\nWe provide a [JavaScript and TypeScript SDK](https://www.npmjs.com/package/@google/genai) which you can install by running:\n    \n    \n    npm install @google/genai\n    \n\n## Go (Preview)\n\nWe provide a [Go SDK](https://pkg.go.dev/google.golang.org/genai) which you can install by running:\n    \n    \n    go get https://github.com/googleapis/go-genai\n    \n\n## Previous SDKs\n\nThe following is a set of our previous SDK's which are no longer being actively developed:\n\n  * [Previous Dart and Flutter SDK](https://pub.dev/packages/google_generative_ai/install)\n  * [Previous Swift SDK](https://github.com/google/generative-ai-swift)\n  * [Previous Android SDK](https://github.com/google-gemini/generative-ai-android)\n  * [Previous Python SDK](https://github.com/google-gemini/deprecated-generative-ai-python)\n  * [Previous Node.js SDK](https://github.com/google-gemini/generative-ai-js)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-12 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-12 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/billing#is-Gemini-free-in-EEA-UK-CH",
    "title": "Billing  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Billing \n\nThis guide provides an overview of different Gemini API billing options, explains how to enable billing and monitor usage, and provides answers to frequently asked questions (FAQs) about billing.\n\n[Upgrade to the Gemini API paid tier](https://aistudio.google.com/plan_information)\n\n## About billing\n\nBilling for the Gemini API is based on two pricing tiers: _free of charge_ (or _free_) and _pay-as-you-go_ (or _paid_). Pricing and rate limits differ between these tiers and also vary by model. You can check out the [rate limits](/gemini-api/docs/rate-limits) and [pricing](/gemini-api/docs/pricing) pages for more into. For a model-by-model breakdown of capabilities, see the [Gemini models page](/gemini-api/docs/models/gemini).\n\n#### How to request an upgrade\n\nThe Gemini API uses Cloud Billing for all billing services. To transition from the Free tier to a paid tier, you must first enable Cloud Billing for your Google Cloud project.\n\nWhen your project meets the specified criteria, it becomes eligible for an upgrade to the next tier. To request an upgrade, follow these steps:\n\n  1. Go to the [AI Studio API keys page](https://aistudio.google.com/app/apikey).\n  2. Find the project you want to upgrade and click **Upgrade**.\n  3. The system will automatically verify your project's eligibility. This process takes a few seconds.\n  4. If your project meets all the requirements, it will be instantly upgraded to the next tier.\n\n\n\n### Why use the paid tier?\n\nWhen you enable billing and use the paid tier, you benefit from [higher rate limits](/gemini-api/docs/rate-limits), and your prompts and responses aren't used to improve Google products. For more information on data use for paid services, see the [terms of service](/gemini-api/terms#data-use-paid).\n\n### Cloud Billing\n\nThe Gemini API uses [Cloud Billing](https://cloud.google.com/billing/docs/concepts) for billing services. To use the paid tier, you must set up Cloud Billing on your cloud project. After you've enabled Cloud Billing, you can use Cloud Billing tools to track spending, understand costs, make payments, and access Cloud Billing support.\n\n## Enable billing\n\nYou can enable Cloud Billing starting from Google AI Studio:\n\n  1. Open [Google AI Studio](https://aistudio.google.com/).\n\n  2. In the bottom of the left sidebar, select **Settings** > **Plan information**.\n\n  3. Click **Set up Billing** for your chosen project to enable Cloud Billing.\n\n\n\n\n## Monitor usage\n\nAfter you enable Cloud Billing, you can monitor your usage of the Gemini API in the [Google Cloud console](https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com). The service name for the API is `generativelanguage.googleapis.com`, and in the console the Gemini API is also referred to as the **Generative Language API**.\n\nTo learn more, see the Google Cloud documentation on [monitoring API usage](https://cloud.google.com/apis/docs/monitoring).\n\n## Frequently asked questions\n\nThis section provides answers to frequently asked questions.\n\n### What am I billed for?\n\nGemini API pricing is based on the following:\n\n  * Input token count\n  * Output token count\n  * Cached token count\n  * Cached token storage duration\n\n\n\nFor pricing information, see the [pricing page](/pricing).\n\n### Where can I view my quota?\n\nYou can view your quota and system limits in the [Google Cloud console](https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/quotas).\n\n### How do I request more quota?\n\nTo request more quota, follow the instructions at How to request an upgrade.\n\n### Can I use the Gemini API for free in EEA (including EU), the UK, and CH?\n\nYes, we make the free tier and paid tier available in [many regions](/gemini-api/docs/available-regions).\n\n### If I set up billing with the Gemini API, will I be charged for my Google AI Studio usage?\n\nNo, Google AI Studio usage remains free of charge regardless of if you set up billing across all supported regions.\n\n### Can I use 1M tokens in the free tier?\n\nThe free tier for Gemini API differs based on the model selected. For now, you can try the 1M token context window in the following ways:\n\n  * In Google AI Studio\n  * With pay-as-you-go plans\n  * With free-of-charge plans for select models\n\n\n\nSee the latest free-of-charge rate limits per model on [rate limits page](/gemini-api/docs/rate-limits).\n\n### How can I calculate the number of tokens I'm using?\n\nUse the [`GenerativeModel.count_tokens`](/api/python/google/generativeai/GenerativeModel#count_tokens) method to count the number of tokens. Refer to the [Tokens guide](/gemini-api/docs/tokens) to learn more about tokens.\n\n### Can I use my Google Cloud credits with the Gemini API?\n\nYes, Google Cloud credits can be used towards Gemini API usage.\n\n### How is billing handled?\n\nBilling for the Gemini API is handled by the [Cloud Billing](https://cloud.google.com/billing/docs/concepts) system.\n\n### Am I charged for failed requests?\n\nIf your request fails with a 400 or 500 error, you won't be charged for the tokens used. However, the request will still count against your quota.\n\n### Is there a charge for fine-tuning the models?\n\n[Model tuning](/gemini-api/docs/model-tuning) is free, but inference on tuned models is charged at the same rate as the base models.\n\n### Is GetTokens billed?\n\nRequests to the GetTokens API are not billed, and they don't count against inference quota.\n\n### How is my Google AI Studio data handled if I have a paid API account?\n\nRefer to the [terms](/gemini-api/terms#paid-services) for details on how data is handled when Cloud billing is enabled (see \"How Google Uses Your Data\" under \"Paid Services\"). Note that your Google AI Studio prompts are treated under the same \"Paid Services\" terms so long as at least 1 API project has billing enabled, which you can validate on the [Gemini API Key page](https://aistudio.google.com/apikey) if you see any projects marked as \"Paid\" under \"Plan\".\n\n### Where can I get help with billing?\n\nTo get help with billing, see [Get Cloud Billing support](https://cloud.google.com/support/billing).\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-03-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-03-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/system-instructions",
    "title": "Text generation  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Text generation \n\nThe Gemini API can generate text output in response to various inputs, including text, images, video, and audio. This guide shows you how to generate text using text and image inputs. It also covers streaming, chat, and system instructions.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Text input\n\nThe simplest way to generate text using the Gemini API is to provide the model with a single text-only input, as shown in this example:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[\"How does AI work?\"]\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"How does AI work?\",\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    // import packages here\n    \n    func main() {\n      ctx := context.Background()\n      client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n      if err != nil {\n        log.Fatal(err)\n      }\n      defer client.Close()\n    \n      model := client.GenerativeModel(\"gemini-2.0-flash\")\n      resp, err := model.GenerateContent(ctx, genai.Text(\"How does AI work?\"))\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp) // helper function for printing content parts\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"How does AI work?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { text: 'How AI does work?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Image input\n\nThe Gemini API supports multimodal inputs that combine text and media files. The following example shows how to generate text from text and image input:\n\n### Python\n    \n    \n    from PIL import Image\n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    image = Image.open(\"/path/to/organ.png\")\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[image, \"Tell me about this instrument\"]\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import {\n      GoogleGenAI,\n      createUserContent,\n      createPartFromUri,\n    } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const image = await ai.files.upload({\n        file: \"/path/to/organ.png\",\n      });\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: [\n          createUserContent([\n            \"Tell me about this instrument\",\n            createPartFromUri(image.uri, image.mimeType),\n          ]),\n        ],\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-2.0-flash\")\n    \n    imgData, err := os.ReadFile(filepath.Join(testDataDir, \"organ.jpg\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    \n    resp, err := model.GenerateContent(ctx,\n      genai.Text(\"Tell me about this instrument\"),\n      genai.ImageData(\"jpeg\", imgData))\n    if err != nil {\n      log.Fatal(err)\n    }\n    \n    printResponse(resp)\n    \n\n### REST\n    \n    \n    # Use a temporary file to hold the base64 encoded image data\n    TEMP_B64=$(mktemp)\n    trap 'rm -f \"$TEMP_B64\"' EXIT\n    base64 $B64FLAGS $IMG_PATH > \"$TEMP_B64\"\n    \n    # Use a temporary file to hold the JSON payload\n    TEMP_JSON=$(mktemp)\n    trap 'rm -f \"$TEMP_JSON\"' EXIT\n    \n    cat > \"$TEMP_JSON\" << EOF\n    {\n      \"contents\": [\n        {\n          \"parts\": [\n            {\n              \"text\": \"Tell me about this instrument\"\n            },\n            {\n              \"inline_data\": {\n                \"mime_type\": \"image/jpeg\",\n                \"data\": \"$(cat \"$TEMP_B64\")\"\n              }\n            }\n          ]\n        }\n      ]\n    }\n    EOF\n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d \"@$TEMP_JSON\"\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const imageUrl = 'http://image/url';\n      const image = getImageData(imageUrl);\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { image },\n              { text: 'Tell me about this instrument' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n    function getImageData(url) {\n      const blob = UrlFetchApp.fetch(url).getBlob();\n    \n      return {\n        mimeType: blob.getContentType(),\n        data: Utilities.base64Encode(blob.getBytes())\n      };\n    }\n    \n\n## Streaming output\n\nBy default, the model returns a response after completing the entire text generation process. You can achieve faster interactions by using streaming to return instances of [`GenerateContentResponse`](/api/generate-content#v1beta.GenerateContentResponse) as they're generated.\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content_stream(\n        model=\"gemini-2.0-flash\",\n        contents=[\"Explain how AI works\"]\n    )\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContentStream({\n        model: \"gemini-2.0-flash\",\n        contents: \"Explain how AI works\",\n      });\n    \n      for await (const chunk of response) {\n        console.log(chunk.text);\n      }\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    iter := model.GenerateContentStream(ctx, genai.Text(\"Write a story about a magic backpack.\"))\n    for {\n      resp, err := iter.Next()\n      if err == iterator.Done {\n        break\n      }\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp)\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse&key=${GEMINI_API_KEY}\" \\\n      -H 'Content-Type: application/json' \\\n      --no-buffer \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Explain how AI works\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { text: 'Explain how AI works' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Multi-turn conversations\n\nThe Gemini SDK lets you collect multiple rounds of questions and responses into a chat. The chat format enables users to step incrementally toward answers and to get help with multipart problems. This SDK implementation of chat provides an interface to keep track of conversation history, but behind the scenes it uses the same [`generateContent`](/api/generate-content#method:-models.generatecontent) method to create the response.\n\nThe following code example shows a basic chat implementation:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    chat = client.chats.create(model=\"gemini-2.0-flash\")\n    \n    response = chat.send_message(\"I have 2 dogs in my house.\")\n    print(response.text)\n    \n    response = chat.send_message(\"How many paws are in my house?\")\n    print(response.text)\n    \n    for message in chat.get_history():\n        print(f'role - {message.role}',end=\": \")\n        print(message.parts[0].text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const chat = ai.chats.create({\n        model: \"gemini-2.0-flash\",\n        history: [\n          {\n            role: \"user\",\n            parts: [{ text: \"Hello\" }],\n          },\n          {\n            role: \"model\",\n            parts: [{ text: \"Great to meet you. What would you like to know?\" }],\n          },\n        ],\n      });\n    \n      const response1 = await chat.sendMessage({\n        message: \"I have 2 dogs in my house.\",\n      });\n      console.log(\"Chat response 1:\", response1.text);\n    \n      const response2 = await chat.sendMessage({\n        message: \"How many paws are in my house?\",\n      });\n      console.log(\"Chat response 2:\", response2.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    cs := model.StartChat()\n    \n    cs.History = []*genai.Content{\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Hello, I have 2 dogs in my house.\"),\n        },\n        Role: \"user\",\n      },\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Great to meet you. What would you like to know?\"),\n        },\n        Role: \"model\",\n      },\n    }\n    \n    res, err := cs.SendMessage(ctx, genai.Text(\"How many paws are in my house?\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    printResponse(res)\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"Hello\"\n              }\n            ]\n          },\n          {\n            \"role\": \"model\",\n            \"parts\": [\n              {\n                \"text\": \"Great to meet you. What would you like to know?\"\n              }\n            ]\n          },\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"I have two dogs in my house. How many paws are in my house?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            role: 'user',\n            parts: [\n              { text: 'Hello' },\n            ],\n          },\n          {\n            role: 'model',\n            parts: [\n              { text: 'Great to meet you. What would you like to know?' },\n            ],\n          },\n          {\n            role: 'user',\n            parts: [\n              { text: 'I have two dogs in my house. How many paws are in my house?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nYou can also use streaming with chat, as shown in the following example:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    chat = client.chats.create(model=\"gemini-2.0-flash\")\n    \n    response = chat.send_message_stream(\"I have 2 dogs in my house.\")\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n    response = chat.send_message_stream(\"How many paws are in my house?\")\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n    for message in chat.get_history():\n        print(f'role - {message.role}', end=\": \")\n        print(message.parts[0].text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const chat = ai.chats.create({\n        model: \"gemini-2.0-flash\",\n        history: [\n          {\n            role: \"user\",\n            parts: [{ text: \"Hello\" }],\n          },\n          {\n            role: \"model\",\n            parts: [{ text: \"Great to meet you. What would you like to know?\" }],\n          },\n        ],\n      });\n    \n      const stream1 = await chat.sendMessageStream({\n        message: \"I have 2 dogs in my house.\",\n      });\n      for await (const chunk of stream1) {\n        console.log(chunk.text);\n        console.log(\"_\".repeat(80));\n      }\n    \n      const stream2 = await chat.sendMessageStream({\n        message: \"How many paws are in my house?\",\n      });\n      for await (const chunk of stream2) {\n        console.log(chunk.text);\n        console.log(\"_\".repeat(80));\n      }\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    cs := model.StartChat()\n    \n    cs.History = []*genai.Content{\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Hello, I have 2 dogs in my house.\"),\n        },\n        Role: \"user\",\n      },\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Great to meet you. What would you like to know?\"),\n        },\n        Role: \"model\",\n      },\n    }\n    \n    iter := cs.SendMessageStream(ctx, genai.Text(\"How many paws are in my house?\"))\n    for {\n      resp, err := iter.Next()\n      if err == iterator.Done {\n        break\n      }\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp)\n    }\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse&key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"Hello\"\n              }\n            ]\n          },\n          {\n            \"role\": \"model\",\n            \"parts\": [\n              {\n                \"text\": \"Great to meet you. What would you like to know?\"\n              }\n            ]\n          },\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"I have two dogs in my house. How many paws are in my house?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            role: 'user',\n            parts: [\n              { text: 'Hello' },\n            ],\n          },\n          {\n            role: 'model',\n            parts: [\n              { text: 'Great to meet you. What would you like to know?' },\n            ],\n          },\n          {\n            role: 'user',\n            parts: [\n              { text: 'I have two dogs in my house. How many paws are in my house?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Configuration parameters\n\nEvery prompt you send to the model includes parameters that control how the model generates responses. You can configure these parameters, or let the model use the default options.\n\nThe following example shows how to configure model parameters:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[\"Explain how AI works\"],\n        config=types.GenerateContentConfig(\n            max_output_tokens=500,\n            temperature=0.1\n        )\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Explain how AI works\",\n        config: {\n          maxOutputTokens: 500,\n          temperature: 0.1,\n        },\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-pro-latest\")\n    model.SetTemperature(0.9)\n    model.SetTopP(0.5)\n    model.SetTopK(20)\n    model.SetMaxOutputTokens(100)\n    model.SystemInstruction = genai.NewUserContent(genai.Text(\"You are Yoda from Star Wars.\"))\n    model.ResponseMIMEType = \"application/json\"\n    resp, err := model.GenerateContent(ctx, genai.Text(\"What is the average size of a swallow?\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    printResponse(resp)\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Explain how AI works\"\n              }\n            ]\n          }\n        ],\n        \"generationConfig\": {\n          \"stopSequences\": [\n            \"Title\"\n          ],\n          \"temperature\": 1.0,\n          \"maxOutputTokens\": 800,\n          \"topP\": 0.8,\n          \"topK\": 10\n        }\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const generationConfig = {\n        temperature: 1,\n        topP: 0.95,\n        topK: 40,\n        maxOutputTokens: 8192,\n        responseMimeType: 'text/plain',\n      };\n    \n      const payload = {\n        generationConfig,\n        contents: [\n          {\n            parts: [\n              { text: 'Explain how AI works in a few words' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nHere are some of the model parameters you can configure. (Naming conventions vary by programming language.)\n\n  * `stopSequences`: Specifies the set of character sequences (up to 5) that will stop output generation. If specified, the API will stop at the first appearance of a `stop_sequence`. The stop sequence won't be included as part of the response.\n  * `temperature`: Controls the randomness of the output. Use higher values for more creative responses, and lower values for more deterministic responses. Values can range from [0.0, 2.0].\n  * `maxOutputTokens`: Sets the maximum number of tokens to include in a candidate.\n  * `topP`: Changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the `topP` value. The default `topP` value is 0.95.\n  * `topK`: Changes how the model selects tokens for output. A `topK` of 1 means the selected token is the most probable among all the tokens in the model's vocabulary, while a `topK` of 3 means that the next token is selected from among the 3 most probable using the temperature. Tokens are further filtered based on `topP` with the final token selected using temperature sampling.\n\n\n\n## System instructions\n\nSystem instructions let you steer the behavior of a model based on your specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full interaction with the user, enabling you to specify product-level behavior separate from the prompts provided by end users.\n\nYou can set system instructions when you initialize your model:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=types.GenerateContentConfig(\n            system_instruction=\"You are a cat. Your name is Neko.\"),\n        contents=\"Hello there\"\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Hello there\",\n        config: {\n          systemInstruction: \"You are a cat. Your name is Neko.\",\n        },\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    // import packages here\n    \n    func main() {\n      ctx := context.Background()\n      client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n      if err != nil {\n        log.Fatal(err)\n      }\n      defer client.Close()\n    \n      model := client.GenerativeModel(\"gemini-2.0-flash\")\n      model.SystemInstruction = &genai.Content{\n        Parts: []genai.Part{genai.Text(`\n          You are a cat. Your name is Neko.\n        `)},\n      }\n      resp, err := model.GenerateContent(ctx, genai.Text(\"Hello there\"))\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp) // helper function for printing content parts\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -d '{\n        \"system_instruction\": {\n          \"parts\": [\n            {\n              \"text\": \"You are a cat. Your name is Neko.\"\n            }\n          ]\n        },\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Hello there\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const systemInstruction = {\n        parts: [{\n          text: 'You are a cat. Your name is Neko.'\n        }]\n      };\n    \n      const payload = {\n        systemInstruction,\n        contents: [\n          {\n            parts: [\n              { text: 'Hello there' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nThen, you can send requests to the model as usual.\n\n## Supported models\n\nThe entire Gemini family of models supports text generation. To learn more about the models and their capabilities, see [Models](/gemini-api/docs/models).\n\n## Prompting tips\n\nFor basic text generation use cases, your prompt might not need to include any output examples, system instructions, or formatting information. This is a [zero-shot](/gemini-api/docs/models/generative-models#zero-shot-prompts) approach. For some use cases, a [one-shot](/gemini-api/docs/models/generative-models#one-shot-prompts) or [few-shot](/gemini-api/docs/models/generative-models#few-shot-prompts) prompt might produce output that's more aligned with user expectations. In some cases, you might also want to provide system instructions to help the model understand the task or follow specific guidelines.\n\n## What's next\n\n  * Try the [Gemini API getting started Colab](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb).\n  * Learn how to use Gemini's [vision understanding](/gemini-api/docs/vision) to process images and videos.\n  * Learn how to use Gemini's [audio understanding](/gemini-api/docs/audio) to process audio files.\n  * Learn about multimodal [file prompting strategies](/gemini-api/docs/file-prompting-strategies).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/downloads",
    "title": "Gemini API Libraries  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini API Libraries \n\nThis page provides information about how to download or install the libraries for accessing the Gemini API. If you're new to the Gemini API, try out the [API quickstart](/gemini-api/docs/quickstart).\n\n## Python\n\nWe provide a [Python SDK](https://pypi.org/project/google-genai) which you can install by running:\n    \n    \n    pip install google-genai\n    \n\n## JavaScript and TypeScript (Preview)\n\nWe provide a [JavaScript and TypeScript SDK](https://www.npmjs.com/package/@google/genai) which you can install by running:\n    \n    \n    npm install @google/genai\n    \n\n## Go (Preview)\n\nWe provide a [Go SDK](https://pkg.go.dev/google.golang.org/genai) which you can install by running:\n    \n    \n    go get https://github.com/googleapis/go-genai\n    \n\n## Previous SDKs\n\nThe following is a set of our previous SDK's which are no longer being actively developed:\n\n  * [Previous Dart and Flutter SDK](https://pub.dev/packages/google_generative_ai/install)\n  * [Previous Swift SDK](https://github.com/google/generative-ai-swift)\n  * [Previous Android SDK](https://github.com/google-gemini/generative-ai-android)\n  * [Previous Python SDK](https://github.com/google-gemini/deprecated-generative-ai-python)\n  * [Previous Node.js SDK](https://github.com/google-gemini/generative-ai-js)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-12 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-12 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/generative-models#zero-shot-prompts",
    "title": "About generative models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  About generative models \n\nGenerative artificial intelligence (AI) models such as the Gemini family of models are able to create content from varying types of data input, including text, images, and audio. At their most basic level, these models operate like sophisticated autocomplete applications. Given input text (\"You can lead a horse to water,\"), a generative model can predict that the output text that's statistically likely to follow (\"but you can't make it drink\"), based on patterns learned from their training data. You can use this basic property of generative models for various applications:\n\n  * Generate creative writing such as poetry, short stories, metaphors, and blog posts\n  * Convert structured data to freeform text\n  * Extract and summarize information from freeform text\n  * Generate code\n  * Translate between languages\n\n\n\nUsing generative models to build these features doesn't require any machine learning (ML) expertise. You don't need to collect a large dataset or train a model. All it takes to start your first prototype is to describe what you want the model to do in a few sentences.\n\nGenerative models that only handle text input and output are also known as _large language models_ (LLMs). The Gemini family of models can process more than text data, and while they can perform the functions of an LLM, they are more properly known as _generative models_.\n\n## Example applications\n\nThe following examples show typical text inputs to, and outputs from, a generative model.\n\n### Generate a poem\n\nUser input: `Write me a four-line poem about puppies and Android phones. Make sure it rhymes.`\n\nModel output:\n    \n    \n    Puppies are cute,\n    Android phones are neat,\n    They both bring joy,\n    And make life complete.\n    \n\n### Generate a list\n\nUser input: `Generate a bulleted list of items I need to pack for a three-day camping trip.`\n\nModel output:\n    \n    \n    *   Tent\n    *   Sleeping bag\n    *   Sleeping pad\n    *   Camp stove\n    *   Pots and pans\n    *   Utensils\n    *   Food\n    *   Water\n    *   First-aid kit\n    *   Sunscreen\n    *   Insect repellent\n    *   ...\n    \n\nYou can get generative models to produce all sorts of useful behaviors like this, just by crafting the right input text, also called a **prompt**. The art and science of figuring out the right wording to get generative models to do what you want is called **prompt design** (also called \"prompt engineering\" or simply \"prompting\").\n\n## Prompt design 101\n\nThe previous section showed some examples of prompts that contain an instruction, like 'Write me a poem'. This kind of instruction may work well for certain types of tasks. However, for other applications, another prompting technique called **few-shot prompting** may work better. Few-shot prompts take advantage of the fact that large language models are incredibly good at recognizing and replicating patterns in text data. The idea is to send the generative model a text pattern that it learns to complete. For example, say you want to build an application that takes as input a country name and outputs its capital city. Here's a text prompt designed to do just that:\n    \n    \n    Italy : Rome\n    France : Paris\n    Germany :\n    \n\nIn this prompt, you establish a pattern: `[country] : [capital]`. If you send this prompt to a large language model, it will autocomplete the pattern and return something like this:\n    \n    \n         Berlin\n    Turkey : Ankara\n    Greece : Athens\n    \n\nThis model response may look a little strange. The model returned not only the capital of Germany (the last country in your hand-written prompt), but also a whole list of additional country and capital pairs. That's because the generative model is \"continuing the pattern.\" If all you're trying to do is build a function that tells you the capital of an input country (\"Germany : Berlin\"), you probably don't really care about any of the text the model generates after \"Berlin.\" Indeed, as application designers, you'd probably want to truncate those extraneous examples. What's more, you'd probably want to **parameterize** the input, so that Germany is not a fixed string but a variable that the end user provides:\n    \n    \n    Italy : Rome\n    France : Paris\n    <user input here> :\n    \n\nYou have just written a few-shot prompt for generating country capitals.\n\nYou can accomplish a large number of tasks by following this **few-shot prompt** template. Here's a few-shot prompt with a slightly different format that converts Python to JavaScript:\n    \n    \n    Convert Python to JavaScript.\n    Python: print(\"hello world\")\n    JavaScript: console.log(\"hello world\")\n    Python: for x in range(0, 100):\n    JavaScript: for(var i = 0; i < 100; i++) {\n    Python: ${USER INPUT HERE}\n    JavaScript:\n    \n\nOr, take this \"reverse dictionary\" prompt. Given a definition, it returns the word that fits that definition:\n    \n    \n    Given a definition, return the word it defines.\n    Definition: When you're happy that other people are also sad.\n    Word: schadenfreude\n    Definition: existing purely in the mind, but not in physical reality\n    Word: abstract\n    Definition: ${USER INPUT HERE}\n    Word:\n    \n\nYou might have noticed that the exact pattern of these few-shot prompts varies slightly. In addition to containing examples, providing instructions in your prompts is an additional strategy to consider when writing your own prompts, as it helps to communicate your intent to the model.\n\n### Prompting versus traditional software development\n\nUnlike traditional software that's designed to a carefully written spec, the behavior of generative models is largely opaque even to the model trainers. As a result, you often can't predict in advance what types of prompt structures will work best for a particular model. What's more, the behavior of a generative model is determined in large part by its training data, and since models are continually tuned on new datasets, sometimes the model changes enough that it inadvertently changes which prompt structures work best. What does this mean for you? Experiment! Try different prompt formats.\n\n## Model parameters\n\nEvery prompt you send to the model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values. The most common model parameters are:\n\n  1. **Max output tokens:** Specifies the maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.\n\n  2. **Temperature:** The temperature controls the degree of randomness in token selection. The temperature is used for sampling during response generation, which occurs when `topP` and `topK` are applied. Lower temperatures are good for prompts that require a more deterministic or less open-ended response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected.\n\n  3. **`topK`:** The `topK` parameter changes how the model selects tokens for output. A `topK` of 1 means the selected token is the most probable among all the tokens in the model's vocabulary (also called greedy decoding), while a `topK` of 3 means that the next token is selected from among the 3 most probable using the temperature. For each token selection step, the `topK` tokens with the highest probabilities are sampled. Tokens are then further filtered based on `topP` with the final token selected using temperature sampling.\n\n  4. **`topP`:** The `topP` parameter changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the `topP` value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the `topP` value is 0.5, then the model will select either A or B as the next token by using the temperature and exclude C as a candidate. The default `topP` value is 0.95.\n\n  5. **`stop_sequences`:** Set a stop sequence to tell the model to stop generating content. A stop sequence can be any sequence of characters. Try to avoid using a sequence of characters that may appear in the generated content.\n\n\n\n\n## Types of prompts\n\nDepending on the level of contextual information contained in them, prompts are broadly classified into three types.\n\n### Zero-shot prompts\n\nThese prompts don't contain examples for the model to replicate. Zero-shot prompts essentially show the model's ability to complete the prompt without any additional examples or information. It means the model has to rely on its pre-existing knowledge to generate a plausible answer.\n\nSome commonly used zero-shot prompt patterns are:\n\n  * Instruction-content\n\n\n    \n    \n    <Overall instruction>\n    <Content to operate on>\n    \n\nFor example,\n    \n    \n    Summarize the following into two sentences at the third-grade level:\n    \n    Hummingbirds are the smallest birds in the world, and they are also one of the\n    most fascinating. They are found in North and South America, and they are known\n    for their long, thin beaks and their ability to fly at high speeds.\n    \n    Hummingbirds are made up of three main parts: the head, the body, and the tail.\n    The head is small and round, and it contains the eyes, the beak, and the brain.\n    The body is long and slender, and it contains the wings, the legs, and the\n    heart. The tail is long and forked, and it helps the hummingbird to balance\n    while it is flying.\n    \n    Hummingbirds are also known for their coloration. They come in a variety of\n    colors, including green, blue, red, and purple. Some hummingbirds are even able\n    to change their color!\n    \n    Hummingbirds are very active creatures. They spend most of their time flying,\n    and they are also very good at hovering. Hummingbirds need to eat a lot of food\n    in order to maintain their energy, and they often visit flowers to drink nectar.\n    \n    Hummingbirds are amazing creatures. They are small, but they are also very\n    powerful. They are beautiful, and they are very important to the ecosystem.\n    \n\n  * Instruction-content-instruction\n\n\n    \n    \n    <Overall instruction or context setting>\n    <Content to operate on>\n    <Final instruction>\n    \n\nFor example,\n    \n    \n    Here is some text I'd like you to summarize:\n    \n    Hummingbirds are the smallest birds in the world, and they are also one of the\n    most fascinating. They are found in North and South America, and they are known\n    for their long, thin beaks and their ability to fly at high speeds. Hummingbirds\n    are made up of three main parts: the head, the body, and the tail. The head is\n    small and round, and it contains the eyes, the beak, and the brain. The body is\n    long and slender, and it contains the wings, the legs, and the heart. The tail\n    is long and forked, and it helps the hummingbird to balance while it is flying.\n    Hummingbirds are also known for their coloration. They come in a variety of\n    colors, including green, blue, red, and purple. Some hummingbirds are even able\n    to change their color! Hummingbirds are very active creatures. They spend most\n    of their time flying, and they are also very good at hovering. Hummingbirds need\n    to eat a lot of food in order to maintain their energy, and they often visit\n    flowers to drink nectar. Hummingbirds are amazing creatures. They are small, but\n    they are also very powerful. They are beautiful, and they are very important to\n    the ecosystem.\n    \n    Summarize it in two sentences at the third-grade reading level.\n    \n\n  * Continuation. Sometimes, you can have the model continue text without any instructions. For example, here is a zero-shot prompt where the model is intended to continue the input provided:\n\n\n    \n    \n    Once upon a time, there was a little sparrow building a nest in a farmer's\n    barn. This sparrow\n    \n\nUse zero-shot prompts to generate creative text formats, such as poems, code, scripts, musical pieces, email, or letters.\n\n### One-shot prompts\n\nThese prompts provide the model with a single example to replicate and continue the pattern. This allows for the generation of predictable responses from the model.\n\nFor example, you can generate food pairings like:\n    \n    \n    Food: Apple\n    Pairs with: Cheese\n    Food: Pear\n    Pairs with:\n    \n\n### Few-shot prompts\n\nThese prompts provide the model with multiple examples to replicate. Use few-shot prompts to complete complicated tasks, such as synthesizing data based on a pattern.\n\nAn example prompt may be:\n    \n    \n    Generate a grocery shopping list for a week for one person. Use the JSON format\n    given below.\n    {\"item\": \"eggs\", \"quantity\": \"6\"}\n    {\"item\": \"bread\", \"quantity\": \"one loaf\"}\n    \n\n## Generative models under the hood\n\nThis section aims to answer the question - **_Is there randomness in generative models' responses, or are they deterministic?_**\n\nThe short answer - yes to both. When you prompt a generative model, a text response is generated in two stages. In the first stage, the generative model processes the input prompt and generates a **probability distribution** over possible tokens (words) that are likely to come next. For example, if you prompt with the input text \"The dog jumped over the ... \", the generative model will produce an array of probable next words:\n    \n    \n    [(\"fence\", 0.77), (\"ledge\", 0.12), (\"blanket\", 0.03), ...]\n    \n\nThis process is deterministic; a generative model will produce this same distribution every time it's input the same prompt text.\n\nIn the second stage, the generative model converts these distributions into actual text responses through one of several decoding strategies. A simple decoding strategy might select the most likely token at every timestep. This process would always be deterministic. However, you could instead choose to generate a response by _randomly sampling_ over the distribution returned by the model. This process would be stochastic (random). Control the degree of randomness allowed in this decoding process by setting the temperature. A temperature of 0 means only the most likely tokens are selected, and there's no randomness. Conversely, a high temperature injects a high degree of randomness into the tokens selected by the model, leading to more unexpected, surprising model responses.\n\n## Further reading\n\n  * Now that you have a deeper understanding of prompts and generative models, try writing your own prompts using [Google AI Studio](https://aistudio.google.com).\n  * Refer to the [Prompt guidelines](/gemini-api/docs/prompting-intro) to learn more about best practices for creating prompts.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/generative-models#one-shot-prompts",
    "title": "About generative models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  About generative models \n\nGenerative artificial intelligence (AI) models such as the Gemini family of models are able to create content from varying types of data input, including text, images, and audio. At their most basic level, these models operate like sophisticated autocomplete applications. Given input text (\"You can lead a horse to water,\"), a generative model can predict that the output text that's statistically likely to follow (\"but you can't make it drink\"), based on patterns learned from their training data. You can use this basic property of generative models for various applications:\n\n  * Generate creative writing such as poetry, short stories, metaphors, and blog posts\n  * Convert structured data to freeform text\n  * Extract and summarize information from freeform text\n  * Generate code\n  * Translate between languages\n\n\n\nUsing generative models to build these features doesn't require any machine learning (ML) expertise. You don't need to collect a large dataset or train a model. All it takes to start your first prototype is to describe what you want the model to do in a few sentences.\n\nGenerative models that only handle text input and output are also known as _large language models_ (LLMs). The Gemini family of models can process more than text data, and while they can perform the functions of an LLM, they are more properly known as _generative models_.\n\n## Example applications\n\nThe following examples show typical text inputs to, and outputs from, a generative model.\n\n### Generate a poem\n\nUser input: `Write me a four-line poem about puppies and Android phones. Make sure it rhymes.`\n\nModel output:\n    \n    \n    Puppies are cute,\n    Android phones are neat,\n    They both bring joy,\n    And make life complete.\n    \n\n### Generate a list\n\nUser input: `Generate a bulleted list of items I need to pack for a three-day camping trip.`\n\nModel output:\n    \n    \n    *   Tent\n    *   Sleeping bag\n    *   Sleeping pad\n    *   Camp stove\n    *   Pots and pans\n    *   Utensils\n    *   Food\n    *   Water\n    *   First-aid kit\n    *   Sunscreen\n    *   Insect repellent\n    *   ...\n    \n\nYou can get generative models to produce all sorts of useful behaviors like this, just by crafting the right input text, also called a **prompt**. The art and science of figuring out the right wording to get generative models to do what you want is called **prompt design** (also called \"prompt engineering\" or simply \"prompting\").\n\n## Prompt design 101\n\nThe previous section showed some examples of prompts that contain an instruction, like 'Write me a poem'. This kind of instruction may work well for certain types of tasks. However, for other applications, another prompting technique called **few-shot prompting** may work better. Few-shot prompts take advantage of the fact that large language models are incredibly good at recognizing and replicating patterns in text data. The idea is to send the generative model a text pattern that it learns to complete. For example, say you want to build an application that takes as input a country name and outputs its capital city. Here's a text prompt designed to do just that:\n    \n    \n    Italy : Rome\n    France : Paris\n    Germany :\n    \n\nIn this prompt, you establish a pattern: `[country] : [capital]`. If you send this prompt to a large language model, it will autocomplete the pattern and return something like this:\n    \n    \n         Berlin\n    Turkey : Ankara\n    Greece : Athens\n    \n\nThis model response may look a little strange. The model returned not only the capital of Germany (the last country in your hand-written prompt), but also a whole list of additional country and capital pairs. That's because the generative model is \"continuing the pattern.\" If all you're trying to do is build a function that tells you the capital of an input country (\"Germany : Berlin\"), you probably don't really care about any of the text the model generates after \"Berlin.\" Indeed, as application designers, you'd probably want to truncate those extraneous examples. What's more, you'd probably want to **parameterize** the input, so that Germany is not a fixed string but a variable that the end user provides:\n    \n    \n    Italy : Rome\n    France : Paris\n    <user input here> :\n    \n\nYou have just written a few-shot prompt for generating country capitals.\n\nYou can accomplish a large number of tasks by following this **few-shot prompt** template. Here's a few-shot prompt with a slightly different format that converts Python to JavaScript:\n    \n    \n    Convert Python to JavaScript.\n    Python: print(\"hello world\")\n    JavaScript: console.log(\"hello world\")\n    Python: for x in range(0, 100):\n    JavaScript: for(var i = 0; i < 100; i++) {\n    Python: ${USER INPUT HERE}\n    JavaScript:\n    \n\nOr, take this \"reverse dictionary\" prompt. Given a definition, it returns the word that fits that definition:\n    \n    \n    Given a definition, return the word it defines.\n    Definition: When you're happy that other people are also sad.\n    Word: schadenfreude\n    Definition: existing purely in the mind, but not in physical reality\n    Word: abstract\n    Definition: ${USER INPUT HERE}\n    Word:\n    \n\nYou might have noticed that the exact pattern of these few-shot prompts varies slightly. In addition to containing examples, providing instructions in your prompts is an additional strategy to consider when writing your own prompts, as it helps to communicate your intent to the model.\n\n### Prompting versus traditional software development\n\nUnlike traditional software that's designed to a carefully written spec, the behavior of generative models is largely opaque even to the model trainers. As a result, you often can't predict in advance what types of prompt structures will work best for a particular model. What's more, the behavior of a generative model is determined in large part by its training data, and since models are continually tuned on new datasets, sometimes the model changes enough that it inadvertently changes which prompt structures work best. What does this mean for you? Experiment! Try different prompt formats.\n\n## Model parameters\n\nEvery prompt you send to the model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values. The most common model parameters are:\n\n  1. **Max output tokens:** Specifies the maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.\n\n  2. **Temperature:** The temperature controls the degree of randomness in token selection. The temperature is used for sampling during response generation, which occurs when `topP` and `topK` are applied. Lower temperatures are good for prompts that require a more deterministic or less open-ended response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected.\n\n  3. **`topK`:** The `topK` parameter changes how the model selects tokens for output. A `topK` of 1 means the selected token is the most probable among all the tokens in the model's vocabulary (also called greedy decoding), while a `topK` of 3 means that the next token is selected from among the 3 most probable using the temperature. For each token selection step, the `topK` tokens with the highest probabilities are sampled. Tokens are then further filtered based on `topP` with the final token selected using temperature sampling.\n\n  4. **`topP`:** The `topP` parameter changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the `topP` value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the `topP` value is 0.5, then the model will select either A or B as the next token by using the temperature and exclude C as a candidate. The default `topP` value is 0.95.\n\n  5. **`stop_sequences`:** Set a stop sequence to tell the model to stop generating content. A stop sequence can be any sequence of characters. Try to avoid using a sequence of characters that may appear in the generated content.\n\n\n\n\n## Types of prompts\n\nDepending on the level of contextual information contained in them, prompts are broadly classified into three types.\n\n### Zero-shot prompts\n\nThese prompts don't contain examples for the model to replicate. Zero-shot prompts essentially show the model's ability to complete the prompt without any additional examples or information. It means the model has to rely on its pre-existing knowledge to generate a plausible answer.\n\nSome commonly used zero-shot prompt patterns are:\n\n  * Instruction-content\n\n\n    \n    \n    <Overall instruction>\n    <Content to operate on>\n    \n\nFor example,\n    \n    \n    Summarize the following into two sentences at the third-grade level:\n    \n    Hummingbirds are the smallest birds in the world, and they are also one of the\n    most fascinating. They are found in North and South America, and they are known\n    for their long, thin beaks and their ability to fly at high speeds.\n    \n    Hummingbirds are made up of three main parts: the head, the body, and the tail.\n    The head is small and round, and it contains the eyes, the beak, and the brain.\n    The body is long and slender, and it contains the wings, the legs, and the\n    heart. The tail is long and forked, and it helps the hummingbird to balance\n    while it is flying.\n    \n    Hummingbirds are also known for their coloration. They come in a variety of\n    colors, including green, blue, red, and purple. Some hummingbirds are even able\n    to change their color!\n    \n    Hummingbirds are very active creatures. They spend most of their time flying,\n    and they are also very good at hovering. Hummingbirds need to eat a lot of food\n    in order to maintain their energy, and they often visit flowers to drink nectar.\n    \n    Hummingbirds are amazing creatures. They are small, but they are also very\n    powerful. They are beautiful, and they are very important to the ecosystem.\n    \n\n  * Instruction-content-instruction\n\n\n    \n    \n    <Overall instruction or context setting>\n    <Content to operate on>\n    <Final instruction>\n    \n\nFor example,\n    \n    \n    Here is some text I'd like you to summarize:\n    \n    Hummingbirds are the smallest birds in the world, and they are also one of the\n    most fascinating. They are found in North and South America, and they are known\n    for their long, thin beaks and their ability to fly at high speeds. Hummingbirds\n    are made up of three main parts: the head, the body, and the tail. The head is\n    small and round, and it contains the eyes, the beak, and the brain. The body is\n    long and slender, and it contains the wings, the legs, and the heart. The tail\n    is long and forked, and it helps the hummingbird to balance while it is flying.\n    Hummingbirds are also known for their coloration. They come in a variety of\n    colors, including green, blue, red, and purple. Some hummingbirds are even able\n    to change their color! Hummingbirds are very active creatures. They spend most\n    of their time flying, and they are also very good at hovering. Hummingbirds need\n    to eat a lot of food in order to maintain their energy, and they often visit\n    flowers to drink nectar. Hummingbirds are amazing creatures. They are small, but\n    they are also very powerful. They are beautiful, and they are very important to\n    the ecosystem.\n    \n    Summarize it in two sentences at the third-grade reading level.\n    \n\n  * Continuation. Sometimes, you can have the model continue text without any instructions. For example, here is a zero-shot prompt where the model is intended to continue the input provided:\n\n\n    \n    \n    Once upon a time, there was a little sparrow building a nest in a farmer's\n    barn. This sparrow\n    \n\nUse zero-shot prompts to generate creative text formats, such as poems, code, scripts, musical pieces, email, or letters.\n\n### One-shot prompts\n\nThese prompts provide the model with a single example to replicate and continue the pattern. This allows for the generation of predictable responses from the model.\n\nFor example, you can generate food pairings like:\n    \n    \n    Food: Apple\n    Pairs with: Cheese\n    Food: Pear\n    Pairs with:\n    \n\n### Few-shot prompts\n\nThese prompts provide the model with multiple examples to replicate. Use few-shot prompts to complete complicated tasks, such as synthesizing data based on a pattern.\n\nAn example prompt may be:\n    \n    \n    Generate a grocery shopping list for a week for one person. Use the JSON format\n    given below.\n    {\"item\": \"eggs\", \"quantity\": \"6\"}\n    {\"item\": \"bread\", \"quantity\": \"one loaf\"}\n    \n\n## Generative models under the hood\n\nThis section aims to answer the question - **_Is there randomness in generative models' responses, or are they deterministic?_**\n\nThe short answer - yes to both. When you prompt a generative model, a text response is generated in two stages. In the first stage, the generative model processes the input prompt and generates a **probability distribution** over possible tokens (words) that are likely to come next. For example, if you prompt with the input text \"The dog jumped over the ... \", the generative model will produce an array of probable next words:\n    \n    \n    [(\"fence\", 0.77), (\"ledge\", 0.12), (\"blanket\", 0.03), ...]\n    \n\nThis process is deterministic; a generative model will produce this same distribution every time it's input the same prompt text.\n\nIn the second stage, the generative model converts these distributions into actual text responses through one of several decoding strategies. A simple decoding strategy might select the most likely token at every timestep. This process would always be deterministic. However, you could instead choose to generate a response by _randomly sampling_ over the distribution returned by the model. This process would be stochastic (random). Control the degree of randomness allowed in this decoding process by setting the temperature. A temperature of 0 means only the most likely tokens are selected, and there's no randomness. Conversely, a high temperature injects a high degree of randomness into the tokens selected by the model, leading to more unexpected, surprising model responses.\n\n## Further reading\n\n  * Now that you have a deeper understanding of prompts and generative models, try writing your own prompts using [Google AI Studio](https://aistudio.google.com).\n  * Refer to the [Prompt guidelines](/gemini-api/docs/prompting-intro) to learn more about best practices for creating prompts.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/generative-models#few-shot-prompts",
    "title": "About generative models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  About generative models \n\nGenerative artificial intelligence (AI) models such as the Gemini family of models are able to create content from varying types of data input, including text, images, and audio. At their most basic level, these models operate like sophisticated autocomplete applications. Given input text (\"You can lead a horse to water,\"), a generative model can predict that the output text that's statistically likely to follow (\"but you can't make it drink\"), based on patterns learned from their training data. You can use this basic property of generative models for various applications:\n\n  * Generate creative writing such as poetry, short stories, metaphors, and blog posts\n  * Convert structured data to freeform text\n  * Extract and summarize information from freeform text\n  * Generate code\n  * Translate between languages\n\n\n\nUsing generative models to build these features doesn't require any machine learning (ML) expertise. You don't need to collect a large dataset or train a model. All it takes to start your first prototype is to describe what you want the model to do in a few sentences.\n\nGenerative models that only handle text input and output are also known as _large language models_ (LLMs). The Gemini family of models can process more than text data, and while they can perform the functions of an LLM, they are more properly known as _generative models_.\n\n## Example applications\n\nThe following examples show typical text inputs to, and outputs from, a generative model.\n\n### Generate a poem\n\nUser input: `Write me a four-line poem about puppies and Android phones. Make sure it rhymes.`\n\nModel output:\n    \n    \n    Puppies are cute,\n    Android phones are neat,\n    They both bring joy,\n    And make life complete.\n    \n\n### Generate a list\n\nUser input: `Generate a bulleted list of items I need to pack for a three-day camping trip.`\n\nModel output:\n    \n    \n    *   Tent\n    *   Sleeping bag\n    *   Sleeping pad\n    *   Camp stove\n    *   Pots and pans\n    *   Utensils\n    *   Food\n    *   Water\n    *   First-aid kit\n    *   Sunscreen\n    *   Insect repellent\n    *   ...\n    \n\nYou can get generative models to produce all sorts of useful behaviors like this, just by crafting the right input text, also called a **prompt**. The art and science of figuring out the right wording to get generative models to do what you want is called **prompt design** (also called \"prompt engineering\" or simply \"prompting\").\n\n## Prompt design 101\n\nThe previous section showed some examples of prompts that contain an instruction, like 'Write me a poem'. This kind of instruction may work well for certain types of tasks. However, for other applications, another prompting technique called **few-shot prompting** may work better. Few-shot prompts take advantage of the fact that large language models are incredibly good at recognizing and replicating patterns in text data. The idea is to send the generative model a text pattern that it learns to complete. For example, say you want to build an application that takes as input a country name and outputs its capital city. Here's a text prompt designed to do just that:\n    \n    \n    Italy : Rome\n    France : Paris\n    Germany :\n    \n\nIn this prompt, you establish a pattern: `[country] : [capital]`. If you send this prompt to a large language model, it will autocomplete the pattern and return something like this:\n    \n    \n         Berlin\n    Turkey : Ankara\n    Greece : Athens\n    \n\nThis model response may look a little strange. The model returned not only the capital of Germany (the last country in your hand-written prompt), but also a whole list of additional country and capital pairs. That's because the generative model is \"continuing the pattern.\" If all you're trying to do is build a function that tells you the capital of an input country (\"Germany : Berlin\"), you probably don't really care about any of the text the model generates after \"Berlin.\" Indeed, as application designers, you'd probably want to truncate those extraneous examples. What's more, you'd probably want to **parameterize** the input, so that Germany is not a fixed string but a variable that the end user provides:\n    \n    \n    Italy : Rome\n    France : Paris\n    <user input here> :\n    \n\nYou have just written a few-shot prompt for generating country capitals.\n\nYou can accomplish a large number of tasks by following this **few-shot prompt** template. Here's a few-shot prompt with a slightly different format that converts Python to JavaScript:\n    \n    \n    Convert Python to JavaScript.\n    Python: print(\"hello world\")\n    JavaScript: console.log(\"hello world\")\n    Python: for x in range(0, 100):\n    JavaScript: for(var i = 0; i < 100; i++) {\n    Python: ${USER INPUT HERE}\n    JavaScript:\n    \n\nOr, take this \"reverse dictionary\" prompt. Given a definition, it returns the word that fits that definition:\n    \n    \n    Given a definition, return the word it defines.\n    Definition: When you're happy that other people are also sad.\n    Word: schadenfreude\n    Definition: existing purely in the mind, but not in physical reality\n    Word: abstract\n    Definition: ${USER INPUT HERE}\n    Word:\n    \n\nYou might have noticed that the exact pattern of these few-shot prompts varies slightly. In addition to containing examples, providing instructions in your prompts is an additional strategy to consider when writing your own prompts, as it helps to communicate your intent to the model.\n\n### Prompting versus traditional software development\n\nUnlike traditional software that's designed to a carefully written spec, the behavior of generative models is largely opaque even to the model trainers. As a result, you often can't predict in advance what types of prompt structures will work best for a particular model. What's more, the behavior of a generative model is determined in large part by its training data, and since models are continually tuned on new datasets, sometimes the model changes enough that it inadvertently changes which prompt structures work best. What does this mean for you? Experiment! Try different prompt formats.\n\n## Model parameters\n\nEvery prompt you send to the model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values. The most common model parameters are:\n\n  1. **Max output tokens:** Specifies the maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.\n\n  2. **Temperature:** The temperature controls the degree of randomness in token selection. The temperature is used for sampling during response generation, which occurs when `topP` and `topK` are applied. Lower temperatures are good for prompts that require a more deterministic or less open-ended response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected.\n\n  3. **`topK`:** The `topK` parameter changes how the model selects tokens for output. A `topK` of 1 means the selected token is the most probable among all the tokens in the model's vocabulary (also called greedy decoding), while a `topK` of 3 means that the next token is selected from among the 3 most probable using the temperature. For each token selection step, the `topK` tokens with the highest probabilities are sampled. Tokens are then further filtered based on `topP` with the final token selected using temperature sampling.\n\n  4. **`topP`:** The `topP` parameter changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the `topP` value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the `topP` value is 0.5, then the model will select either A or B as the next token by using the temperature and exclude C as a candidate. The default `topP` value is 0.95.\n\n  5. **`stop_sequences`:** Set a stop sequence to tell the model to stop generating content. A stop sequence can be any sequence of characters. Try to avoid using a sequence of characters that may appear in the generated content.\n\n\n\n\n## Types of prompts\n\nDepending on the level of contextual information contained in them, prompts are broadly classified into three types.\n\n### Zero-shot prompts\n\nThese prompts don't contain examples for the model to replicate. Zero-shot prompts essentially show the model's ability to complete the prompt without any additional examples or information. It means the model has to rely on its pre-existing knowledge to generate a plausible answer.\n\nSome commonly used zero-shot prompt patterns are:\n\n  * Instruction-content\n\n\n    \n    \n    <Overall instruction>\n    <Content to operate on>\n    \n\nFor example,\n    \n    \n    Summarize the following into two sentences at the third-grade level:\n    \n    Hummingbirds are the smallest birds in the world, and they are also one of the\n    most fascinating. They are found in North and South America, and they are known\n    for their long, thin beaks and their ability to fly at high speeds.\n    \n    Hummingbirds are made up of three main parts: the head, the body, and the tail.\n    The head is small and round, and it contains the eyes, the beak, and the brain.\n    The body is long and slender, and it contains the wings, the legs, and the\n    heart. The tail is long and forked, and it helps the hummingbird to balance\n    while it is flying.\n    \n    Hummingbirds are also known for their coloration. They come in a variety of\n    colors, including green, blue, red, and purple. Some hummingbirds are even able\n    to change their color!\n    \n    Hummingbirds are very active creatures. They spend most of their time flying,\n    and they are also very good at hovering. Hummingbirds need to eat a lot of food\n    in order to maintain their energy, and they often visit flowers to drink nectar.\n    \n    Hummingbirds are amazing creatures. They are small, but they are also very\n    powerful. They are beautiful, and they are very important to the ecosystem.\n    \n\n  * Instruction-content-instruction\n\n\n    \n    \n    <Overall instruction or context setting>\n    <Content to operate on>\n    <Final instruction>\n    \n\nFor example,\n    \n    \n    Here is some text I'd like you to summarize:\n    \n    Hummingbirds are the smallest birds in the world, and they are also one of the\n    most fascinating. They are found in North and South America, and they are known\n    for their long, thin beaks and their ability to fly at high speeds. Hummingbirds\n    are made up of three main parts: the head, the body, and the tail. The head is\n    small and round, and it contains the eyes, the beak, and the brain. The body is\n    long and slender, and it contains the wings, the legs, and the heart. The tail\n    is long and forked, and it helps the hummingbird to balance while it is flying.\n    Hummingbirds are also known for their coloration. They come in a variety of\n    colors, including green, blue, red, and purple. Some hummingbirds are even able\n    to change their color! Hummingbirds are very active creatures. They spend most\n    of their time flying, and they are also very good at hovering. Hummingbirds need\n    to eat a lot of food in order to maintain their energy, and they often visit\n    flowers to drink nectar. Hummingbirds are amazing creatures. They are small, but\n    they are also very powerful. They are beautiful, and they are very important to\n    the ecosystem.\n    \n    Summarize it in two sentences at the third-grade reading level.\n    \n\n  * Continuation. Sometimes, you can have the model continue text without any instructions. For example, here is a zero-shot prompt where the model is intended to continue the input provided:\n\n\n    \n    \n    Once upon a time, there was a little sparrow building a nest in a farmer's\n    barn. This sparrow\n    \n\nUse zero-shot prompts to generate creative text formats, such as poems, code, scripts, musical pieces, email, or letters.\n\n### One-shot prompts\n\nThese prompts provide the model with a single example to replicate and continue the pattern. This allows for the generation of predictable responses from the model.\n\nFor example, you can generate food pairings like:\n    \n    \n    Food: Apple\n    Pairs with: Cheese\n    Food: Pear\n    Pairs with:\n    \n\n### Few-shot prompts\n\nThese prompts provide the model with multiple examples to replicate. Use few-shot prompts to complete complicated tasks, such as synthesizing data based on a pattern.\n\nAn example prompt may be:\n    \n    \n    Generate a grocery shopping list for a week for one person. Use the JSON format\n    given below.\n    {\"item\": \"eggs\", \"quantity\": \"6\"}\n    {\"item\": \"bread\", \"quantity\": \"one loaf\"}\n    \n\n## Generative models under the hood\n\nThis section aims to answer the question - **_Is there randomness in generative models' responses, or are they deterministic?_**\n\nThe short answer - yes to both. When you prompt a generative model, a text response is generated in two stages. In the first stage, the generative model processes the input prompt and generates a **probability distribution** over possible tokens (words) that are likely to come next. For example, if you prompt with the input text \"The dog jumped over the ... \", the generative model will produce an array of probable next words:\n    \n    \n    [(\"fence\", 0.77), (\"ledge\", 0.12), (\"blanket\", 0.03), ...]\n    \n\nThis process is deterministic; a generative model will produce this same distribution every time it's input the same prompt text.\n\nIn the second stage, the generative model converts these distributions into actual text responses through one of several decoding strategies. A simple decoding strategy might select the most likely token at every timestep. This process would always be deterministic. However, you could instead choose to generate a response by _randomly sampling_ over the distribution returned by the model. This process would be stochastic (random). Control the degree of randomness allowed in this decoding process by setting the temperature. A temperature of 0 means only the most likely tokens are selected, and there's no randomness. Conversely, a high temperature injects a high degree of randomness into the tokens selected by the model, leading to more unexpected, surprising model responses.\n\n## Further reading\n\n  * Now that you have a deeper understanding of prompts and generative models, try writing your own prompts using [Google AI Studio](https://aistudio.google.com).\n  * Refer to the [Prompt guidelines](/gemini-api/docs/prompting-intro) to learn more about best practices for creating prompts.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/experimental-models",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/generative-models#prompt-types",
    "title": "About generative models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  About generative models \n\nGenerative artificial intelligence (AI) models such as the Gemini family of models are able to create content from varying types of data input, including text, images, and audio. At their most basic level, these models operate like sophisticated autocomplete applications. Given input text (\"You can lead a horse to water,\"), a generative model can predict that the output text that's statistically likely to follow (\"but you can't make it drink\"), based on patterns learned from their training data. You can use this basic property of generative models for various applications:\n\n  * Generate creative writing such as poetry, short stories, metaphors, and blog posts\n  * Convert structured data to freeform text\n  * Extract and summarize information from freeform text\n  * Generate code\n  * Translate between languages\n\n\n\nUsing generative models to build these features doesn't require any machine learning (ML) expertise. You don't need to collect a large dataset or train a model. All it takes to start your first prototype is to describe what you want the model to do in a few sentences.\n\nGenerative models that only handle text input and output are also known as _large language models_ (LLMs). The Gemini family of models can process more than text data, and while they can perform the functions of an LLM, they are more properly known as _generative models_.\n\n## Example applications\n\nThe following examples show typical text inputs to, and outputs from, a generative model.\n\n### Generate a poem\n\nUser input: `Write me a four-line poem about puppies and Android phones. Make sure it rhymes.`\n\nModel output:\n    \n    \n    Puppies are cute,\n    Android phones are neat,\n    They both bring joy,\n    And make life complete.\n    \n\n### Generate a list\n\nUser input: `Generate a bulleted list of items I need to pack for a three-day camping trip.`\n\nModel output:\n    \n    \n    *   Tent\n    *   Sleeping bag\n    *   Sleeping pad\n    *   Camp stove\n    *   Pots and pans\n    *   Utensils\n    *   Food\n    *   Water\n    *   First-aid kit\n    *   Sunscreen\n    *   Insect repellent\n    *   ...\n    \n\nYou can get generative models to produce all sorts of useful behaviors like this, just by crafting the right input text, also called a **prompt**. The art and science of figuring out the right wording to get generative models to do what you want is called **prompt design** (also called \"prompt engineering\" or simply \"prompting\").\n\n## Prompt design 101\n\nThe previous section showed some examples of prompts that contain an instruction, like 'Write me a poem'. This kind of instruction may work well for certain types of tasks. However, for other applications, another prompting technique called **few-shot prompting** may work better. Few-shot prompts take advantage of the fact that large language models are incredibly good at recognizing and replicating patterns in text data. The idea is to send the generative model a text pattern that it learns to complete. For example, say you want to build an application that takes as input a country name and outputs its capital city. Here's a text prompt designed to do just that:\n    \n    \n    Italy : Rome\n    France : Paris\n    Germany :\n    \n\nIn this prompt, you establish a pattern: `[country] : [capital]`. If you send this prompt to a large language model, it will autocomplete the pattern and return something like this:\n    \n    \n         Berlin\n    Turkey : Ankara\n    Greece : Athens\n    \n\nThis model response may look a little strange. The model returned not only the capital of Germany (the last country in your hand-written prompt), but also a whole list of additional country and capital pairs. That's because the generative model is \"continuing the pattern.\" If all you're trying to do is build a function that tells you the capital of an input country (\"Germany : Berlin\"), you probably don't really care about any of the text the model generates after \"Berlin.\" Indeed, as application designers, you'd probably want to truncate those extraneous examples. What's more, you'd probably want to **parameterize** the input, so that Germany is not a fixed string but a variable that the end user provides:\n    \n    \n    Italy : Rome\n    France : Paris\n    <user input here> :\n    \n\nYou have just written a few-shot prompt for generating country capitals.\n\nYou can accomplish a large number of tasks by following this **few-shot prompt** template. Here's a few-shot prompt with a slightly different format that converts Python to JavaScript:\n    \n    \n    Convert Python to JavaScript.\n    Python: print(\"hello world\")\n    JavaScript: console.log(\"hello world\")\n    Python: for x in range(0, 100):\n    JavaScript: for(var i = 0; i < 100; i++) {\n    Python: ${USER INPUT HERE}\n    JavaScript:\n    \n\nOr, take this \"reverse dictionary\" prompt. Given a definition, it returns the word that fits that definition:\n    \n    \n    Given a definition, return the word it defines.\n    Definition: When you're happy that other people are also sad.\n    Word: schadenfreude\n    Definition: existing purely in the mind, but not in physical reality\n    Word: abstract\n    Definition: ${USER INPUT HERE}\n    Word:\n    \n\nYou might have noticed that the exact pattern of these few-shot prompts varies slightly. In addition to containing examples, providing instructions in your prompts is an additional strategy to consider when writing your own prompts, as it helps to communicate your intent to the model.\n\n### Prompting versus traditional software development\n\nUnlike traditional software that's designed to a carefully written spec, the behavior of generative models is largely opaque even to the model trainers. As a result, you often can't predict in advance what types of prompt structures will work best for a particular model. What's more, the behavior of a generative model is determined in large part by its training data, and since models are continually tuned on new datasets, sometimes the model changes enough that it inadvertently changes which prompt structures work best. What does this mean for you? Experiment! Try different prompt formats.\n\n## Model parameters\n\nEvery prompt you send to the model includes parameter values that control how the model generates a response. The model can generate different results for different parameter values. The most common model parameters are:\n\n  1. **Max output tokens:** Specifies the maximum number of tokens that can be generated in the response. A token is approximately four characters. 100 tokens correspond to roughly 60-80 words.\n\n  2. **Temperature:** The temperature controls the degree of randomness in token selection. The temperature is used for sampling during response generation, which occurs when `topP` and `topK` are applied. Lower temperatures are good for prompts that require a more deterministic or less open-ended response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected.\n\n  3. **`topK`:** The `topK` parameter changes how the model selects tokens for output. A `topK` of 1 means the selected token is the most probable among all the tokens in the model's vocabulary (also called greedy decoding), while a `topK` of 3 means that the next token is selected from among the 3 most probable using the temperature. For each token selection step, the `topK` tokens with the highest probabilities are sampled. Tokens are then further filtered based on `topP` with the final token selected using temperature sampling.\n\n  4. **`topP`:** The `topP` parameter changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the `topP` value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the `topP` value is 0.5, then the model will select either A or B as the next token by using the temperature and exclude C as a candidate. The default `topP` value is 0.95.\n\n  5. **`stop_sequences`:** Set a stop sequence to tell the model to stop generating content. A stop sequence can be any sequence of characters. Try to avoid using a sequence of characters that may appear in the generated content.\n\n\n\n\n## Types of prompts\n\nDepending on the level of contextual information contained in them, prompts are broadly classified into three types.\n\n### Zero-shot prompts\n\nThese prompts don't contain examples for the model to replicate. Zero-shot prompts essentially show the model's ability to complete the prompt without any additional examples or information. It means the model has to rely on its pre-existing knowledge to generate a plausible answer.\n\nSome commonly used zero-shot prompt patterns are:\n\n  * Instruction-content\n\n\n    \n    \n    <Overall instruction>\n    <Content to operate on>\n    \n\nFor example,\n    \n    \n    Summarize the following into two sentences at the third-grade level:\n    \n    Hummingbirds are the smallest birds in the world, and they are also one of the\n    most fascinating. They are found in North and South America, and they are known\n    for their long, thin beaks and their ability to fly at high speeds.\n    \n    Hummingbirds are made up of three main parts: the head, the body, and the tail.\n    The head is small and round, and it contains the eyes, the beak, and the brain.\n    The body is long and slender, and it contains the wings, the legs, and the\n    heart. The tail is long and forked, and it helps the hummingbird to balance\n    while it is flying.\n    \n    Hummingbirds are also known for their coloration. They come in a variety of\n    colors, including green, blue, red, and purple. Some hummingbirds are even able\n    to change their color!\n    \n    Hummingbirds are very active creatures. They spend most of their time flying,\n    and they are also very good at hovering. Hummingbirds need to eat a lot of food\n    in order to maintain their energy, and they often visit flowers to drink nectar.\n    \n    Hummingbirds are amazing creatures. They are small, but they are also very\n    powerful. They are beautiful, and they are very important to the ecosystem.\n    \n\n  * Instruction-content-instruction\n\n\n    \n    \n    <Overall instruction or context setting>\n    <Content to operate on>\n    <Final instruction>\n    \n\nFor example,\n    \n    \n    Here is some text I'd like you to summarize:\n    \n    Hummingbirds are the smallest birds in the world, and they are also one of the\n    most fascinating. They are found in North and South America, and they are known\n    for their long, thin beaks and their ability to fly at high speeds. Hummingbirds\n    are made up of three main parts: the head, the body, and the tail. The head is\n    small and round, and it contains the eyes, the beak, and the brain. The body is\n    long and slender, and it contains the wings, the legs, and the heart. The tail\n    is long and forked, and it helps the hummingbird to balance while it is flying.\n    Hummingbirds are also known for their coloration. They come in a variety of\n    colors, including green, blue, red, and purple. Some hummingbirds are even able\n    to change their color! Hummingbirds are very active creatures. They spend most\n    of their time flying, and they are also very good at hovering. Hummingbirds need\n    to eat a lot of food in order to maintain their energy, and they often visit\n    flowers to drink nectar. Hummingbirds are amazing creatures. They are small, but\n    they are also very powerful. They are beautiful, and they are very important to\n    the ecosystem.\n    \n    Summarize it in two sentences at the third-grade reading level.\n    \n\n  * Continuation. Sometimes, you can have the model continue text without any instructions. For example, here is a zero-shot prompt where the model is intended to continue the input provided:\n\n\n    \n    \n    Once upon a time, there was a little sparrow building a nest in a farmer's\n    barn. This sparrow\n    \n\nUse zero-shot prompts to generate creative text formats, such as poems, code, scripts, musical pieces, email, or letters.\n\n### One-shot prompts\n\nThese prompts provide the model with a single example to replicate and continue the pattern. This allows for the generation of predictable responses from the model.\n\nFor example, you can generate food pairings like:\n    \n    \n    Food: Apple\n    Pairs with: Cheese\n    Food: Pear\n    Pairs with:\n    \n\n### Few-shot prompts\n\nThese prompts provide the model with multiple examples to replicate. Use few-shot prompts to complete complicated tasks, such as synthesizing data based on a pattern.\n\nAn example prompt may be:\n    \n    \n    Generate a grocery shopping list for a week for one person. Use the JSON format\n    given below.\n    {\"item\": \"eggs\", \"quantity\": \"6\"}\n    {\"item\": \"bread\", \"quantity\": \"one loaf\"}\n    \n\n## Generative models under the hood\n\nThis section aims to answer the question - **_Is there randomness in generative models' responses, or are they deterministic?_**\n\nThe short answer - yes to both. When you prompt a generative model, a text response is generated in two stages. In the first stage, the generative model processes the input prompt and generates a **probability distribution** over possible tokens (words) that are likely to come next. For example, if you prompt with the input text \"The dog jumped over the ... \", the generative model will produce an array of probable next words:\n    \n    \n    [(\"fence\", 0.77), (\"ledge\", 0.12), (\"blanket\", 0.03), ...]\n    \n\nThis process is deterministic; a generative model will produce this same distribution every time it's input the same prompt text.\n\nIn the second stage, the generative model converts these distributions into actual text responses through one of several decoding strategies. A simple decoding strategy might select the most likely token at every timestep. This process would always be deterministic. However, you could instead choose to generate a response by _randomly sampling_ over the distribution returned by the model. This process would be stochastic (random). Control the degree of randomness allowed in this decoding process by setting the temperature. A temperature of 0 means only the most likely tokens are selected, and there's no randomness. Conversely, a high temperature injects a high degree of randomness into the tokens selected by the model, leading to more unexpected, surprising model responses.\n\n## Further reading\n\n  * Now that you have a deeper understanding of prompts and generative models, try writing your own prompts using [Google AI Studio](https://aistudio.google.com).\n  * Refer to the [Prompt guidelines](/gemini-api/docs/prompting-intro) to learn more about best practices for creating prompts.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/prompting_with_media",
    "title": "File prompting strategies  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  File prompting strategies \n\nThe Gemini family of artificial intelligence (AI) models is built to handle various types of input data, including text, images, and audio. Since these models can handle more than one type or _mode_ of data, the Gemini models are called _multimodal models_ or explained as having _multimodal capabilities_.\n\n**Note:** Prompting with media files is supported by specific Gemini models. For more information about which models support this feature, see the [Vision](/gemini-api/docs/vision) and [Audio](/gemini-api/docs/audio) guides.\n\nThis page provides guidance and best practices for using media files with prompts for the Gemini API. For information on using the Gemini Files API for uploading media files and including them in your prompts, see the [Vision](/gemini-api/docs/vision), [Audio](/gemini-api/docs/audio), and [Document processing](/gemini-api/docs/document-processing) guides.\n\n## Using media files with prompts\n\nBeing able to use various types of data in your prompts gives you more flexibility in terms of what tasks you can tackle with the Gemini API. For example, you can send the model a photo of a delicious meal and ask it to write a short blog about the meal.\n\n**Prompt** | **Response**  \n---|---  \nWrite a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.   \n|  Meal prepping is a great way to save time and money, and it can also help you to eat healthier. This meal is a great example of a healthy and delicious meal that can be easily prepped ahead of time. This meal features brown rice, roasted vegetables, and chicken teriyaki. The brown rice is a healthy and complex carbohydrate that will give you sustained energy. The roasted vegetables are a great way to get your daily dose of vitamins and minerals, and the chicken teriyaki is a delicious and protein-rich option. This meal is also very easy to prepare. Simply cook the brown rice, roast the vegetables, and cook the chicken teriyaki. Once everything is cooked, divide it into meal prep containers and store them in the refrigerator. You can then grab a container and go on busy mornings! If you're looking for a healthy and delicious meal that can be easily prepped ahead of time, this meal is a great option. It's packed with nutrients and flavor, and it's sure to keep you feeling full and satisfied. Here's to healthy and delicious meal prepping!  \n  \n## Strategies for prompting with media\n\nIf you are having trouble getting the output you want from prompts that use media files, there are some strategies that can help you get the results you want. The following sections provide design approaches and troubleshooting tips for improving prompts that use multimodal input.\n\nYou can improve your multimodal prompts by following these best practices:\n\n  * ### Prompt design fundamentals\n\n    * **Be specific in your instructions** : Craft clear and concise instructions that leave minimal room for misinterpretation.\n    * **Add a few examples to your prompt:** Use realistic few-shot examples to illustrate what you want to achieve.\n    * **Break it down step-by-step** : Divide complex tasks into manageable sub-goals, guiding the model through the process.\n    * **Specify the output format** : In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more. \n    * **Put your image first for single-image prompts** : While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image is placed before the text prompt.\n  * ### Troubleshooting your multimodal prompt\n\n    * **If the model is not drawing information from the relevant part of the image:** Drop hints with which aspects of the image you want the prompt to draw information from.\n    * **If the model output is too generic (not tailored enough to the image):** At the start of the prompt, try asking the model to describe the image(s) before providing the task instruction, or try asking the model to refer to what's in the image.\n\n\n\n## Prompt design fundamentals\n\nThis section expands on the best practices listed in the previous section.\n\nNote: Model results from the following Gemini examples can vary from run to run.\n\n### Be specific in your instructions\n\nPrompts have the most success when they are clear and detailed. If you have a specific output in mind, it's better to include that requirement in the prompt to ensure you get the output you want. \n\nSometimes, a prompt's intent might seem clear to the person who wrote it, but it is actually under-specified. To customize model behavior, consider how your prompt could be (mis)interpreted, and ensure that the instructions you give are specific and clear. Remember to leave minimal room for misinterpretation\n\nFor this image of an airport board, asking the model to just \"describe this image\" could generate a general description. If you need the model to parse the time and city from the image, you can include that request directly in your prompt.\n\nPrompt | Model response  \n---|---  \nDescribe this image. | The image shows an airport arrivals and departures board.  \n**Updated prompt** | **Improved response**  \n---|---  \nParse the time and city from the airport board shown in this image into a list. | \n* 10:50 Moscow\n* 11:05 Edinburgh\n* 11:05 London\n* 11:10 Bucharest\n* 11:30 Kiev\n* 11:35 Dublin\n* 11:45 East Midlands\n* 12:15 Sofia\n* 12:30 London\n* 12:30 Newcastle\n* 12:40 St Petersburg\n* 12:40 London\n* 12:45 Manchester  \n  \n### Add a few examples\n\nThe Gemini model can accept multiple inputs which it can use as examples to understand the output you want. Adding these examples can help the model identify the patterns and apply the relationship between the given images and responses to the new example. This is also called \"few-shot\" learning.\n\nIn the example below, the initial output is written in sentence form, and also contains the country (Brazil). Suppose you want the output in a different format or style, and you want only the city, not the country. Adding few-shot examples to your prompt can steer the model to respond in the way you want.\n\n**Prompt** | **Model response**  \n---|---  \nDetermine the city along with the landmark. | The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil.  \n**Updated prompt** | **Improved response**  \n---|---  \nDetermine the city along with the landmark. |  city: Rome, landmark: the Colosseum.  \n|  city: Beijing, landmark: Forbidden City  \n|  city: Rio de Janeiro, landmark: Christ the Redeemer statue  \n  \n### Break it down step-by-step\n\nFor complex tasks like the ones that require both visual understanding and reasoning, it can be helpful to split the task into smaller, more straightforward steps. Alternatively, it could also be effective if you directly ask the model to “think step by step” in your prompt.\n\nPrompt | Model response  \n---|---  \nWhen will I run out of toilet paper? | Soon, you only have 3 rolls left.  \n**Updated prompt** | **Improved response**  \n---|---  \n  \n1\\. First, count how many toilet paper rolls are in this picture.\n2\\. Then, determine how much toilet paper a typical person uses per day.\n3\\. Calculate how long these rolls of toilet paper will last.\n| \n\n1\\. There are 3 rolls of toilet paper in this picture.\n2\\. A typical person uses about 20 sheets of toilet paper per day.\n3\\. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month.\n  \n  \nMath problems or other types of word problems are great candidates for asking the model to think step-by-step.\n\n**Prompt** | **Response**  \n---|---  \nWhat is the 4th term in the sequence? | -135   \n  \nThe response from the model is incorrect. Some ways to improve this is to ask the model to parse the problem before answering the prompt.\n\n**Prompt** | **Response**  \n---|---  \nParse the formula in the image first. Then based on the formula, answer \"what's the 4th term in the sequence?\". |  _Click to expand the result_ $$ b(n) = b(n-1) * (-3) $$ $$ b(1) = 15 $$\n\n  1. Substitute n = 4 into the formula:\n$$ b(4) = b(4-1) * (-3) $$ $$ b(4) = b(3) * (-3) $$\n  2. We don't know the value of b(3), so substitute n = 3 into the formula:\n$$ b(3) = b(3-1) * (-3) $$ $$ b(3) = b(2) * (-3) $$\n  3. We don't know the value of b(2), so substitute n = 2 into the formula:\n$$ b(2) = b(2-1) * (-3) $$ $$ b(2) = b(1) * (-3) $$\n  4. We know the value of b(1) is 15, so substitute it into the formula:\n$$ b(2) = 15 * (-3) $$ $$ b(2) = -45 $$\n  5. Now that we know the value of b(2), substitute it into the formula for b(3):\n$$ b(3) = b(2) * (-3) $$ $$ b(3) = -45 * (-3) $$ $$ b(3) = 135 $$\n  6. Now that we know the value of b(3), substitute it into the formula for b(4):\n$$ b(4) = b(3) * (-3) $$ $$ b(4) = 135 * (-3) $$ $$ b(4) = -405 $$ \nTherefore, the 4th term in the sequence is -405.  \n  \nThis model response is correct - now to make the response even more streamlined, you can ask the model to to think step-by-step.\n\n**Updated prompt** | **Improved response**  \n---|---  \nWhat is the 4th term in the sequence? Think step by step. | The first term is 15. The second term is b(1) * (-3) = 15 * (-3) = -45. The third term is b(2) * (-3) = -45 * (-3) = 135. The fourth term is b(3) * (-3) = 135 * (-3) = -405.  \n  \n### Try specifying the output format\n\nA common problem is the need for model output to be in a certain format (e.g. markdown, JSON, HTML), especially if the model output needs to be ingested by a downstream task. You can try to produce output in that format by instructing the model to do so within the prompt. Below are two examples:\n\nPrompt | Model response  \n---|---  \nParse the table in this image into markdown format |  | Subject | Status | Time | Start date | Due on | | ------- | ------ | ---- | --------- | ------ | | Math | Done | 1.5 hrs | 11/29/2023 | 12/1/2023 | | Geography | Done | 30 min | 12/2/2023 | 12/4/2023 | | Physics | In progress | 2 hrs | 12/3/2023 | 12/5/2023 | | Biology | Not started | 6 hrs | 11/2/2016 | 12/6/2023 |  \n**Prompt** | **Model response**  \n---|---  \nProvide a list of all the following attributes: ingredients, type of cuisine, vegetarian or not, in JSON format | ` ```json``` { \"ingredients\": [ \"rice\", \"shrimp\", \"clams\", \"mussels\", \"peas\", \"tomatoes\", \"onions\", \"garlic\", \"olive oil\", \"paprika\", \"salt\", \"pepper\" ], \"type of cuisine\": \"Spanish\", \"vegetarian\": false `  \n  \n### Put your image first for single-image prompts\n\nWhile Gemini can interpret image and text in any order within a prompt, placing a single image before the text prompt might lead to better results. In the following samples, you'll notice the image comes first before the text in each prompt.\n\n## Troubleshooting your multimodal prompt\n\nYou might need to troubleshoot your prompt if you are not getting a helpful response. Here are a few strategies you could try.\n\n### If the model is not drawing information from the relevant part of the image\n\nTo get a more specific response, you can point out which aspects of the image you want the prompt to draw information from to inform its response.\n\nPrompt | Model response  \n---|---  \nHow many days will these diapers last a baby? | The diapers will last for 198 days before they run out.  \n**Updated prompt** | **Improved response**  \n---|---  \nHow long will these diapers last before I run out? Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box. Divide the total number by how many diapers the child goes through per day. |  The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs. The average child in this weight range is likely between 1 and 2 years old. A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6).  \n  \n### If the model output is too generic and not tailored enough to the image input\n\nTo help the model tailor its response to the image(s), try asking it to describe the images before performing its reasoning task.\n\nPrompt | Model response |  What is in common between these images? | All four images are examples of landscape photography.  \n---|---  \n**Updated prompt** | **Improved response**  \n---|---  \nFirst, describe what's in each image in detail. What's in common between these images? |  The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea. The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity. The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private and public realms. It's a place where we prepare ourselves to enter or leave the outside world. The fourth image shows the sea in between being calm and being stormy. Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action.  \n  \nYou can also try explicitly asking the model to refer to what's in the image.\n\n**Updated prompt** | **Improved response**  \n---|---  \nWhat is in common between these images? Refer to what's in the images in your response. |  All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a dark and stormy sea. These images are all related to travel because they are all things that people need or use when they travel. The first image shows a person packing a suitcase, which is something that people do when they are going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people often go to buy things for their trips. The third image shows a stack of toilet paper, which is something that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is something that people often encounter when they are traveling.  \n  \nThe same tips work for creative output from images. In the following example with the rabbit, explicitly asking the model to include aspects from the image can improve the output.\n\nPrompt | Model response |  Can you write me a poem about this image? |  Little bunny, so soft and brown,  \nHopping through the field, up and down.  \nYour ears so long, your tail so small,  \nYou're the cutest creature of them all. You nibble on the grass, so green and sweet,  \nAnd hop away when you see me.  \nBut I'll always be your friend,  \nLittle bunny, until the very end.  \n---|---  \n**Updated prompt** | **Improved response**  \n---|---  \nCan you write me a descriptive and dramatic poem about this image and include the location? |  Little bunny on the green,  \nEars so long, so keen.  \nWhat do you see out there,  \nIn the distance, where you stare? Is it the ocean blue,  \nSparkling in the sun's hue?  \nOr the waves that crash and roar,  \nA rhythm that you adore? Little bunny, tell me true,  \nWhat captures your attention so?  \nIs it the salty breeze,  \nOr the seagulls' cries that please? Perhaps it's the vastness of it all,  \nA world so big and small.  \nA place where dreams can roam,  \nAnd adventures can be sown. Little bunny, I wonder what you dream,  \nAs you sit on the grass, so serene.  \nDo you yearn to explore the deep,  \nOr stay on land, where you can leap? Whatever it may be, little bunny,  \nKeep that spark of wonder burning brightly.  \nFor in your dreams and aspirations,  \nLies a world waiting for your creation.  \n  \n### Troubleshooting which part of the prompt failed\n\nIt can be hard to know whether a prompt failed because the model didn't **understand the image** to begin with, or if it did understand the image but did not perform the correct**reasoning steps** afterward.\n\nTo disambiguate between those reasons, ask the model to describe what's in the image.\n\nIn this example below, if the model responds with a snack that seems surprising when paired with tea (e.g. popcorn), you can first troubleshoot to determine whether the model correctly recognized that the image contains tea.\n\nPrompt | Prompt for troubleshooting  \n---|---  \nWhat's a snack I can make in 1 minute that would go well with this? | Describe what's in this image.   \n  \nAnother strategy is to ask the model to explain its reasoning. That can help you narrow down which part of the reasoning broke down, if any.\n\nPrompt | Prompt for troubleshooting  \n---|---  \nWhat's a snack I can make in 1 minute that would go well with this? | What's a snack I can make in 1 minute that would go well with this? Please explain why.  \n  \n### Tuning the sampling parameters\n\nIn each request, you send not only the multimodal prompt but a set of sampling parameters to the model. The model can generate different results for different parameter values. Experiment with the different parameters to get the best values for the task. The most commonly adjusted parameters are the following:\n\n  * Temperature\n  * top-P\n  * top-K\n\n\n\n#### Temperature\n\nTemperature is used for sampling during response generation, which occurs when top-P and top-K are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic and less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected.  \n\n\nFor most use cases, try starting with a temperature of 0.4. If you need more creative results, try increasing the temperature. If you observe clear hallucinations, try reducing the temperature.\n\n#### Top-K\n\nTop-K changes how the model selects tokens for output. A top-K of 1 means the next selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-K of 3 means that the next token is selected from among the three most probable tokens by using temperature.  \n\n\nFor each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further filtered based on top-P with the final token selected using temperature sampling.\n\nSpecify a lower value for less random responses and a higher value for more random responses. The default value of top-K is 32.\n\n#### Top-P\n\nTop-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K) to least probable until the sum of their probabilities equals the top-P value. For example, if tokens A, B, and C have a probability of 0.6, 0.3, 0.1 and the top-P value is 0.9, then the model will select either A or B as the next token by using temperature and excludes C as a candidate.  \n\n\nSpecify a lower value for less random responses and a higher value for more random responses. The default value of top-P is 1.0.\n\n## Next steps\n\n  * Try writing your own multimodal prompts using [Google AI Studio](http://aistudio.google.com).\n  * For more guidance on prompt design, see the [Prompt strategies](/gemini-api/docs/prompting-strategies) page.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/live#context-window-compression",
    "title": "Live API  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Live API \n\n**Preview:** The Live API is in preview.\n\nThe Live API enables low-latency bidirectional voice and video interactions with Gemini. Using the Live API, you can provide end users with the experience of natural, human-like voice conversations, and with the ability to interrupt the model's responses using voice commands. The model can process text, audio, and video input, and it can provide text and audio output.\n\nYou can try the Live API in [Google AI Studio](https://aistudio.google.com/app/live).\n\nTo try a tutorial that lets you use your voice and camera to talk to Gemini through the Live API, see the [`Web Console Demo` project](https://github.com/google-gemini/live-api-web-console). \n\n## What's new\n\nThe Live API has new features and capabilities!\n\n**New capabilities:**\n\n  * Two new voices and 30 new languages, with configurable output language\n  * Configurable image resolutions 66/256 tokens\n  * Configurable turn coverage: Send all inputs all the time or only when the user is speaking\n  * Configure if input should interrupt the model or not\n  * Configurable Voice Activity Detection and new client events for end of turn signaling\n  * Token counts\n  * A client event for signaling end of stream\n  * Text streaming\n  * Configurable session resumption, with session data stored on the server for 24 hours\n  * Longer session support with a sliding context window\n\n\n\n**New client events:**\n\n  * End of audio stream / mic closed\n  * Activity start/end events for manually controlling turn transition\n\n\n\n**New server events:**\n\n  * Go away notification signaling a need to restart a session\n  * Generation complete\n\n\n\n## Use the Live API\n\nThis section describes how to use the Live API with one of our SDKs. For more information about the underlying WebSockets API, see the [WebSockets API reference](/api/live).\n\n### Send and receive text\n    \n    \n    import asyncio\n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    model = \"gemini-2.0-flash-live-001\"\n    \n    config = {\"response_modalities\": [\"TEXT\"]}\n    \n    async def main():\n        async with client.aio.live.connect(model=model, config=config) as session:\n            while True:\n                message = input(\"User> \")\n                if message.lower() == \"exit\":\n                    break\n                await session.send_client_content(\n                    turns={\"role\": \"user\", \"parts\": [{\"text\": message}]}, turn_complete=True\n                )\n    \n                async for response in session.receive():\n                    if response.text is not None:\n                        print(response.text, end=\"\")\n    \n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    \n\n### Receive audio\n\nThe following example shows how to receive audio data and write it to a `.wav` file.\n    \n    \n    import asyncio\n    import wave\n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\", http_options={'api_version': 'v1alpha'})\n    model = \"gemini-2.0-flash-live-001\"\n    \n    config = {\"response_modalities\": [\"AUDIO\"]}\n    \n    async def main():\n        async with client.aio.live.connect(model=model, config=config) as session:\n            wf = wave.open(\"audio.wav\", \"wb\")\n            wf.setnchannels(1)\n            wf.setsampwidth(2)\n            wf.setframerate(24000)\n    \n            message = \"Hello? Gemini are you there?\"\n            await session.send_client_content(\n                turns={\"role\": \"user\", \"parts\": [{\"text\": message}]}, turn_complete=True\n            )\n    \n            async for idx,response in async_enumerate(session.receive()):\n                if response.data is not None:\n                    wf.writeframes(response.data)\n    \n                # Un-comment this code to print audio data info\n                # if response.server_content.model_turn is not None:\n                #      print(response.server_content.model_turn.parts[0].inline_data.mime_type)\n    \n            wf.close()\n    \n    if __name__ == \"__main__\":\n        asyncio.run(main())\n    \n\n#### Audio formats\n\nThe Live API supports the following audio formats:\n\n  * Input audio format: Raw 16 bit PCM audio at 16kHz little-endian\n  * Output audio format: Raw 16 bit PCM audio at 24kHz little-endian\n\n\n\n### Stream audio and video\n\nTo see an example of how to use the Live API in a streaming audio and video format, run the \"Live API - Quickstart\" file in the cookbooks repository: \n\n[View on GitHub](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI.py)\n\n### System instructions\n\nSystem instructions let you steer the behavior of a model based on your specific needs and use cases. System instructions can be set in the setup configuration and will remain in effect for the entire session.\n    \n    \n    from google.genai import types\n    \n    config = {\n        \"system_instruction\": types.Content(\n            parts=[\n                types.Part(\n                    text=\"You are a helpful assistant and answer in a friendly tone.\"\n                )\n            ]\n        ),\n        \"response_modalities\": [\"TEXT\"],\n    }\n    \n\n### Incremental content updates\n\nUse incremental updates to send text input, establish session context, or restore session context. For short contexts you can send turn-by-turn interactions to represent the exact sequence of events:\n\n### Python\n    \n    \n    turns = [\n        {\"role\": \"user\", \"parts\": [{\"text\": \"What is the capital of France?\"}]},\n        {\"role\": \"model\", \"parts\": [{\"text\": \"Paris\"}]},\n    ]\n    \n    await session.send_client_content(turns=turns, turn_complete=False)\n    \n    turns = [{\"role\": \"user\", \"parts\": [{\"text\": \"What is the capital of Germany?\"}]}]\n    \n    await session.send_client_content(turns=turns, turn_complete=True)\n    \n\n### JSON\n    \n    \n    {\n      \"clientContent\": {\n        \"turns\": [\n          {\n            \"parts\":[\n              {\n                \"text\": \"\"\n              }\n            ],\n            \"role\":\"user\"\n          },\n          {\n            \"parts\":[\n              {\n                \"text\": \"\"\n              }\n            ],\n            \"role\":\"model\"\n          }\n        ],\n        \"turnComplete\": true\n      }\n    }\n    \n\nFor longer contexts it's recommended to provide a single message summary to free up the context window for subsequent interactions.\n\n### Change voices\n\nThe Live API supports the following voices: Puck, Charon, Kore, Fenrir, Aoede, Leda, Orus, and Zephyr.\n\nTo specify a voice, set the voice name within the `speechConfig` object as part of the session configuration:\n\n### Python\n    \n    \n    from google.genai import types\n    \n    config = types.LiveConnectConfig(\n        response_modalities=[\"AUDIO\"],\n        speech_config=types.SpeechConfig(\n            voice_config=types.VoiceConfig(\n                prebuilt_voice_config=types.PrebuiltVoiceConfig(voice_name=\"Kore\")\n            )\n        )\n    )\n    \n\n### JSON\n    \n    \n    {\n      \"voiceConfig\": {\n        \"prebuiltVoiceConfig\": {\n          \"voiceName\": \"Kore\"\n        }\n      }\n    }\n    \n\n### Use function calling\n\nYou can define tools with the Live API. See the [Function calling tutorial](https://ai.google.dev/gemini-api/docs/function-calling) to learn more about function calling.\n\nTools must be defined as part of the session configuration:\n    \n    \n    config = types.LiveConnectConfig(\n        response_modalities=[\"TEXT\"],\n        tools=[set_light_values]\n    )\n    \n    async with client.aio.live.connect(model=model, config=config) as session:\n        await session.send_client_content(\n            turns={\n                \"role\": \"user\",\n                \"parts\": [{\"text\": \"Turn the lights down to a romantic level\"}],\n            },\n            turn_complete=True,\n        )\n    \n        async for response in session.receive():\n            print(response.tool_call)\n    \n\nFrom a single prompt, the model can generate multiple function calls and the code necessary to chain their outputs. This code executes in a sandbox environment, generating subsequent [BidiGenerateContentToolCall](/api/live#bidigeneratecontenttoolcall) messages. The execution pauses until the results of each function call are available, which ensures sequential processing.\n\nThe client should respond with [BidiGenerateContentToolResponse](/api/live#bidigeneratecontenttoolresponse).\n\nAudio inputs and audio outputs negatively impact the model's ability to use function calling.\n\n### Handle interruptions\n\nUsers can interrupt the model's output at any time. When [Voice activity detection](/api/live#voice-activity-detection) (VAD) detects an interruption, the ongoing generation is canceled and discarded. Only the information already sent to the client is retained in the session history. The server then sends a [BidiGenerateContentServerContent](/api/live#bidigeneratecontentservercontent) message to report the interruption.\n\nIn addition, the Gemini server discards any pending function calls and sends a `BidiGenerateContentServerContent` message with the IDs of the canceled calls.\n    \n    \n    async for response in session.receive():\n        if response.server_content.interrupted is not None:\n            # The generation was interrupted\n    \n\n### Configure voice activity detection (VAD)\n\nBy default, the model automatically performs voice activity detection (VAD) on a continuous audio input stream. VAD can be configured with the [`realtimeInputConfig.automaticActivityDetection`](/api/live#RealtimeInputConfig.AutomaticActivityDetection) field of the [setup configuration](/api/live#BidiGenerateContentSetup).\n\nWhen the audio stream is paused for more than a second (for example, because the user switched off the microphone), an [`audioStreamEnd`](/api/live#BidiGenerateContentRealtimeInput.FIELDS.bool.BidiGenerateContentRealtimeInput.audio_stream_end) event should be sent to flush any cached audio. The client can resume sending audio data at any time.\n\nAlternatively, the automatic VAD can be disabled by setting `realtimeInputConfig.automaticActivityDetection.disabled` to `true` in the setup message. In this configuration the client is responsible for detecting user speech and sending [`activityStart`](/api/live#BidiGenerateContentRealtimeInput.FIELDS.BidiGenerateContentRealtimeInput.ActivityStart.BidiGenerateContentRealtimeInput.activity_start) and [`activityEnd`](/api/live#BidiGenerateContentRealtimeInput.FIELDS.BidiGenerateContentRealtimeInput.ActivityEnd.BidiGenerateContentRealtimeInput.activity_end) messages at the appropriate times. An `audioStreamEnd` isn't sent in this configuration. Instead, any interruption of the stream is marked by an `activityEnd` message.\n\n_SDK support for this feature will be available in the coming weeks._\n\n### Get the token count\n\nYou can find the total number of consumed tokens in the [usageMetadata](/api/live#usagemetadata) field of the returned server message.\n    \n    \n    from google.genai import types\n    \n    async with client.aio.live.connect(\n        model='gemini-2.0-flash-live-001',\n        config=types.LiveConnectConfig(\n            response_modalities=['AUDIO'],\n        ),\n    ) as session:\n        # Session connected\n        while True:\n            await session.send_client_content(\n                turns=types.Content(role='user', parts=[types.Part(text='Hello world!')])\n            )\n            async for message in session.receive():\n                # The server will periodically send messages that include\n                # UsageMetadata.\n                if message.usage_metadata:\n                    usage = message.usage_metadata\n                    print(\n                        f'Used {usage.total_token_count} tokens in total. Response token'\n                        ' breakdown:'\n                    )\n                for detail in usage.response_tokens_details:\n                    match detail:\n                      case types.ModalityTokenCount(modality=modality, token_count=count):\n                          print(f'{modality}: {count}')\n    \n                # For the purposes of this example, placeholder input is continually fed\n                # to the model. In non-sample code, the model inputs would come from\n                # the user.\n                if message.server_content and message.server_content.turn_complete:\n                    break\n    \n\n### Configure session resumption\n\nTo prevent session termination when the server periodically resets the WebSocket connection, configure the [sessionResumption](/api/live#BidiGenerateContentSetup.FIELDS.SessionResumptionConfig.BidiGenerateContentSetup.session_resumption) field within the [setup configuration](/api/live#BidiGenerateContentSetup).\n\nPassing this configuration causes the server to send [SessionResumptionUpdate](/api/live#SessionResumptionUpdate) messages, which can be used to resume the session by passing the last resumption token as the [`SessionResumptionConfig.handle`](/api/liveSessionResumptionConfig.FIELDS.string.SessionResumptionConfig.handle) of the subsequent connection.\n    \n    \n    from google.genai import types\n    \n    print(f\"Connecting to the service with handle {previous_session_handle}...\")\n    async with client.aio.live.connect(\n        model=\"gemini-2.0-flash-live-001\",\n        config=types.LiveConnectConfig(\n            response_modalities=[\"AUDIO\"],\n            session_resumption=types.SessionResumptionConfig(\n                # The handle of the session to resume is passed here,\n                # or else None to start a new session.\n                handle=previous_session_handle\n            ),\n        ),\n    ) as session:\n        # Session connected\n        while True:\n            await session.send_client_content(\n                turns=types.Content(\n                    role=\"user\", parts=[types.Part(text=\"Hello world!\")]\n                )\n            )\n            async for message in session.receive():\n                # Periodically, the server will send update messages that may\n                # contain a handle for the current state of the session.\n                if message.session_resumption_update:\n                    update = message.session_resumption_update\n                    if update.resumable and update.new_handle:\n                        # The handle should be retained and linked to the session.\n                        return update.new_handle\n    \n                # For the purposes of this example, placeholder input is continually fed\n                # to the model. In non-sample code, the model inputs would come from\n                # the user.\n                if message.server_content and message.server_content.turn_complete:\n                    break\n    \n\n### Receive a message before the session disconnects\n\nThe server sends a [GoAway](/api/live#GoAway) message that signals that the current connection will soon be terminated. This message includes the [timeLeft](/api/live#GoAway.FIELDS.google.protobuf.Duration.GoAway.time_left), indicating the remaining time and lets you take further action before the connection will be terminated as ABORTED.\n\n### Receive a message when the generation is complete\n\nThe server sends a [generationComplete](/api/live#BidiGenerateContentServerContent.FIELDS.bool.BidiGenerateContentServerContent.generation_complete) message that signals that the model finished generating the response.\n\n### Enable context window compression\n\nTo enable longer sessions, and avoid abrupt connection termination, you can enable context window compression by setting the [contextWindowCompression](/api/live#BidiGenerateContentSetup.FIELDS.ContextWindowCompressionConfig.BidiGenerateContentSetup.context_window_compression) field as part of the session configuration.\n\nIn the [ContextWindowCompressionConfig](/api/live#contextwindowcompressionconfig), you can configure a [sliding-window mechanism](/api/live#ContextWindowCompressionConfig.FIELDS.ContextWindowCompressionConfig.SlidingWindow.ContextWindowCompressionConfig.sliding_window) and the [number of tokens](/api/live#ContextWindowCompressionConfig.FIELDS.int64.ContextWindowCompressionConfig.trigger_tokens) that triggers compression.\n    \n    \n    from google.genai import types\n    \n    config = types.LiveConnectConfig(\n        response_modalities=[\"AUDIO\"],\n        context_window_compression=(\n            # Configures compression with default parameters.\n            types.ContextWindowCompressionConfig(\n                sliding_window=types.SlidingWindow(),\n            )\n        ),\n    )\n    \n\n### Change the media resolution\n\nYou can specify the media resolution for the input media by setting the `mediaResolution` field as part of the session configuration:\n    \n    \n    from google.genai import types\n    \n    config = types.LiveConnectConfig(\n        response_modalities=[\"AUDIO\"],\n        media_resolution=types.MediaResolution.MEDIA_RESOLUTION_LOW,\n    )\n    \n\n## Limitations\n\nConsider the following limitations of the Live API and Gemini 2.0 when you plan your project.\n\n### Client authentication\n\nThe Live API only provides server to server authentication and isn't recommended for direct client use. Client input should be routed through an intermediate application server for secure authentication with the Live API.\n\n### Session duration\n\nSession duration can be extended to unlimited by enabling session [compression](/gemini-api/docs/live#context-window-compression). Without compression, audio-only sessions are limited to 15 minutes, and audio plus video sessions are limited to 2 minutes. Exceeding these limits without compression will terminate the connection.\n\n### Context window\n\nA session has a context window limit of 32k tokens.\n\n## Third-party integrations\n\nFor web and mobile app deployments, you can explore options from:\n\n  * [Daily](https://www.daily.co/products/gemini/multimodal-live-api/)\n  * [Livekit](https://docs.livekit.io/agents/integrations/google/#multimodal-live-api)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-11 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-11 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/embeddings#use-cases",
    "title": "Embeddings  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Embeddings \n\n**Note:** Introducing our first Gemini embedding model, [available now to developers](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) as `gemini-embedding-exp-03-07` in the API.\n\nThe Gemini API supports several embedding models that generate embeddings for words, phrases, code, and sentences. The resulting embeddings can then be used for tasks such as semantic search, text classification, and clustering, among many others.\n\n## What are embeddings?\n\nEmbeddings are numerical representations of text (or other media formats) that capture relationships between inputs. Text embeddings work by converting text into arrays of floating point numbers, called _vectors_. These vectors are designed to capture the meaning of the text. The length of the embedding array is called the vector's _dimensionality_. A passage of text might be represented by a vector containing hundreds of dimensions.\n\nEmbeddings capture semantic meaning and context, which results in text with similar meanings having \"closer\" embeddings. For example, the sentence \"I took my dog to the vet\" and \"I took my cat to the vet\" would have embeddings that are close to each other in the vector space.\n\nYou can use embeddings to compare different texts and understand how they relate. For example, if the embeddings of the text \"cat\" and \"dog\" are close together you can infer that these words are similar in meaning, context, or both. This enables a variety of [common AI use cases](/gemini-api/docs/embeddings#use-cases).\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Generate embeddings\n\nUse the `embedContent` method to generate text embeddings:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    result = client.models.embed_content(\n            model=\"gemini-embedding-exp-03-07\",\n            contents=\"What is the meaning of life?\")\n    \n    print(result.embeddings)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    async function main() {\n    \n        const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n        const response = await ai.models.embedContent({\n            model: 'gemini-embedding-exp-03-07',\n            contents: 'What is the meaning of life?',\n        });\n    \n        console.log(response.embeddings);\n    }\n    \n    main();\n    \n\n### Go\n    \n    \n    ctx := context.Background()\n    \n    client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer client.Close()\n    \n    em := client.EmbeddingModel(\"gemini-embedding-exp-03-07\")\n    res, err := em.EmbedContent(ctx, genai.Text(\"What is the meaning of life?\"))\n    \n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(res.Embedding.Values)\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-exp-03-07:embedContent?key=$GEMINI_API_KEY\" \\\n    -H 'Content-Type: application/json' \\\n    -d '{\"model\": \"models/gemini-embedding-exp-03-07\",\n         \"content\": {\n         \"parts\":[{\n         \"text\": \"What is the meaning of life?\"}]}\n        }'\n    \n\nYou can also generate embeddings for multiple chunks at once by passing them in as a list of strings.\n\n## Task types\n\nWhen building Retrieval Augmented Generation (RAG) systems, a common design is to use text embeddings to perform a similarity search. In some cases this can lead to degraded quality, because questions and their answers are not semantically similar. For example, a question like \"Why is the sky blue?\" and its answer \"The scattering of sunlight causes the blue color,\" have distinctly different meanings as statements, which means that a RAG system won't automatically recognize their relation.\n\nTask types enable you to generate optimized embeddings for specific tasks, saving you time and cost and improving performance.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    result = client.models.embed_content(\n            model=\"gemini-embedding-exp-03-07\",\n            contents=\"What is the meaning of life?\",\n            config=types.EmbedContentConfig(task_type=\"SEMANTIC_SIMILARITY\")\n    )\n    print(result.embeddings)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    async function main() {\n    \n        const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n        const response = await ai.models.embedContent({\n            model: 'gemini-embedding-exp-03-07',\n            contents: 'What is the meaning of life?',\n            config: {\n                taskType: \"SEMANTIC_SIMILARITY\",\n            }\n        });\n    \n        console.log(response.embeddings);\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-exp-03-07:embedContent?key=$GEMINI_API_KEY\" \\\n    -H 'Content-Type: application/json' \\\n    -d '{\"model\": \"models/gemini-embedding-exp-03-07\",\n         \"content\": {\n         \"parts\":[{\n         \"text\": \"What is the meaning of life?\"}]},\n         \"taskType\": \"SEMANTIC_SIMILARITY\"\n        }'\n    \n\n### Supported task types\n\nTask type | Description  \n---|---  \n`SEMANTIC_SIMILARITY` | Used to generate embeddings that are optimized to assess text similarity.  \n`CLASSIFICATION` | Used to generate embeddings that are optimized to classify texts according to preset labels.  \n`CLUSTERING` | Used to generate embeddings that are optimized to cluster texts based on their similarities.  \n`RETRIEVAL_DOCUMENT`, `RETRIEVAL_QUERY`, `QUESTION_ANSWERING`, and `FACT_VERIFICATION` | Used to generate embeddings that are optimized for document search or information retrieval.  \n`CODE_RETRIEVAL_QUERY` | Used to retrieve a code block based on a natural language query, such as sort an array or reverse a linked list. Embeddings of the code blocks are computed using `RETRIEVAL_DOCUMENT`.  \n  \n## Use cases\n\nText embeddings are used in a variety of common AI use cases, such as:\n\n  * **Information retrieval:** You can use embeddings to retrieve semantically similar text given a piece of input text.\n\n[ Document search tutorialtask ](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/document_search.ipynb)\n\n  * **Clustering:** Comparing groups of embeddings can help identify hidden trends.\n\n[ Embedding clustering tutorialbubble_chart ](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/clustering_with_embeddings.ipynb)\n\n  * **Vector database:** As you take different embedding use cases to production, it is common to store embeddings in a vector database.\n\n[ Vector database tutorialbolt ](https://github.com/google-gemini/cookbook/blob/main/examples/chromadb/Vectordb_with_chroma.ipynb)\n\n  * **Classification:** You can train a model using embeddings to classify documents into categories.\n\n[ Classification tutorialtoken ](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/text_classifier_embeddings.ipynb)\n\n\n\n\n## Embedding models\n\nThe Gemini API offers three models that generate text embeddings:\n\n  * [gemini-embedding-exp-03-07](/gemini-api/docs/models#gemini-embedding)\n  * [text-embedding-004](/gemini-api/docs/models/gemini#text-embedding)\n  * [embedding-001](/gemini-api/docs/models/gemini#embedding)\n\n\n\n## What's next\n\nCheck out the [embeddings quickstart notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Embeddings.ipynb).\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models#gemini-embedding",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/gemini#text-embedding",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/gemini#embedding",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=python",
    "title": "Fine-tuning tutorial  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Fine-tuning tutorial \n\nPython REST\n\nThis tutorial will help you get started with the Gemini API tuning service using either the Python SDK or the REST API using [curl](https://curl.se/). The examples show how to tune the text model behind the Gemini API text generation service.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n[View on ai.google.dev](https://ai.google.dev/gemini-api/docs/model-tuning/python) |  [Try a Colab notebook](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/model-tuning/python.ipynb) |  [View notebook on GitHub](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/model-tuning/python.ipynb)\n\n## Limitations\n\nBefore tuning a model, you should be aware of the following limitations:\n\n### Fine-tuning datasets\n\nFine-tuning datasets for Gemini 1.5 Flash have the following limitations:\n\n  * The maximum input size per example is 40,000 characters.\n  * The maximum output size per example is 5,000 characters.\n  * Only input-output pair examples are supported. Chat-style multi-turn conversations are not supported.\n\n\n\n### Tuned models\n\nTuned models have the following limitations:\n\n  * The input limit of a tuned Gemini 1.5 Flash model is 40,000 characters.\n  * JSON mode is not supported with tuned models.\n  * Only text input is supported.\n\n\n\n## List tuned models\n\nYou can check your existing tuned models with the [`tunedModels.list`](/api/tuning#method:-tunedmodels.list) method.\n    \n    \n    from google import genai\n    from google.genai import types\n    client = genai.Client() # Get the key from the GOOGLE_API_KEY env variable\n    \n    for model_info in client.models.list():\n        print(model_info.name)\n    \n\n## Create a tuned model\n\nTo create a tuned model, you need to pass your [dataset](/api/tuning#Dataset) to the model in the [`tunedModels.create`](/api/tuning#method:-tunedmodels.create) method. For this example, you will tune a model to generate the next number in the sequence. For example, if the input is `1`, the model should output `2`. If the input is `one hundred`, the output should be `one hundred one`.\n    \n    \n    # create tuning model\n    training_dataset =  [\n        [\"1\", \"2\"],\n        [\"3\", \"4\"],\n        [\"-3\", \"-2\"],\n        [\"twenty two\", \"twenty three\"],\n        [\"two hundred\", \"two hundred one\"],\n        [\"ninety nine\", \"one hundred\"],\n        [\"8\", \"9\"],\n        [\"-98\", \"-97\"],\n        [\"1,000\", \"1,001\"],\n        [\"10,100,000\", \"10,100,001\"],\n        [\"thirteen\", \"fourteen\"],\n        [\"eighty\", \"eighty one\"],\n        [\"one\", \"two\"],\n        [\"three\", \"four\"],\n        [\"seven\", \"eight\"],\n    ]\n    training_dataset=types.TuningDataset(\n            examples=[\n                types.TuningExample(\n                    text_input=i,\n                    output=o,\n                )\n                for i,o in training_dataset\n            ],\n        )\n    tuning_job = client.tunings.tune(\n        base_model='models/gemini-1.5-flash-001-tuning',\n        training_dataset=training_dataset,\n        config=types.CreateTuningJobConfig(\n            epoch_count= 5,\n            batch_size=4,\n            learning_rate=0.001,\n            tuned_model_display_name=\"test tuned model\"\n        )\n    )\n    \n    # generate content with the tuned model\n    response = client.models.generate_content(\n        model=tuning_job.tuned_model.model,\n        contents='III',\n    )\n    \n    print(response.text)\n    \n\nThe optimal values for epoch count, batch size, and learning rate are dependent on your dataset and other constraints of your use case. To learn more about these values, see [Advanced tuning settings](/gemini-api/docs/model-tuning#advanced-settings) and [Hyperparameters](/api/tuning#Hyperparameters). **Tip:** For a more general introduction to these hyperparameters, see [Linear regression: Hyperparameters](https://developers.google.com/machine-learning/crash-course/linear-regression/hyperparameters) in the [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course).\n\n## Try the model\n\nYou can use the [`tunedModels.generateContent`](/api/tuning#method:-tunedmodels.generatecontent) method and specify the name of the tuned model to test its performance.\n    \n    \n    response = client.models.generate_content(\n        model=tuning_job.tuned_model.model,\n        contents='III'\n    )\n    \n\n## Not implemented\n\nSome features (progress reporting, updating the description, and deleting tuned models) has not yet been implemented in the new SDK. Send feedback  Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-03 UTC. Need to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/model-tuning#advanced-settings",
    "title": "Fine-tuning with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\nSend feedback \n\n#  Fine-tuning with the Gemini API \n\nPrompt design strategies such as few-shot prompting may not always produce the results you need. _Fine-tuning_ is a process that can improve your model's performance on specific tasks or help the model adhere to specific output requirements when instructions aren't sufficient and you have a set of examples that demonstrate the outputs you want. This page provides a conceptual overview of fine-tuning the text model behind the Gemini API text service. When you're ready to start tuning, try the [fine-tuning tutorial](/gemini-api/docs/model-tuning/tutorial). If you'd like a more general introduction to customizing LLMs for specific use cases, check out [LLMs: Fine-tuning, distillation, and prompt engineering](https://developers.google.com/machine-learning/crash-course/llm/tuning) in the [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/).\n\n## How fine-tuning works\n\nThe goal of fine-tuning is to further improve the performance of the model for your specific task. Fine-tuning works by providing the model with a training dataset containing many examples of the task. For niche tasks, you can get significant improvements in model performance by tuning the model on a modest number of examples. This kind of model tuning is sometimes referred to as _supervised fine-tuning_ , to distinguish it from other kinds of fine-tuning. Your training data should be structured as examples with prompt inputs and expected response outputs. You can also tune models using example data directly in Google AI Studio. The goal is to teach the model to mimic the wanted behavior or task, by giving it many examples illustrating that behavior or task. When you run a tuning job, the model learns additional parameters that help it encode the necessary information to perform the wanted task or learn the wanted behavior. These parameters can then be used at inference time. The output of the tuning job is a new model, which is effectively a combination of the newly learned parameters and the original model.\n\n## Prepare your dataset\n\nBefore you can start fine-tuning, you need a dataset to tune the model with. For the best performance, the examples in the dataset should be of high quality, diverse, and representative of real inputs and outputs.\n\n### Format\n\n**Note:** Fine-tuning only supports input-output pair examples. Chat-style multi-turn conversations are not supported at this time. The examples included in your dataset should match your expected production traffic. If your dataset contains specific formatting, keywords, instructions, or information, the production data should be formatted in the same way and contain the same instructions. For example, if the examples in your dataset include a `\"question:\"` and a `\"context:\"`, production traffic should also be formatted to include a `\"question:\"` and a `\"context:\"` in the same order as it appears in the dataset examples. If you exclude the context, the model can't recognize the pattern, even if the exact question was in an example in the dataset. As another example, here's Python training data for an application that generates the next number in a sequence:\n    \n    \n    training_data = [\n      {\"text_input\": \"1\", \"output\": \"2\"},\n      {\"text_input\": \"3\", \"output\": \"4\"},\n      {\"text_input\": \"-3\", \"output\": \"-2\"},\n      {\"text_input\": \"twenty two\", \"output\": \"twenty three\"},\n      {\"text_input\": \"two hundred\", \"output\": \"two hundred one\"},\n      {\"text_input\": \"ninety nine\", \"output\": \"one hundred\"},\n      {\"text_input\": \"8\", \"output\": \"9\"},\n      {\"text_input\": \"-98\", \"output\": \"-97\"},\n      {\"text_input\": \"1,000\", \"output\": \"1,001\"},\n      {\"text_input\": \"10,100,000\", \"output\": \"10,100,001\"},\n      {\"text_input\": \"thirteen\", \"output\": \"fourteen\"},\n      {\"text_input\": \"eighty\", \"output\": \"eighty one\"},\n      {\"text_input\": \"one\", \"output\": \"two\"},\n      {\"text_input\": \"three\", \"output\": \"four\"},\n      {\"text_input\": \"seven\", \"output\": \"eight\"},\n    ]\n    \n\nAdding a prompt or preamble to each example in your dataset can also help improve the performance of the tuned model. Note, if a prompt or preamble is included in your dataset, it should also be included in the prompt to the tuned model at inference time.\n\n### Limitations\n\n**Note:** Fine-tuning datasets for Gemini 1.5 Flash have the following limitations:\n\n  * The maximum input size per example is 40,000 characters.\n  * The maximum output size per example is 5,000 characters.\n\n\n\n### Training data size\n\nYou can fine-tune a model with as little as 20 examples. Additional data generally improves the quality of the responses. You should target between 100 and 500 examples, depending on your application. The following table shows recommended dataset sizes for fine-tuning a text model for various common tasks: | Task  | No. of examples in dataset   \n---|---  \nClassification  | 100+   \nSummarization  | 100-500+   \nDocument search  | 100+   \n  \n## Upload your tuning dataset\n\nData is either passed inline using the API or through files uploaded in Google AI Studio.\n\nTo use the client library, provide the data file in the `createTunedModel` call. File size limit is 4 MB. See the [fine-tuning quickstart with Python](/gemini-api/docs/model-tuning/tutorial?lang=python) to get started.\n\nTo call the REST API using cURL, provide training examples in JSON format to the `training_data` argument. See the [tuning quickstart with cURL](/gemini-api/docs/model-tuning/tutorial?lang=rest) to get started.\n\n## Advanced tuning settings\n\nWhen creating a tuning job, you can specify the following advanced settings:\n\n  * **Epochs:** A full training pass over the entire training set such that each example has been processed once.\n  * **Batch size:** The set of examples used in one training [iteration](https://developers.google.com/machine-learning/glossary#iteration). The batch size determines the number of examples in a batch.\n  * **Learning rate:** A floating-point number that tells the algorithm how strongly to adjust the model parameters on each iteration. For example, a learning rate of 0.3 would adjust weights and biases three times more powerfully than a learning rate of 0.1. High and low learning rates have their own unique trade-offs and should be adjusted based on your use case.\n  * **Learning rate multiplier:** The rate multiplier modifies the model's original learning rate. A value of 1 uses the original learning rate of the model. Values greater than 1 increase the learning rate and values between 1 and 0 lower the learning rate.\n\n\n\n### Recommended configurations\n\nThe following table shows the recommended configurations for fine-tuning a foundation model:\n\nHyperparameter | Default value | Recommended adjustments  \n---|---|---  \nEpoch | 5 |  If the loss starts to plateau before 5 epochs, use a smaller value. If the loss is converging and doesn't seem to plateau, use a higher value.  \nBatch size | 4 |   \nLearning rate | 0.001 | Use a smaller value for smaller datasets.  \n  \nThe loss curve shows how much the model's prediction deviates from the ideal predictions in the training examples after each epoch. Ideally you want to stop training at the lowest point in the curve right before it plateaus. For example, the graph below shows the loss curve plateauing at about epoch 4-6 which means you can set the `Epoch` parameter to 4 and still get the same performance.\n\n## Check the tuning job status\n\nYou can check the status of your tuning job in Google AI Studio under the **My Library** tab or using the `metadata` property of the tuned model in the Gemini API.\n\n## Troubleshoot errors\n\nThis section includes tips on how to resolve errors you may encounter while creating your tuned model.\n\n### Authentication\n\n**Note:** Starting September 30, 2024, OAuth authentication is no longer required. New projects should use API key authentication instead.\n\nTuning using the API and client library requires authentication. You can set up authentication using either an API key (recommended) or using OAuth credentials. For documentation on setting up an API key, see [Set up API key](/gemini-api/docs/quickstart#set-up-api-key).\n\nIf you see a `'PermissionDenied: 403 Request had insufficient authentication scopes'` error, you may need to set up user authentication using OAuth credentials. To configure OAuth credentials for Python, visit our [the OAuth setup tutorial](/gemini-api/docs/oauth).\n\n### Canceled models\n\nYou can cancel a fine-tuning job any time before the job is finished. However, the inference performance of a canceled model is unpredictable, particularly if the tuning job is canceled early in the training. If you canceled because you want to stop the training at an earlier epoch, you should create a new tuning job and set the epoch to a lower value.\n\n## Limitations of tuned models\n\n**Note:** Tuned models have the following limitations:\n\n  * The input limit of a tuned Gemini 1.5 Flash model is 40,000 characters.\n  * JSON mode is not supported with tuned models.\n  * Only text input is supported.\n\n\n\n## What's next\n\nGet started with the fine-tuning tutorials:\n\n  * [Fine-tuning tutorial (Python)](/gemini-api/docs/model-tuning/tutorial?lang=python)\n  * [Fine-tuning tutorial (REST)](/gemini-api/docs/model-tuning/tutorial?lang=rest)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/model-tuning/tutorial?lang=rest",
    "title": "Fine-tuning tutorial  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Fine-tuning tutorial \n\nPython REST\n\nThis tutorial will help you get started with the Gemini API tuning service using either the Python SDK or the REST API using [curl](https://curl.se/). The examples show how to tune the text model behind the Gemini API text generation service.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n[View on ai.google.dev](https://ai.google.dev/gemini-api/docs/model-tuning/rest) |  [Try a Colab notebook](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/model-tuning/rest.ipynb) |  [View notebook on GitHub](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/model-tuning/rest.ipynb)\n\n## Limitations\n\nBefore tuning a model, you should be aware of the following limitations:\n\n### Fine-tuning datasets\n\nFine-tuning datasets for Gemini 1.5 Flash have the following limitations:\n\n  * The maximum input size per example is 40,000 characters.\n  * The maximum output size per example is 5,000 characters.\n  * Only input-output pair examples are supported. Chat-style multi-turn conversations are not supported.\n\n\n\n### Tuned models\n\nTuned models have the following limitations:\n\n  * The input limit of a tuned Gemini 1.5 Flash model is 40,000 characters.\n  * JSON mode is not supported with tuned models.\n  * Only text input is supported.\n\n\n\n## List tuned models\n\nYou can check your existing tuned models with the [`tunedModels.list`](/api/tuning#method:-tunedmodels.list) method.\n    \n    \n    # Sending a page_size is optional\n    curl -X GET https://generativelanguage.googleapis.com/v1beta/tunedModels?page_size=5 \\\n        -H \"Content-Type: application/json\" \\\n        -H \"Authorization: Bearer ${access_token}\" \\\n        -H \"x-goog-user-project: ${project_id}\" > tuned_models.json\n    \n    jq .tunedModels[].name < tuned_models.json\n    \n    # Send the nextPageToken to get the next page.\n    page_token=$(jq .nextPageToken < tuned_models.json | tr -d '\"')\n    \n    if [[ \"$page_token\" != \"null\"\" ]]; then\n    curl -X GET https://generativelanguage.googleapis.com/v1beta/tunedModels?page_size=5\\&page_token=${page_token}?key=$GEMINI_API_KEY \\\n        -H \"Content-Type: application/json\"  > tuned_models2.json\n    jq .tunedModels[].name < tuned_models.json\n    fi[tuned_models.sh](https://github.com/google-gemini/generative-ai-python/blob/61867f68149a63fd0cf3c37bd46177a75ec5d8ed/samples/rest/tuned_models.sh#L120-L136)\n\n## Create a tuned model\n\nTo create a tuned model, you need to pass your [dataset](/api/tuning#Dataset) to the model in the [`tunedModels.create`](/api/tuning#method:-tunedmodels.create) method. For this example, you will tune a model to generate the next number in the sequence. For example, if the input is `1`, the model should output `2`. If the input is `one hundred`, the output should be `one hundred one`.\n    \n    \n    curl -X POST \"https://generativelanguage.googleapis.com/v1beta/tunedModels?key=$GEMINI_API_KEY\" \\\n        -H 'Content-Type: application/json' \\\n        -d '\n          {\n            \"display_name\": \"number generator model\",\n            \"base_model\": \"models/gemini-1.5-flash-001-tuning\",\n            \"tuning_task\": {\n              \"hyperparameters\": {\n                \"batch_size\": 2,\n                \"learning_rate\": 0.001,\n                \"epoch_count\":5,\n              },\n              \"training_data\": {\n                \"examples\": {\n                  \"examples\": [\n                    {\n                        \"text_input\": \"1\",\n                        \"output\": \"2\",\n                    },{\n                        \"text_input\": \"3\",\n                        \"output\": \"4\",\n                    },{\n                        \"text_input\": \"-3\",\n                        \"output\": \"-2\",\n                    },{\n                        \"text_input\": \"twenty two\",\n                        \"output\": \"twenty three\",\n                    },{\n                        \"text_input\": \"two hundred\",\n                        \"output\": \"two hundred one\",\n                    },{\n                        \"text_input\": \"ninety nine\",\n                        \"output\": \"one hundred\",\n                    },{\n                        \"text_input\": \"8\",\n                        \"output\": \"9\",\n                    },{\n                        \"text_input\": \"-98\",\n                        \"output\": \"-97\",\n                    },{\n                        \"text_input\": \"1,000\",\n                        \"output\": \"1,001\",\n                    },{\n                        \"text_input\": \"10,100,000\",\n                        \"output\": \"10,100,001\",\n                    },{\n                        \"text_input\": \"thirteen\",\n                        \"output\": \"fourteen\",\n                    },{\n                        \"text_input\": \"eighty\",\n                        \"output\": \"eighty one\",\n                    },{\n                        \"text_input\": \"one\",\n                        \"output\": \"two\",\n                    },{\n                        \"text_input\": \"three\",\n                        \"output\": \"four\",\n                    },{\n                        \"text_input\": \"seven\",\n                        \"output\": \"eight\",\n                    }\n                  ]\n                }\n              }\n            }\n          }' | tee tunemodel.json\n    \n    # Check the operation for status updates during training.\n    # Note: you can only check the operation on v1/\n    operation=$(cat tunemodel.json | jq \".name\" | tr -d '\"')\n    tuning_done=false\n    \n    while [[ \"$tuning_done\" != \"true\" ]];\n    do\n      sleep 5\n      curl -X GET \"https://generativelanguage.googleapis.com/v1/${operation}?key=$GEMINI_API_KEY\" \\\n        -H 'Content-Type: application/json' \\\n         2> /dev/null > tuning_operation.json\n    \n      complete=$(jq .metadata.completedPercent < tuning_operation.json)\n      tput cuu1\n      tput el\n      echo \"Tuning...${complete}%\"\n      tuning_done=$(jq .done < tuning_operation.json)\n    done\n    \n    # Or get the TunedModel and check it's state. The model is ready to use if the state is active.\n    modelname=$(cat tunemodel.json | jq \".metadata.tunedModel\" | tr -d '\"')\n    curl -X GET  https://generativelanguage.googleapis.com/v1beta/${modelname}?key=$GEMINI_API_KEY \\\n        -H 'Content-Type: application/json' > tuned_model.json\n    \n    cat tuned_model.json | jq \".state\"[tuned_models.sh](https://github.com/google-gemini/generative-ai-python/blob/61867f68149a63fd0cf3c37bd46177a75ec5d8ed/samples/rest/tuned_models.sh#L4-L96)\n    \n\nThe optimal values for epoch count, batch size, and learning rate are dependent on your dataset and other constraints of your use case. To learn more about these values, see [Advanced tuning settings](/gemini-api/docs/model-tuning#advanced-settings) and [Hyperparameters](/api/tuning#Hyperparameters). **Tip:** For a more general introduction to these hyperparameters, see [Linear regression: Hyperparameters](https://developers.google.com/machine-learning/crash-course/linear-regression/hyperparameters) in the [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course). Your tuned model is immediately added to the list of tuned models, but its state is set to \"creating\" while the model is tuned.\n\n## Try the model\n\nYou can use the [`tunedModels.generateContent`](/api/tuning#method:-tunedmodels.generatecontent) method and specify the name of the tuned model to test its performance.\n    \n    \n    curl -X POST https://generativelanguage.googleapis.com/v1beta/$modelname:generateContent?key=$GEMINI_API_KEY \\\n        -H 'Content-Type: application/json' \\\n        -d '{\n            \"contents\": [{\n            \"parts\": [{\n              \"text\": \"LXIII\"\n              }]\n            }]\n            }' 2> /dev/null[tuned_models.sh](https://github.com/google-gemini/generative-ai-python/blob/61867f68149a63fd0cf3c37bd46177a75ec5d8ed/samples/rest/tuned_models.sh#L101-L110)\n    \n\n## Delete the model\n\nYou can clean up your tuned model list by deleting models you no longer need. Use the [`tunedModels.delete`](/api/tuning#method:-tunedmodels.delete) method to delete a model. If you canceled any tuning jobs, you may want to delete those as their performance may be unpredictable.\n    \n    \n    curl -X DELETE https://generativelanguage.googleapis.com/v1beta/${modelname}?key=$GEMINI_API_KEY \\\n        -H 'Content-Type: application/json' [tuned_models.sh](https://github.com/google-gemini/generative-ai-python/blob/61867f68149a63fd0cf3c37bd46177a75ec5d8ed/samples/rest/tuned_models.sh#L140-L142)\n    \n\nSend feedback  Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-03 UTC. Need to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/quickstart#set-up-api-key",
    "title": "Gemini API quickstart  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\nSend feedback \n\n#  Gemini API quickstart \n\nThis quickstart shows you how to install your SDK of choice and then make your first Gemini API request. Python JavaScript REST Go Apps Script\n\n## Install the Gemini API library\n\n**Note:** We're rolling out a new set of Gemini API libraries, the [Google Gen AI SDK](/gemini-api/docs/sdks).\n\n## Make your first request\n\n[Get a Gemini API key in Google AI Studio](https://aistudio.google.com/app/apikey) Use the [`generateContent`](/api/generate-content#method:-models.generatecontent) method to send a request to the Gemini API.\n\n## What's next\n\nNow that you made your first API request, you might want to explore the following guides that show Gemini in action:\n\n  * [Text generation](/gemini-api/docs/text-generation)\n  * [Vision](/gemini-api/docs/vision)\n  * [Long context](/gemini-api/docs/long-context)\n\nSend feedback  Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-04 UTC. Need to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/text-generation?lang=node#chat",
    "title": "Text generation  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\nSend feedback \n\n#  Text generation \n\nThe Gemini API can generate text output in response to various inputs, including text, images, video, and audio. This guide shows you how to generate text using text and image inputs. It also covers streaming, chat, and system instructions.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Text input\n\nThe simplest way to generate text using the Gemini API is to provide the model with a single text-only input, as shown in this example:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[\"How does AI work?\"]\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"How does AI work?\",\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    // import packages here\n    \n    func main() {\n      ctx := context.Background()\n      client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n      if err != nil {\n        log.Fatal(err)\n      }\n      defer client.Close()\n    \n      model := client.GenerativeModel(\"gemini-2.0-flash\")\n      resp, err := model.GenerateContent(ctx, genai.Text(\"How does AI work?\"))\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp) // helper function for printing content parts\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"How does AI work?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { text: 'How AI does work?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Image input\n\nThe Gemini API supports multimodal inputs that combine text and media files. The following example shows how to generate text from text and image input:\n\n### Python\n    \n    \n    from PIL import Image\n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    image = Image.open(\"/path/to/organ.png\")\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[image, \"Tell me about this instrument\"]\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import {\n      GoogleGenAI,\n      createUserContent,\n      createPartFromUri,\n    } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const image = await ai.files.upload({\n        file: \"/path/to/organ.png\",\n      });\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: [\n          createUserContent([\n            \"Tell me about this instrument\",\n            createPartFromUri(image.uri, image.mimeType),\n          ]),\n        ],\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-2.0-flash\")\n    \n    imgData, err := os.ReadFile(filepath.Join(testDataDir, \"organ.jpg\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    \n    resp, err := model.GenerateContent(ctx,\n      genai.Text(\"Tell me about this instrument\"),\n      genai.ImageData(\"jpeg\", imgData))\n    if err != nil {\n      log.Fatal(err)\n    }\n    \n    printResponse(resp)\n    \n\n### REST\n    \n    \n    # Use a temporary file to hold the base64 encoded image data\n    TEMP_B64=$(mktemp)\n    trap 'rm -f \"$TEMP_B64\"' EXIT\n    base64 $B64FLAGS $IMG_PATH > \"$TEMP_B64\"\n    \n    # Use a temporary file to hold the JSON payload\n    TEMP_JSON=$(mktemp)\n    trap 'rm -f \"$TEMP_JSON\"' EXIT\n    \n    cat > \"$TEMP_JSON\" << EOF\n    {\n      \"contents\": [\n        {\n          \"parts\": [\n            {\n              \"text\": \"Tell me about this instrument\"\n            },\n            {\n              \"inline_data\": {\n                \"mime_type\": \"image/jpeg\",\n                \"data\": \"$(cat \"$TEMP_B64\")\"\n              }\n            }\n          ]\n        }\n      ]\n    }\n    EOF\n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d \"@$TEMP_JSON\"\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const imageUrl = 'http://image/url';\n      const image = getImageData(imageUrl);\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { image },\n              { text: 'Tell me about this instrument' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n    function getImageData(url) {\n      const blob = UrlFetchApp.fetch(url).getBlob();\n    \n      return {\n        mimeType: blob.getContentType(),\n        data: Utilities.base64Encode(blob.getBytes())\n      };\n    }\n    \n\n## Streaming output\n\nBy default, the model returns a response after completing the entire text generation process. You can achieve faster interactions by using streaming to return instances of [`GenerateContentResponse`](/api/generate-content#v1beta.GenerateContentResponse) as they're generated.\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content_stream(\n        model=\"gemini-2.0-flash\",\n        contents=[\"Explain how AI works\"]\n    )\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContentStream({\n        model: \"gemini-2.0-flash\",\n        contents: \"Explain how AI works\",\n      });\n    \n      for await (const chunk of response) {\n        console.log(chunk.text);\n      }\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    iter := model.GenerateContentStream(ctx, genai.Text(\"Write a story about a magic backpack.\"))\n    for {\n      resp, err := iter.Next()\n      if err == iterator.Done {\n        break\n      }\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp)\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse&key=${GEMINI_API_KEY}\" \\\n      -H 'Content-Type: application/json' \\\n      --no-buffer \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Explain how AI works\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { text: 'Explain how AI works' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Multi-turn conversations\n\nThe Gemini SDK lets you collect multiple rounds of questions and responses into a chat. The chat format enables users to step incrementally toward answers and to get help with multipart problems. This SDK implementation of chat provides an interface to keep track of conversation history, but behind the scenes it uses the same [`generateContent`](/api/generate-content#method:-models.generatecontent) method to create the response. The following code example shows a basic chat implementation:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    chat = client.chats.create(model=\"gemini-2.0-flash\")\n    \n    response = chat.send_message(\"I have 2 dogs in my house.\")\n    print(response.text)\n    \n    response = chat.send_message(\"How many paws are in my house?\")\n    print(response.text)\n    \n    for message in chat.get_history():\n        print(f'role - {message.role}',end=\": \")\n        print(message.parts[0].text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const chat = ai.chats.create({\n        model: \"gemini-2.0-flash\",\n        history: [\n          {\n            role: \"user\",\n            parts: [{ text: \"Hello\" }],\n          },\n          {\n            role: \"model\",\n            parts: [{ text: \"Great to meet you. What would you like to know?\" }],\n          },\n        ],\n      });\n    \n      const response1 = await chat.sendMessage({\n        message: \"I have 2 dogs in my house.\",\n      });\n      console.log(\"Chat response 1:\", response1.text);\n    \n      const response2 = await chat.sendMessage({\n        message: \"How many paws are in my house?\",\n      });\n      console.log(\"Chat response 2:\", response2.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    cs := model.StartChat()\n    \n    cs.History = []*genai.Content{\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Hello, I have 2 dogs in my house.\"),\n        },\n        Role: \"user\",\n      },\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Great to meet you. What would you like to know?\"),\n        },\n        Role: \"model\",\n      },\n    }\n    \n    res, err := cs.SendMessage(ctx, genai.Text(\"How many paws are in my house?\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    printResponse(res)\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"Hello\"\n              }\n            ]\n          },\n          {\n            \"role\": \"model\",\n            \"parts\": [\n              {\n                \"text\": \"Great to meet you. What would you like to know?\"\n              }\n            ]\n          },\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"I have two dogs in my house. How many paws are in my house?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            role: 'user',\n            parts: [\n              { text: 'Hello' },\n            ],\n          },\n          {\n            role: 'model',\n            parts: [\n              { text: 'Great to meet you. What would you like to know?' },\n            ],\n          },\n          {\n            role: 'user',\n            parts: [\n              { text: 'I have two dogs in my house. How many paws are in my house?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nYou can also use streaming with chat, as shown in the following example:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    chat = client.chats.create(model=\"gemini-2.0-flash\")\n    \n    response = chat.send_message_stream(\"I have 2 dogs in my house.\")\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n    response = chat.send_message_stream(\"How many paws are in my house?\")\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n    for message in chat.get_history():\n        print(f'role - {message.role}', end=\": \")\n        print(message.parts[0].text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const chat = ai.chats.create({\n        model: \"gemini-2.0-flash\",\n        history: [\n          {\n            role: \"user\",\n            parts: [{ text: \"Hello\" }],\n          },\n          {\n            role: \"model\",\n            parts: [{ text: \"Great to meet you. What would you like to know?\" }],\n          },\n        ],\n      });\n    \n      const stream1 = await chat.sendMessageStream({\n        message: \"I have 2 dogs in my house.\",\n      });\n      for await (const chunk of stream1) {\n        console.log(chunk.text);\n        console.log(\"_\".repeat(80));\n      }\n    \n      const stream2 = await chat.sendMessageStream({\n        message: \"How many paws are in my house?\",\n      });\n      for await (const chunk of stream2) {\n        console.log(chunk.text);\n        console.log(\"_\".repeat(80));\n      }\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    cs := model.StartChat()\n    \n    cs.History = []*genai.Content{\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Hello, I have 2 dogs in my house.\"),\n        },\n        Role: \"user\",\n      },\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Great to meet you. What would you like to know?\"),\n        },\n        Role: \"model\",\n      },\n    }\n    \n    iter := cs.SendMessageStream(ctx, genai.Text(\"How many paws are in my house?\"))\n    for {\n      resp, err := iter.Next()\n      if err == iterator.Done {\n        break\n      }\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp)\n    }\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse&key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"Hello\"\n              }\n            ]\n          },\n          {\n            \"role\": \"model\",\n            \"parts\": [\n              {\n                \"text\": \"Great to meet you. What would you like to know?\"\n              }\n            ]\n          },\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"I have two dogs in my house. How many paws are in my house?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            role: 'user',\n            parts: [\n              { text: 'Hello' },\n            ],\n          },\n          {\n            role: 'model',\n            parts: [\n              { text: 'Great to meet you. What would you like to know?' },\n            ],\n          },\n          {\n            role: 'user',\n            parts: [\n              { text: 'I have two dogs in my house. How many paws are in my house?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Configuration parameters\n\nEvery prompt you send to the model includes parameters that control how the model generates responses. You can configure these parameters, or let the model use the default options. The following example shows how to configure model parameters:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[\"Explain how AI works\"],\n        config=types.GenerateContentConfig(\n            max_output_tokens=500,\n            temperature=0.1\n        )\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Explain how AI works\",\n        config: {\n          maxOutputTokens: 500,\n          temperature: 0.1,\n        },\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-pro-latest\")\n    model.SetTemperature(0.9)\n    model.SetTopP(0.5)\n    model.SetTopK(20)\n    model.SetMaxOutputTokens(100)\n    model.SystemInstruction = genai.NewUserContent(genai.Text(\"You are Yoda from Star Wars.\"))\n    model.ResponseMIMEType = \"application/json\"\n    resp, err := model.GenerateContent(ctx, genai.Text(\"What is the average size of a swallow?\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    printResponse(resp)\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Explain how AI works\"\n              }\n            ]\n          }\n        ],\n        \"generationConfig\": {\n          \"stopSequences\": [\n            \"Title\"\n          ],\n          \"temperature\": 1.0,\n          \"maxOutputTokens\": 800,\n          \"topP\": 0.8,\n          \"topK\": 10\n        }\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const generationConfig = {\n        temperature: 1,\n        topP: 0.95,\n        topK: 40,\n        maxOutputTokens: 8192,\n        responseMimeType: 'text/plain',\n      };\n    \n      const payload = {\n        generationConfig,\n        contents: [\n          {\n            parts: [\n              { text: 'Explain how AI works in a few words' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nHere are some of the model parameters you can configure. (Naming conventions vary by programming language.)\n\n  * `stopSequences`: Specifies the set of character sequences (up to 5) that will stop output generation. If specified, the API will stop at the first appearance of a `stop_sequence`. The stop sequence won't be included as part of the response.\n  * `temperature`: Controls the randomness of the output. Use higher values for more creative responses, and lower values for more deterministic responses. Values can range from [0.0, 2.0].\n  * `maxOutputTokens`: Sets the maximum number of tokens to include in a candidate.\n  * `topP`: Changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the `topP` value. The default `topP` value is 0.95.\n  * `topK`: Changes how the model selects tokens for output. A `topK` of 1 means the selected token is the most probable among all the tokens in the model's vocabulary, while a `topK` of 3 means that the next token is selected from among the 3 most probable using the temperature. Tokens are further filtered based on `topP` with the final token selected using temperature sampling.\n\n\n\n## System instructions\n\nSystem instructions let you steer the behavior of a model based on your specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full interaction with the user, enabling you to specify product-level behavior separate from the prompts provided by end users. You can set system instructions when you initialize your model:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=types.GenerateContentConfig(\n            system_instruction=\"You are a cat. Your name is Neko.\"),\n        contents=\"Hello there\"\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Hello there\",\n        config: {\n          systemInstruction: \"You are a cat. Your name is Neko.\",\n        },\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    // import packages here\n    \n    func main() {\n      ctx := context.Background()\n      client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n      if err != nil {\n        log.Fatal(err)\n      }\n      defer client.Close()\n    \n      model := client.GenerativeModel(\"gemini-2.0-flash\")\n      model.SystemInstruction = &genai.Content{\n        Parts: []genai.Part{genai.Text(`\n          You are a cat. Your name is Neko.\n        `)},\n      }\n      resp, err := model.GenerateContent(ctx, genai.Text(\"Hello there\"))\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp) // helper function for printing content parts\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -d '{\n        \"system_instruction\": {\n          \"parts\": [\n            {\n              \"text\": \"You are a cat. Your name is Neko.\"\n            }\n          ]\n        },\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Hello there\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const systemInstruction = {\n        parts: [{\n          text: 'You are a cat. Your name is Neko.'\n        }]\n      };\n    \n      const payload = {\n        systemInstruction,\n        contents: [\n          {\n            parts: [\n              { text: 'Hello there' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nThen, you can send requests to the model as usual.\n\n## Supported models\n\nThe entire Gemini family of models supports text generation. To learn more about the models and their capabilities, see [Models](/gemini-api/docs/models).\n\n## Prompting tips\n\nFor basic text generation use cases, your prompt might not need to include any output examples, system instructions, or formatting information. This is a [zero-shot](/gemini-api/docs/models/generative-models#zero-shot-prompts) approach. For some use cases, a [one-shot](/gemini-api/docs/models/generative-models#one-shot-prompts) or [few-shot](/gemini-api/docs/models/generative-models#few-shot-prompts) prompt might produce output that's more aligned with user expectations. In some cases, you might also want to provide system instructions to help the model understand the task or follow specific guidelines.\n\n## What's next\n\n  * Try the [Gemini API getting started Colab](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb).\n  * Learn how to use Gemini's [vision understanding](/gemini-api/docs/vision) to process images and videos.\n  * Learn how to use Gemini's [audio understanding](/gemini-api/docs/audio) to process audio files.\n  * Learn about multimodal [file prompting strategies](/gemini-api/docs/file-prompting-strategies).\n\nSend feedback  Except as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates. Last updated 2025-04-04 UTC. Need to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/function-calling#parallel_function_calling",
    "title": "Function Calling with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\nSend feedback \n\n#  Function Calling with the Gemini API \n\nFunction calling lets you connect models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:\n\n  * **Augment Knowledge:** Access information from external sources like databases, APIs, and knowledge bases.\n  * **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.\n  * **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices\n\nGet Weather Schedule Meeting Create Chart\n\n## How Function Calling Works\n\nFunction calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:\n\n  1. **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.\n  2. **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\n  3. **Execute Function Code (Your Responsibility):** The Model _does not_ execute the function itself. It's your application's responsibility to process the response and check for Function Call, if \n     * **Yes** : Extract the name and args of the function and execute the corresponding function in your application.\n     * **No:** The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).\n  4. **Create User friendly response:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.\n\nThis process can be repeated over multiple turns, allowing for complex interactions and workflows. The model also supports calling multiple functions in a single turn ([parallel function calling](/gemini-api/docs/function-calling#parallel_function_calling)) and in sequence ([compositional function calling](/gemini-api/docs/function-calling#compositional_function_calling)).\n\n### Step 1: Define Function Declaration\n\nDefine a function and its declaration within your application code that allows users to set light values and make an API request. This function could call external services or APIs.\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Define a function that the model can call to control smart lights\n    set_light_values_declaration = {\n        \"name\": \"set_light_values\",\n        \"description\": \"Sets the brightness and color temperature of a light.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n                },\n                \"color_temp\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"daylight\", \"cool\", \"warm\"],\n                    \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n                },\n            },\n            \"required\": [\"brightness\", \"color_temp\"],\n        },\n    }\n    \n    # This is the actual function that would be called based on the model's suggestion\n    def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n        \"\"\"Set the brightness and color temperature of a room light. (mock API).\n    \n        Args:\n            brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n            color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    \n        Returns:\n            A dictionary containing the set brightness and color temperature.\n        \"\"\"\n        return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n    \n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    // Define a function that the model can call to control smart lights\n    const setLightValuesFunctionDeclaration = {\n      name: 'set_light_values',\n      description: 'Sets the brightness and color temperature of a light.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',\n          },\n          color_temp: {\n            type: Type.STRING,\n            enum: ['daylight', 'cool', 'warm'],\n            description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',\n          },\n        },\n        required: ['brightness', 'color_temp'],\n      },\n    };\n    \n    /**\n    * Set the brightness and color temperature of a room light. (mock API)\n    * @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness\n    * @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    * @return {Object} A dictionary containing the set brightness and color temperature.\n    */\n    function setLightValues(brightness, color_temp) {\n      return {\n        brightness: brightness,\n        colorTemperature: color_temp\n      };\n    }\n    \n\n### Step 2: Call the model with function declarations\n\nOnce you have defined your function declarations, you can prompt the model to use the function. It analyzes the prompt and function declarations and decides to respond directly or to call a function. If a function is called the response object will contain a function call suggestion.\n\n### Python\n    \n    \n    from google import genai\n    \n    # Generation Config with Function Declaration\n    tools = types.Tool(function_declarations=[set_light_values_declaration])\n    config = types.GenerateContentConfig(tools=[tools])\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Define user prompt\n    contents = [\n        types.Content(\n            role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n        )\n    ]\n    \n    # Send request with function declarations\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\", config=config, contents=contents\n    )\n    \n    print(response.candidates[0].content.parts[0].function_call)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Generation Config with Function Declaration\n    const config = {\n      tools: [{\n        functionDeclarations: [setLightValuesFunctionDeclaration]\n      }]\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Define user prompt\n    const contents = [\n      {\n        role: 'user',\n        parts: [{ text: 'Turn the lights down to a romantic level' }]\n      }\n    ];\n    \n    // Send request with function declarations\n    const response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(response.functionCalls[0]); \n    \n\nThe model then returns a `functionCall` object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.\n\n### Python\n    \n    \n    id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'\n    \n\n### JavaScript\n    \n    \n    {\n      name: 'set_light_values',\n      args: { brightness: 25, color_temp: 'warm' }\n    }\n    \n\n### Step 3: Execute set_light_values function code\n\nExtract the function call details from the model's response, parse the arguments , and execute the `set_light_values` function in our code.\n\n### Python\n    \n    \n    # Extract tool call details\n    tool_call = response.candidates[0].content.parts[0].function_call\n    \n    if tool_call.name == \"set_light_values\":\n        result = set_light_values(**tool_call.args)\n        print(f\"Function execution result: {result}\")\n    \n\n### JavaScript\n    \n    \n    // Extract tool call details\n    const tool_call = response.functionCalls[0]\n    \n    let result;\n    if (tool_call.name === 'set_light_values') {\n      result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);\n      console.log(`Function execution result: ${JSON.stringify(result)}`);\n    }\n    \n\n### Step 4: Create User friendly response with function result and call the model again\n\nFinally, send the result of the function execution back to the model so it can incorporate this information into its final response to the user.\n\n### Python\n    \n    \n    # Create a function response part\n    function_response_part = types.Part.from_function_response(\n        name=tool_call.name,\n        response={\"result\": result},\n    )\n    \n    # Append function call and result of the function execution to contents\n    contents.append(types.Content(role=\"model\", parts=[types.Part(function_call=tool_call)])) # Append the model's function call message\n    contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n    \n    final_response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=config,\n        contents=contents,\n    )\n    \n    print(final_response.text)\n    \n\n### JavaScript\n    \n    \n    // Create a function response part\n    const function_response_part = {\n      name: tool_call.name,\n      response: { result }\n    }\n    \n    // Append function call and result of the function execution to contents\n    contents.push({ role: 'model', parts: [{ functionCall: tool_call }] });\n    contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });\n    \n    // Get the final response from the model\n    const final_response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(final_response.text);\n    \n\nThis completes the function calling flow. The Model successfully used the `set_light_values` function to perform the request action of the user.\n\n## Function declarations\n\nWhen you implement function calling in a prompt, you create a `tools` object, which contains one or more _`function declarations`_. You define functions using JSON, specifically with a [select subset](https://ai.google.dev/api/caching#Schema) of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schemawr) format. A single function declaration can include the following parameters:\n\n  * `name` (string): A unique name for the function (`get_weather_forecast`, `send_email`). Use descriptive names without spaces or special characters (use underscores or camelCase).\n  * `description` (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful (\"Finds theaters based on location and optionally movie title which is currently playing in theaters.\").\n  * `parameters` (object): Defines the input parameters the function expects. \n    * `type` (string): Specifies the overall data type, such as `object`.\n    * `properties` (object): Lists individual parameters, each with: \n      * `type` (string): The data type of the parameter, such as `string`, `integer`, `boolean, array`.\n      * `description` (string): A description of the parameter's purpose and format. Provide examples and constraints (\"The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.\").\n      * `enum` (array, optional): If the parameter values are from a fixed set, use \"enum\" to list the allowed values instead of just describing them in the description. This improves accuracy (\"enum\": [\"daylight\", \"cool\", \"warm\"]).\n    * `required` (array): An array of strings listing the parameter names that are mandatory for the function to operate.\n\n\n\n## Parallel Function Calling\n\nIn addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco.\n\n### Python\n    \n    \n    power_disco_ball = {\n        \"name\": \"power_disco_ball\",\n        \"description\": \"Powers the spinning disco ball.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"power\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether to turn the disco ball on or off.\",\n                }\n            },\n            \"required\": [\"power\"],\n        },\n    }\n    \n    start_music = {\n        \"name\": \"start_music\",\n        \"description\": \"Play some music matching the specified parameters.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"energetic\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is energetic or not.\",\n                },\n                \"loud\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is loud or not.\",\n                },\n            },\n            \"required\": [\"energetic\", \"loud\"],\n        },\n    }\n    \n    dim_lights = {\n        \"name\": \"dim_lights\",\n        \"description\": \"Dim the lights.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"number\",\n                    \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n                }\n            },\n            \"required\": [\"brightness\"],\n        },\n    }\n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    const powerDiscoBall = {\n      name: 'power_disco_ball',\n      description: 'Powers the spinning disco ball.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          power: {\n            type: Type.BOOLEAN,\n            description: 'Whether to turn the disco ball on or off.'\n          }\n        },\n        required: ['power']\n      }\n    };\n    \n    const startMusic = {\n      name: 'start_music',\n      description: 'Play some music matching the specified parameters.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          energetic: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is energetic or not.'\n          },\n          loud: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is loud or not.'\n          }\n        },\n        required: ['energetic', 'loud']\n      }\n    };\n    \n    const dimLights = {\n      name: 'dim_lights',\n      description: 'Dim the lights.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'\n          }\n        },\n        required: ['brightness']\n      }\n    };\n    \n\nCall the model with an instruction that could use all of the specified tools. This example uses a `tool_config`. To learn more you can read about [configuring function calling](/gemini-api/docs/function-calling#function_calling_modes).\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Set up function declarations\n    house_tools = [\n        types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])\n    ]\n    \n    config = {\n        \"tools\": house_tools,\n        \"automatic_function_calling\": {\"disable\": True},\n        # Force the model to call 'any' function, instead of chatting.\n        \"tool_config\": {\"function_calling_config\": {\"mode\": \"any\"}},\n    }\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Turn this place into a party!\")\n    \n    # Print out each of the function calls requested from this single call\n    print(\"Example 1: Forced function calling\")\n    for fn in response.function_calls:\n        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n        print(f\"{fn.name}({args})\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Set up function declarations\n    const houseFns = [powerDiscoBall, startMusic, dimLights];\n    \n    const config = {\n        tools: [{\n            functionDeclarations: houseFns\n        }],\n        // Force the model to call 'any' function, instead of chatting.\n        toolConfig: {\n            functionCallingConfig: {\n            mode: 'any'\n            }\n        }\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Create a chat session\n    const chat = ai.chats.create({\n        model: 'gemini-2.0-flash',\n        config: config\n    });\n    const response = await chat.sendMessage({message: 'Turn this place into a party!'});\n    \n    // Print out each of the function calls requested from this single call\n    console.log(\"Example 1: Forced function calling\");\n    for (const fn of response.functionCalls) {\n        const args = Object.entries(fn.args)\n            .map(([key, val]) => `${key}=${val}`)\n            .join(', ');\n        console.log(`${fn.name}(${args})`);\n    }\n    \n\nEach of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested.  The Python SDK supports a feature called [automatic function calling](/gemini-api/docs/function-calling#automatic_function_calling_python_only) which converts the Python function to declarations, handles the function call execution and response cycle for you. Following is an example for our disco use case. **Note:** Automatic Function Calling is a Python SDK only feature at the moment.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Actual implementation functions\n    def power_disco_ball_impl(power: bool) -> dict:\n        \"\"\"Powers the spinning disco ball.\n    \n        Args:\n            power: Whether to turn the disco ball on or off.\n    \n        Returns:\n            A status dictionary indicating the current state.\n        \"\"\"\n        return {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n    \n    def start_music_impl(energetic: bool, loud: bool) -> dict:\n        \"\"\"Play some music matching the specified parameters.\n    \n        Args:\n            energetic: Whether the music is energetic or not.\n            loud: Whether the music is loud or not.\n    \n        Returns:\n            A dictionary containing the music settings.\n        \"\"\"\n        music_type = \"energetic\" if energetic else \"chill\"\n        volume = \"loud\" if loud else \"quiet\"\n        return {\"music_type\": music_type, \"volume\": volume}\n    \n    def dim_lights_impl(brightness: float) -> dict:\n        \"\"\"Dim the lights.\n    \n        Args:\n            brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n    \n        Returns:\n            A dictionary containing the new brightness setting.\n        \"\"\"\n        return {\"brightness\": brightness}\n    \n    config = {\n        \"tools\": [power_disco_ball_impl, start_music_impl, dim_lights_impl],\n    }\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Do everything you need to this place into party!\")\n    \n    print(\"\\nExample 2: Automatic function calling\")\n    print(response.text)\n    # I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!\n    \n\n## Compositional Function Calling\n\nGemini 2.0 supports compositional function calling, meaning the model can chain multiple function calls together. For example, to answer \"Get the temperature in my current location\", the Gemini API might invoke both a `get_current_location()` function and a `get_weather()` function that takes the location as a parameter. **Note:** Compositional function calling is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Light control schemas\n    turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}\n    turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}\n    \n    prompt = \"\"\"\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n      \"\"\"\n    \n    tools = [\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}\n    ]\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Light control schemas\n    const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };\n    const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };\n    \n    const prompt = `\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n    `;\n    \n    const tools = [\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }\n    ];\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n## Function calling modes\n\nThe Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the `function_calling_config`.\n\n  * `AUTO (Default)`: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n  * `ANY`: The model is constrained to always predict a function call and guarantee function schema adherence. If `allowed_function_names` is not specified, the model can choose from any of the provided function declarations. If `allowed_function_names` is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call in response to every prompt (if applicable).\n  * `NONE`: The model is _prohibited_ from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.\n\n\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Configure function calling mode\n    tool_config = types.ToolConfig(\n        function_calling_config=types.FunctionCallingConfig(\n            mode=\"ANY\", allowed_function_names=[\"get_current_temperature\"]\n        )\n    )\n    \n    # Create the generation config\n    config = types.GenerateContentConfig(\n        temperature=0,\n        tools=[tools],  # not defined here.\n        tool_config=tool_config,\n    )\n    \n\n### JavaScript\n    \n    \n    import { FunctionCallingConfigMode } from '@google/genai';\n    \n    // Configure function calling mode\n    const toolConfig = {\n      functionCallingConfig: {\n        mode: FunctionCallingConfigMode.ANY,\n        allowedFunctionNames: ['get_current_temperature']\n      }\n    };\n    \n    // Create the generation config\n    const config = {\n      temperature: 0,\n      tools: tools, // not defined here.\n      toolConfig: toolConfig,\n    };\n    \n\n## Automatic Function Calling (Python Only)\n\nWhen using the Python SDK, you can provide Python functions directly as tools. The SDK automatically converts the Python function to declarations, handles the function call execution and response cycle for you. The Python SDK then automatically:\n\n  1. Detects function call responses from the model.\n  2. Call the corresponding Python function in your code.\n  3. Sends the function response back to the model.\n  4. Returns the model's final text response.\n\nTo use this, define your function with type hints and a docstring, and then pass the function itself (not a JSON declaration) as a tool:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Define the function with type hints and docstring\n    def get_current_temperature(location: str) -> dict:\n        \"\"\"Gets the current temperature for a given location.\n    \n        Args:\n            location: The city and state, e.g. San Francisco, CA\n    \n        Returns:\n            A dictionary containing the temperature and unit.\n        \"\"\"\n        # ... (implementation) ...\n        return {\"temperature\": 25, \"unit\": \"Celsius\"}\n    \n    # Configure the client and model\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))  # Replace with your actual API key setup\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature]\n    )  # Pass the function itself\n    \n    # Make the request\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=\"What's the temperature in London?\",\n        config=config,\n    )\n    \n    print(response.text)  # The SDK handles the function call and returns the final text\n    \n\nYou can disable automatic function calling with:\n\n### Python\n    \n    \n    # To disable automatic function calling:\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature],\n        automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n    )\n    \n\n### Automatic Function schema declaration\n\nAutomatic schema extraction from Python functions doesn't work in all cases. For example: it doesn't handle cases where you describe the fields of a nested dictionary-object. The API is able to describe any of the following types:\n\n### Python\n    \n    \n    AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])\n    \n\nTo see what the inferred schema looks like, you can convert it using [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable):\n\n### Python\n    \n    \n    def multiply(a: float, b: float):\n        \"\"\"Returns a * b.\"\"\"\n        return a * b\n    \n    fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)\n    \n    # to_json_dict() provides a clean JSON representation.\n    print(fn_decl.to_json_dict())\n    \n\n## Multi-tool use: Combine Native Tools with Function Calling\n\nWith Gemini 2.0, you can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, [Grounding with Google Search](/gemini-api/docs/grounding) and [code execution](/gemini-api/docs/code-execution), in a request using the [Live API](/gemini-api/docs/live). **Note:** Multi-tool use is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Multiple tasks example - combining lights, code execution, and search\n    prompt = \"\"\"\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n      \"\"\"\n    \n    tools = [\n        {'google_search': {}},\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.\n    ]\n    \n    # Execute the prompt with specified tools in audio modality\n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Multiple tasks example - combining lights, code execution, and search\n    const prompt = `\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n    `;\n    \n    const tools = [\n      { googleSearch: {} },\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.\n    ];\n    \n    // Execute the prompt with specified tools in audio modality\n    await run(prompt, {tools: tools, modality: \"AUDIO\"});\n    \n\nPython developers can try this out in the [Live API Tool Use notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).\n\n## Use Model Context Protocol (MCP)\n\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard to connect AI applications with external tools, data sources, and systems. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts. You can use models with MCP server using their tool calling capabilities. MCP servers expose the tools as JSON schema definitions, which can be used with Gemini compatible function declarations. This lets you to use a MCP server with Gemini models directly. Here, you can find an example of how to use a local MCP server with Gemini SDK and the `mcp` SDK.\n\n### Python\n    \n    \n    import asyncio\n    import os\n    from datetime import datetime\n    from google import genai\n    from google.genai import types\n    from mcp import ClientSession, StdioServerParameters\n    from mcp.client.stdio import stdio_client\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Create server parameters for stdio connection\n    server_params = StdioServerParameters(\n        command=\"npx\",  # Executable\n        args=[\"-y\", \"@philschmid/weather-mcp\"],  # Weather MCP Server\n        env=None,  # Optional environment variables\n    )\n    \n    async def run():\n        async with stdio_client(server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                # Prompt to get the weather for the current day in London.\n                prompt = f\"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?\"\n                # Initialize the connection between client and server\n                await session.initialize()\n    \n                # Get tools from MCP session and convert to Gemini Tool objects\n                mcp_tools = await session.list_tools()\n                tools = [\n                    types.Tool(\n                        function_declarations=[\n                            {\n                                \"name\": tool.name,\n                                \"description\": tool.description,\n                                \"parameters\": {\n                                    k: v\n                                    for k, v in tool.inputSchema.items()\n                                    if k not in [\"additionalProperties\", \"$schema\"]\n                                },\n                            }\n                        ]\n                    )\n                    for tool in mcp_tools.tools\n                ]\n    \n                # Send request to the model with MCP function declarations\n                response = client.models.generate_content(\n                    model=\"gemini-2.0-flash\",\n                    contents=prompt,\n                    config=types.GenerateContentConfig(\n                        temperature=0,\n                        tools=tools,\n                    ),\n                )\n    \n                # Check for a function call\n                if response.candidates[0].content.parts[0].function_call:\n                    function_call = response.candidates[0].content.parts[0].function_call\n                    print(function_call)\n                    # Call the MCP server with the predicted tool\n                    result = await session.call_tool(\n                        function_call.name, arguments=function_call.args\n                    )\n                    print(result.content[0].text)\n                    # Continue as shown in step 4 of \"How Function Calling Works\"\n                    # and create a user friendly response\n                else:\n                    print(\"No function call found in the response.\")\n                    print(response.text)\n    \n    # Start the asyncio event loop and run the main function\n    asyncio.run(run())\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    import { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\n    import { StdioClientTransport } from \"@modelcontextprotocol/sdk/client/stdio.js\";\n    \n    // Create server parameters for stdio connection\n    const serverParams = new StdioClientTransport({\n      command: \"npx\",\n      args: [\"-y\", \"@philschmid/weather-mcp\"]\n    });\n    \n    const client = new Client(\n      {\n        name: \"example-client\",\n        version: \"1.0.0\"\n      }\n    );\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Initialize the connection between client and server\n    await client.connect(serverParams);\n    \n    // Get tools from MCP session and convert to Gemini Tool objects\n    const mcpTools = await client.listTools();\n    const tools = mcpTools.tools.map((tool) => {\n      // Filter the parameters to exclude not supported keys\n      const parameters = Object.fromEntries(\n        Object.entries(tool.inputSchema).filter(([key]) => ![\"additionalProperties\", \"$schema\"].includes(key))\n      );\n      return {\n        name: tool.name,\n        description: tool.description,\n        parameters: parameters\n      };\n    });\n    \n    // Send request to the model with MCP function declarations\n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: \"What is the weather in London in the UK on 2024-04-04?\",\n      config: {\n        tools: [{\n          functionDeclarations: tools\n        }],\n      },\n    });\n    \n    // Check for function calls in the response\n    if (response.functionCalls && response.functionCalls.length > 0) {\n      const functionCall = response.functionCalls[0]; // Assuming one function call\n      console.log(`Function to call: ${functionCall.name}`);\n      console.log(`Arguments: ${JSON.stringify(functionCall.args)}`);\n      // Call the MCP server with the predicted tool\n      const result = await client.callTool({name: functionCall.name, arguments: functionCall.args});\n      console.log(result.content[0].text);\n      // Continue as shown in step 4 of \"How Function Calling Works\"\n      // and create a user friendly response\n    } else {\n      console.log(\"No function call found in the response.\");\n      console.log(response.text);\n    }\n    \n    // Close the connection\n    await client.close();\n    \n\n## Supported Models\n\nExperimental models are not included. You can find their capabilities on the [model overview](https://ai.google.dev/gemini-api/docs/models) page. | Model | Function Calling | Parallel Function Calling | Compositional Function Calling(Live API only)  \n---|---|---|---  \nGemini 2.0 Flash | ✔️ | ✔️ | ✔️  \nGemini 2.0 Flash-Lite | X | X | X  \nGemini 1.5 Flash | ✔️ | ✔️ | ✔️  \nGemini 1.5 Pro | ✔️ | ✔️ | ✔️  \n  \n## Best Practices\n\n  * **Function and Parameter Descriptions:** Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.\n  * **Naming:** Use descriptive function names (without spaces, periods, or dashes).\n  * **Strong Typing:** Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.\n  * **Tool Selection:** While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.\n  * **Prompt Engineering:**\n    * Provide context: Tell the model its role (e.g., \"You are a helpful weather assistant.\").\n    * Give instructions: Specify how and when to use functions (e.g., \"Don't guess dates; always use a future date for forecasts.\").\n    * Encourage clarification: Instruct the model to ask clarifying questions if needed.\n  * **Temperature:** Use a low temperature (e.g., 0) for more deterministic and reliable function calls.\n  * **Validation:** If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.\n  * **Error Handling** : Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.\n  * **Security:** Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.\n  * **Token Limits:** Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.\n\n\n\n## Notes and Limitations\n\n  * Only a [subset of the OpenAPI schema](https://ai.google.dev/api/caching#FunctionDeclaration) is supported.\n  * Supported parameter types in Python are limited.\n  * Automatic function calling is a Python SDK feature only.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-07 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-07 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/function-calling#compositional_function_calling",
    "title": "Function Calling with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Function Calling with the Gemini API \n\nFunction calling lets you connect models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:\n\n  * **Augment Knowledge:** Access information from external sources like databases, APIs, and knowledge bases.\n  * **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.\n  * **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices\n\n\n\nGet Weather Schedule Meeting Create Chart\n\n## How Function Calling Works\n\nFunction calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:\n\n  1. **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.\n  2. **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\n  3. **Execute Function Code (Your Responsibility):** The Model _does not_ execute the function itself. It's your application's responsibility to process the response and check for Function Call, if \n     * **Yes** : Extract the name and args of the function and execute the corresponding function in your application.\n     * **No:** The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).\n  4. **Create User friendly response:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.\n\n\n\nThis process can be repeated over multiple turns, allowing for complex interactions and workflows. The model also supports calling multiple functions in a single turn ([parallel function calling](/gemini-api/docs/function-calling#parallel_function_calling)) and in sequence ([compositional function calling](/gemini-api/docs/function-calling#compositional_function_calling)).\n\n### Step 1: Define Function Declaration\n\nDefine a function and its declaration within your application code that allows users to set light values and make an API request. This function could call external services or APIs.\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Define a function that the model can call to control smart lights\n    set_light_values_declaration = {\n        \"name\": \"set_light_values\",\n        \"description\": \"Sets the brightness and color temperature of a light.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n                },\n                \"color_temp\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"daylight\", \"cool\", \"warm\"],\n                    \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n                },\n            },\n            \"required\": [\"brightness\", \"color_temp\"],\n        },\n    }\n    \n    # This is the actual function that would be called based on the model's suggestion\n    def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n        \"\"\"Set the brightness and color temperature of a room light. (mock API).\n    \n        Args:\n            brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n            color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    \n        Returns:\n            A dictionary containing the set brightness and color temperature.\n        \"\"\"\n        return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n    \n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    // Define a function that the model can call to control smart lights\n    const setLightValuesFunctionDeclaration = {\n      name: 'set_light_values',\n      description: 'Sets the brightness and color temperature of a light.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',\n          },\n          color_temp: {\n            type: Type.STRING,\n            enum: ['daylight', 'cool', 'warm'],\n            description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',\n          },\n        },\n        required: ['brightness', 'color_temp'],\n      },\n    };\n    \n    /**\n    * Set the brightness and color temperature of a room light. (mock API)\n    * @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness\n    * @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    * @return {Object} A dictionary containing the set brightness and color temperature.\n    */\n    function setLightValues(brightness, color_temp) {\n      return {\n        brightness: brightness,\n        colorTemperature: color_temp\n      };\n    }\n    \n\n### Step 2: Call the model with function declarations\n\nOnce you have defined your function declarations, you can prompt the model to use the function. It analyzes the prompt and function declarations and decides to respond directly or to call a function. If a function is called the response object will contain a function call suggestion.\n\n### Python\n    \n    \n    from google import genai\n    \n    # Generation Config with Function Declaration\n    tools = types.Tool(function_declarations=[set_light_values_declaration])\n    config = types.GenerateContentConfig(tools=[tools])\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Define user prompt\n    contents = [\n        types.Content(\n            role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n        )\n    ]\n    \n    # Send request with function declarations\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\", config=config, contents=contents\n    )\n    \n    print(response.candidates[0].content.parts[0].function_call)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Generation Config with Function Declaration\n    const config = {\n      tools: [{\n        functionDeclarations: [setLightValuesFunctionDeclaration]\n      }]\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Define user prompt\n    const contents = [\n      {\n        role: 'user',\n        parts: [{ text: 'Turn the lights down to a romantic level' }]\n      }\n    ];\n    \n    // Send request with function declarations\n    const response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(response.functionCalls[0]); \n    \n\nThe model then returns a `functionCall` object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.\n\n### Python\n    \n    \n    id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'\n    \n\n### JavaScript\n    \n    \n    {\n      name: 'set_light_values',\n      args: { brightness: 25, color_temp: 'warm' }\n    }\n    \n\n### Step 3: Execute set_light_values function code\n\nExtract the function call details from the model's response, parse the arguments , and execute the `set_light_values` function in our code.\n\n### Python\n    \n    \n    # Extract tool call details\n    tool_call = response.candidates[0].content.parts[0].function_call\n    \n    if tool_call.name == \"set_light_values\":\n        result = set_light_values(**tool_call.args)\n        print(f\"Function execution result: {result}\")\n    \n\n### JavaScript\n    \n    \n    // Extract tool call details\n    const tool_call = response.functionCalls[0]\n    \n    let result;\n    if (tool_call.name === 'set_light_values') {\n      result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);\n      console.log(`Function execution result: ${JSON.stringify(result)}`);\n    }\n    \n\n### Step 4: Create User friendly response with function result and call the model again\n\nFinally, send the result of the function execution back to the model so it can incorporate this information into its final response to the user.\n\n### Python\n    \n    \n    # Create a function response part\n    function_response_part = types.Part.from_function_response(\n        name=tool_call.name,\n        response={\"result\": result},\n    )\n    \n    # Append function call and result of the function execution to contents\n    contents.append(types.Content(role=\"model\", parts=[types.Part(function_call=tool_call)])) # Append the model's function call message\n    contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n    \n    final_response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=config,\n        contents=contents,\n    )\n    \n    print(final_response.text)\n    \n\n### JavaScript\n    \n    \n    // Create a function response part\n    const function_response_part = {\n      name: tool_call.name,\n      response: { result }\n    }\n    \n    // Append function call and result of the function execution to contents\n    contents.push({ role: 'model', parts: [{ functionCall: tool_call }] });\n    contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });\n    \n    // Get the final response from the model\n    const final_response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(final_response.text);\n    \n\nThis completes the function calling flow. The Model successfully used the `set_light_values` function to perform the request action of the user.\n\n## Function declarations\n\nWhen you implement function calling in a prompt, you create a `tools` object, which contains one or more _`function declarations`_. You define functions using JSON, specifically with a [select subset](https://ai.google.dev/api/caching#Schema) of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schemawr) format. A single function declaration can include the following parameters:\n\n  * `name` (string): A unique name for the function (`get_weather_forecast`, `send_email`). Use descriptive names without spaces or special characters (use underscores or camelCase).\n  * `description` (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful (\"Finds theaters based on location and optionally movie title which is currently playing in theaters.\").\n  * `parameters` (object): Defines the input parameters the function expects. \n    * `type` (string): Specifies the overall data type, such as `object`.\n    * `properties` (object): Lists individual parameters, each with: \n      * `type` (string): The data type of the parameter, such as `string`, `integer`, `boolean, array`.\n      * `description` (string): A description of the parameter's purpose and format. Provide examples and constraints (\"The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.\").\n      * `enum` (array, optional): If the parameter values are from a fixed set, use \"enum\" to list the allowed values instead of just describing them in the description. This improves accuracy (\"enum\": [\"daylight\", \"cool\", \"warm\"]).\n    * `required` (array): An array of strings listing the parameter names that are mandatory for the function to operate.\n\n\n\n## Parallel Function Calling\n\nIn addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco.\n\n### Python\n    \n    \n    power_disco_ball = {\n        \"name\": \"power_disco_ball\",\n        \"description\": \"Powers the spinning disco ball.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"power\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether to turn the disco ball on or off.\",\n                }\n            },\n            \"required\": [\"power\"],\n        },\n    }\n    \n    start_music = {\n        \"name\": \"start_music\",\n        \"description\": \"Play some music matching the specified parameters.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"energetic\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is energetic or not.\",\n                },\n                \"loud\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is loud or not.\",\n                },\n            },\n            \"required\": [\"energetic\", \"loud\"],\n        },\n    }\n    \n    dim_lights = {\n        \"name\": \"dim_lights\",\n        \"description\": \"Dim the lights.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"number\",\n                    \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n                }\n            },\n            \"required\": [\"brightness\"],\n        },\n    }\n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    const powerDiscoBall = {\n      name: 'power_disco_ball',\n      description: 'Powers the spinning disco ball.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          power: {\n            type: Type.BOOLEAN,\n            description: 'Whether to turn the disco ball on or off.'\n          }\n        },\n        required: ['power']\n      }\n    };\n    \n    const startMusic = {\n      name: 'start_music',\n      description: 'Play some music matching the specified parameters.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          energetic: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is energetic or not.'\n          },\n          loud: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is loud or not.'\n          }\n        },\n        required: ['energetic', 'loud']\n      }\n    };\n    \n    const dimLights = {\n      name: 'dim_lights',\n      description: 'Dim the lights.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'\n          }\n        },\n        required: ['brightness']\n      }\n    };\n    \n\nCall the model with an instruction that could use all of the specified tools. This example uses a `tool_config`. To learn more you can read about [configuring function calling](/gemini-api/docs/function-calling#function_calling_modes).\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Set up function declarations\n    house_tools = [\n        types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])\n    ]\n    \n    config = {\n        \"tools\": house_tools,\n        \"automatic_function_calling\": {\"disable\": True},\n        # Force the model to call 'any' function, instead of chatting.\n        \"tool_config\": {\"function_calling_config\": {\"mode\": \"any\"}},\n    }\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Turn this place into a party!\")\n    \n    # Print out each of the function calls requested from this single call\n    print(\"Example 1: Forced function calling\")\n    for fn in response.function_calls:\n        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n        print(f\"{fn.name}({args})\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Set up function declarations\n    const houseFns = [powerDiscoBall, startMusic, dimLights];\n    \n    const config = {\n        tools: [{\n            functionDeclarations: houseFns\n        }],\n        // Force the model to call 'any' function, instead of chatting.\n        toolConfig: {\n            functionCallingConfig: {\n            mode: 'any'\n            }\n        }\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Create a chat session\n    const chat = ai.chats.create({\n        model: 'gemini-2.0-flash',\n        config: config\n    });\n    const response = await chat.sendMessage({message: 'Turn this place into a party!'});\n    \n    // Print out each of the function calls requested from this single call\n    console.log(\"Example 1: Forced function calling\");\n    for (const fn of response.functionCalls) {\n        const args = Object.entries(fn.args)\n            .map(([key, val]) => `${key}=${val}`)\n            .join(', ');\n        console.log(`${fn.name}(${args})`);\n    }\n    \n\nEach of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested. \n\nThe Python SDK supports a feature called [automatic function calling](/gemini-api/docs/function-calling#automatic_function_calling_python_only) which converts the Python function to declarations, handles the function call execution and response cycle for you. Following is an example for our disco use case.\n\n**Note:** Automatic Function Calling is a Python SDK only feature at the moment.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Actual implementation functions\n    def power_disco_ball_impl(power: bool) -> dict:\n        \"\"\"Powers the spinning disco ball.\n    \n        Args:\n            power: Whether to turn the disco ball on or off.\n    \n        Returns:\n            A status dictionary indicating the current state.\n        \"\"\"\n        return {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n    \n    def start_music_impl(energetic: bool, loud: bool) -> dict:\n        \"\"\"Play some music matching the specified parameters.\n    \n        Args:\n            energetic: Whether the music is energetic or not.\n            loud: Whether the music is loud or not.\n    \n        Returns:\n            A dictionary containing the music settings.\n        \"\"\"\n        music_type = \"energetic\" if energetic else \"chill\"\n        volume = \"loud\" if loud else \"quiet\"\n        return {\"music_type\": music_type, \"volume\": volume}\n    \n    def dim_lights_impl(brightness: float) -> dict:\n        \"\"\"Dim the lights.\n    \n        Args:\n            brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n    \n        Returns:\n            A dictionary containing the new brightness setting.\n        \"\"\"\n        return {\"brightness\": brightness}\n    \n    config = {\n        \"tools\": [power_disco_ball_impl, start_music_impl, dim_lights_impl],\n    }\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Do everything you need to this place into party!\")\n    \n    print(\"\\nExample 2: Automatic function calling\")\n    print(response.text)\n    # I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!\n    \n\n## Compositional Function Calling\n\nGemini 2.0 supports compositional function calling, meaning the model can chain multiple function calls together. For example, to answer \"Get the temperature in my current location\", the Gemini API might invoke both a `get_current_location()` function and a `get_weather()` function that takes the location as a parameter.\n\n**Note:** Compositional function calling is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Light control schemas\n    turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}\n    turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}\n    \n    prompt = \"\"\"\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n      \"\"\"\n    \n    tools = [\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}\n    ]\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Light control schemas\n    const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };\n    const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };\n    \n    const prompt = `\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n    `;\n    \n    const tools = [\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }\n    ];\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n## Function calling modes\n\nThe Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the `function_calling_config`.\n\n  * `AUTO (Default)`: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n  * `ANY`: The model is constrained to always predict a function call and guarantee function schema adherence. If `allowed_function_names` is not specified, the model can choose from any of the provided function declarations. If `allowed_function_names` is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call in response to every prompt (if applicable).\n  * `NONE`: The model is _prohibited_ from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.\n\n\n\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Configure function calling mode\n    tool_config = types.ToolConfig(\n        function_calling_config=types.FunctionCallingConfig(\n            mode=\"ANY\", allowed_function_names=[\"get_current_temperature\"]\n        )\n    )\n    \n    # Create the generation config\n    config = types.GenerateContentConfig(\n        temperature=0,\n        tools=[tools],  # not defined here.\n        tool_config=tool_config,\n    )\n    \n\n### JavaScript\n    \n    \n    import { FunctionCallingConfigMode } from '@google/genai';\n    \n    // Configure function calling mode\n    const toolConfig = {\n      functionCallingConfig: {\n        mode: FunctionCallingConfigMode.ANY,\n        allowedFunctionNames: ['get_current_temperature']\n      }\n    };\n    \n    // Create the generation config\n    const config = {\n      temperature: 0,\n      tools: tools, // not defined here.\n      toolConfig: toolConfig,\n    };\n    \n\n## Automatic Function Calling (Python Only)\n\nWhen using the Python SDK, you can provide Python functions directly as tools. The SDK automatically converts the Python function to declarations, handles the function call execution and response cycle for you. The Python SDK then automatically:\n\n  1. Detects function call responses from the model.\n  2. Call the corresponding Python function in your code.\n  3. Sends the function response back to the model.\n  4. Returns the model's final text response.\n\n\n\nTo use this, define your function with type hints and a docstring, and then pass the function itself (not a JSON declaration) as a tool:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Define the function with type hints and docstring\n    def get_current_temperature(location: str) -> dict:\n        \"\"\"Gets the current temperature for a given location.\n    \n        Args:\n            location: The city and state, e.g. San Francisco, CA\n    \n        Returns:\n            A dictionary containing the temperature and unit.\n        \"\"\"\n        # ... (implementation) ...\n        return {\"temperature\": 25, \"unit\": \"Celsius\"}\n    \n    # Configure the client and model\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))  # Replace with your actual API key setup\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature]\n    )  # Pass the function itself\n    \n    # Make the request\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=\"What's the temperature in London?\",\n        config=config,\n    )\n    \n    print(response.text)  # The SDK handles the function call and returns the final text\n    \n\nYou can disable automatic function calling with:\n\n### Python\n    \n    \n    # To disable automatic function calling:\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature],\n        automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n    )\n    \n\n### Automatic Function schema declaration\n\nAutomatic schema extraction from Python functions doesn't work in all cases. For example: it doesn't handle cases where you describe the fields of a nested dictionary-object. The API is able to describe any of the following types:\n\n### Python\n    \n    \n    AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])\n    \n\nTo see what the inferred schema looks like, you can convert it using [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable):\n\n### Python\n    \n    \n    def multiply(a: float, b: float):\n        \"\"\"Returns a * b.\"\"\"\n        return a * b\n    \n    fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)\n    \n    # to_json_dict() provides a clean JSON representation.\n    print(fn_decl.to_json_dict())\n    \n\n## Multi-tool use: Combine Native Tools with Function Calling\n\nWith Gemini 2.0, you can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, [Grounding with Google Search](/gemini-api/docs/grounding) and [code execution](/gemini-api/docs/code-execution), in a request using the [Live API](/gemini-api/docs/live).\n\n**Note:** Multi-tool use is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Multiple tasks example - combining lights, code execution, and search\n    prompt = \"\"\"\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n      \"\"\"\n    \n    tools = [\n        {'google_search': {}},\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.\n    ]\n    \n    # Execute the prompt with specified tools in audio modality\n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Multiple tasks example - combining lights, code execution, and search\n    const prompt = `\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n    `;\n    \n    const tools = [\n      { googleSearch: {} },\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.\n    ];\n    \n    // Execute the prompt with specified tools in audio modality\n    await run(prompt, {tools: tools, modality: \"AUDIO\"});\n    \n\nPython developers can try this out in the [Live API Tool Use notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).\n\n## Use Model Context Protocol (MCP)\n\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard to connect AI applications with external tools, data sources, and systems. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts. You can use models with MCP server using their tool calling capabilities.\n\nMCP servers expose the tools as JSON schema definitions, which can be used with Gemini compatible function declarations. This lets you to use a MCP server with Gemini models directly. Here, you can find an example of how to use a local MCP server with Gemini SDK and the `mcp` SDK.\n\n### Python\n    \n    \n    import asyncio\n    import os\n    from datetime import datetime\n    from google import genai\n    from google.genai import types\n    from mcp import ClientSession, StdioServerParameters\n    from mcp.client.stdio import stdio_client\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Create server parameters for stdio connection\n    server_params = StdioServerParameters(\n        command=\"npx\",  # Executable\n        args=[\"-y\", \"@philschmid/weather-mcp\"],  # Weather MCP Server\n        env=None,  # Optional environment variables\n    )\n    \n    async def run():\n        async with stdio_client(server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                # Prompt to get the weather for the current day in London.\n                prompt = f\"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?\"\n                # Initialize the connection between client and server\n                await session.initialize()\n    \n                # Get tools from MCP session and convert to Gemini Tool objects\n                mcp_tools = await session.list_tools()\n                tools = [\n                    types.Tool(\n                        function_declarations=[\n                            {\n                                \"name\": tool.name,\n                                \"description\": tool.description,\n                                \"parameters\": {\n                                    k: v\n                                    for k, v in tool.inputSchema.items()\n                                    if k not in [\"additionalProperties\", \"$schema\"]\n                                },\n                            }\n                        ]\n                    )\n                    for tool in mcp_tools.tools\n                ]\n    \n                # Send request to the model with MCP function declarations\n                response = client.models.generate_content(\n                    model=\"gemini-2.0-flash\",\n                    contents=prompt,\n                    config=types.GenerateContentConfig(\n                        temperature=0,\n                        tools=tools,\n                    ),\n                )\n    \n                # Check for a function call\n                if response.candidates[0].content.parts[0].function_call:\n                    function_call = response.candidates[0].content.parts[0].function_call\n                    print(function_call)\n                    # Call the MCP server with the predicted tool\n                    result = await session.call_tool(\n                        function_call.name, arguments=function_call.args\n                    )\n                    print(result.content[0].text)\n                    # Continue as shown in step 4 of \"How Function Calling Works\"\n                    # and create a user friendly response\n                else:\n                    print(\"No function call found in the response.\")\n                    print(response.text)\n    \n    # Start the asyncio event loop and run the main function\n    asyncio.run(run())\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    import { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\n    import { StdioClientTransport } from \"@modelcontextprotocol/sdk/client/stdio.js\";\n    \n    // Create server parameters for stdio connection\n    const serverParams = new StdioClientTransport({\n      command: \"npx\",\n      args: [\"-y\", \"@philschmid/weather-mcp\"]\n    });\n    \n    const client = new Client(\n      {\n        name: \"example-client\",\n        version: \"1.0.0\"\n      }\n    );\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Initialize the connection between client and server\n    await client.connect(serverParams);\n    \n    // Get tools from MCP session and convert to Gemini Tool objects\n    const mcpTools = await client.listTools();\n    const tools = mcpTools.tools.map((tool) => {\n      // Filter the parameters to exclude not supported keys\n      const parameters = Object.fromEntries(\n        Object.entries(tool.inputSchema).filter(([key]) => ![\"additionalProperties\", \"$schema\"].includes(key))\n      );\n      return {\n        name: tool.name,\n        description: tool.description,\n        parameters: parameters\n      };\n    });\n    \n    // Send request to the model with MCP function declarations\n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: \"What is the weather in London in the UK on 2024-04-04?\",\n      config: {\n        tools: [{\n          functionDeclarations: tools\n        }],\n      },\n    });\n    \n    // Check for function calls in the response\n    if (response.functionCalls && response.functionCalls.length > 0) {\n      const functionCall = response.functionCalls[0]; // Assuming one function call\n      console.log(`Function to call: ${functionCall.name}`);\n      console.log(`Arguments: ${JSON.stringify(functionCall.args)}`);\n      // Call the MCP server with the predicted tool\n      const result = await client.callTool({name: functionCall.name, arguments: functionCall.args});\n      console.log(result.content[0].text);\n      // Continue as shown in step 4 of \"How Function Calling Works\"\n      // and create a user friendly response\n    } else {\n      console.log(\"No function call found in the response.\");\n      console.log(response.text);\n    }\n    \n    // Close the connection\n    await client.close();\n    \n\n## Supported Models\n\nExperimental models are not included. You can find their capabilities on the [model overview](https://ai.google.dev/gemini-api/docs/models) page.\n\nModel | Function Calling | Parallel Function Calling | Compositional Function Calling(Live API only)  \n---|---|---|---  \nGemini 2.0 Flash | ✔️ | ✔️ | ✔️  \nGemini 2.0 Flash-Lite | X | X | X  \nGemini 1.5 Flash | ✔️ | ✔️ | ✔️  \nGemini 1.5 Pro | ✔️ | ✔️ | ✔️  \n  \n## Best Practices\n\n  * **Function and Parameter Descriptions:** Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.\n  * **Naming:** Use descriptive function names (without spaces, periods, or dashes).\n  * **Strong Typing:** Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.\n  * **Tool Selection:** While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.\n  * **Prompt Engineering:**\n    * Provide context: Tell the model its role (e.g., \"You are a helpful weather assistant.\").\n    * Give instructions: Specify how and when to use functions (e.g., \"Don't guess dates; always use a future date for forecasts.\").\n    * Encourage clarification: Instruct the model to ask clarifying questions if needed.\n  * **Temperature:** Use a low temperature (e.g., 0) for more deterministic and reliable function calls.\n  * **Validation:** If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.\n  * **Error Handling** : Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.\n  * **Security:** Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.\n  * **Token Limits:** Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.\n\n\n\n## Notes and Limitations\n\n  * Only a [subset of the OpenAPI schema](https://ai.google.dev/api/caching#FunctionDeclaration) is supported.\n  * Supported parameter types in Python are limited.\n  * Automatic function calling is a Python SDK feature only.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-07 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-07 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/function-calling#function_calling_modes",
    "title": "Function Calling with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Function Calling with the Gemini API \n\nFunction calling lets you connect models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:\n\n  * **Augment Knowledge:** Access information from external sources like databases, APIs, and knowledge bases.\n  * **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.\n  * **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices\n\n\n\nGet Weather Schedule Meeting Create Chart\n\n## How Function Calling Works\n\nFunction calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:\n\n  1. **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.\n  2. **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\n  3. **Execute Function Code (Your Responsibility):** The Model _does not_ execute the function itself. It's your application's responsibility to process the response and check for Function Call, if \n     * **Yes** : Extract the name and args of the function and execute the corresponding function in your application.\n     * **No:** The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).\n  4. **Create User friendly response:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.\n\n\n\nThis process can be repeated over multiple turns, allowing for complex interactions and workflows. The model also supports calling multiple functions in a single turn ([parallel function calling](/gemini-api/docs/function-calling#parallel_function_calling)) and in sequence ([compositional function calling](/gemini-api/docs/function-calling#compositional_function_calling)).\n\n### Step 1: Define Function Declaration\n\nDefine a function and its declaration within your application code that allows users to set light values and make an API request. This function could call external services or APIs.\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Define a function that the model can call to control smart lights\n    set_light_values_declaration = {\n        \"name\": \"set_light_values\",\n        \"description\": \"Sets the brightness and color temperature of a light.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n                },\n                \"color_temp\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"daylight\", \"cool\", \"warm\"],\n                    \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n                },\n            },\n            \"required\": [\"brightness\", \"color_temp\"],\n        },\n    }\n    \n    # This is the actual function that would be called based on the model's suggestion\n    def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n        \"\"\"Set the brightness and color temperature of a room light. (mock API).\n    \n        Args:\n            brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n            color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    \n        Returns:\n            A dictionary containing the set brightness and color temperature.\n        \"\"\"\n        return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n    \n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    // Define a function that the model can call to control smart lights\n    const setLightValuesFunctionDeclaration = {\n      name: 'set_light_values',\n      description: 'Sets the brightness and color temperature of a light.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',\n          },\n          color_temp: {\n            type: Type.STRING,\n            enum: ['daylight', 'cool', 'warm'],\n            description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',\n          },\n        },\n        required: ['brightness', 'color_temp'],\n      },\n    };\n    \n    /**\n    * Set the brightness and color temperature of a room light. (mock API)\n    * @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness\n    * @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    * @return {Object} A dictionary containing the set brightness and color temperature.\n    */\n    function setLightValues(brightness, color_temp) {\n      return {\n        brightness: brightness,\n        colorTemperature: color_temp\n      };\n    }\n    \n\n### Step 2: Call the model with function declarations\n\nOnce you have defined your function declarations, you can prompt the model to use the function. It analyzes the prompt and function declarations and decides to respond directly or to call a function. If a function is called the response object will contain a function call suggestion.\n\n### Python\n    \n    \n    from google import genai\n    \n    # Generation Config with Function Declaration\n    tools = types.Tool(function_declarations=[set_light_values_declaration])\n    config = types.GenerateContentConfig(tools=[tools])\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Define user prompt\n    contents = [\n        types.Content(\n            role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n        )\n    ]\n    \n    # Send request with function declarations\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\", config=config, contents=contents\n    )\n    \n    print(response.candidates[0].content.parts[0].function_call)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Generation Config with Function Declaration\n    const config = {\n      tools: [{\n        functionDeclarations: [setLightValuesFunctionDeclaration]\n      }]\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Define user prompt\n    const contents = [\n      {\n        role: 'user',\n        parts: [{ text: 'Turn the lights down to a romantic level' }]\n      }\n    ];\n    \n    // Send request with function declarations\n    const response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(response.functionCalls[0]); \n    \n\nThe model then returns a `functionCall` object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.\n\n### Python\n    \n    \n    id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'\n    \n\n### JavaScript\n    \n    \n    {\n      name: 'set_light_values',\n      args: { brightness: 25, color_temp: 'warm' }\n    }\n    \n\n### Step 3: Execute set_light_values function code\n\nExtract the function call details from the model's response, parse the arguments , and execute the `set_light_values` function in our code.\n\n### Python\n    \n    \n    # Extract tool call details\n    tool_call = response.candidates[0].content.parts[0].function_call\n    \n    if tool_call.name == \"set_light_values\":\n        result = set_light_values(**tool_call.args)\n        print(f\"Function execution result: {result}\")\n    \n\n### JavaScript\n    \n    \n    // Extract tool call details\n    const tool_call = response.functionCalls[0]\n    \n    let result;\n    if (tool_call.name === 'set_light_values') {\n      result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);\n      console.log(`Function execution result: ${JSON.stringify(result)}`);\n    }\n    \n\n### Step 4: Create User friendly response with function result and call the model again\n\nFinally, send the result of the function execution back to the model so it can incorporate this information into its final response to the user.\n\n### Python\n    \n    \n    # Create a function response part\n    function_response_part = types.Part.from_function_response(\n        name=tool_call.name,\n        response={\"result\": result},\n    )\n    \n    # Append function call and result of the function execution to contents\n    contents.append(types.Content(role=\"model\", parts=[types.Part(function_call=tool_call)])) # Append the model's function call message\n    contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n    \n    final_response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=config,\n        contents=contents,\n    )\n    \n    print(final_response.text)\n    \n\n### JavaScript\n    \n    \n    // Create a function response part\n    const function_response_part = {\n      name: tool_call.name,\n      response: { result }\n    }\n    \n    // Append function call and result of the function execution to contents\n    contents.push({ role: 'model', parts: [{ functionCall: tool_call }] });\n    contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });\n    \n    // Get the final response from the model\n    const final_response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(final_response.text);\n    \n\nThis completes the function calling flow. The Model successfully used the `set_light_values` function to perform the request action of the user.\n\n## Function declarations\n\nWhen you implement function calling in a prompt, you create a `tools` object, which contains one or more _`function declarations`_. You define functions using JSON, specifically with a [select subset](https://ai.google.dev/api/caching#Schema) of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schemawr) format. A single function declaration can include the following parameters:\n\n  * `name` (string): A unique name for the function (`get_weather_forecast`, `send_email`). Use descriptive names without spaces or special characters (use underscores or camelCase).\n  * `description` (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful (\"Finds theaters based on location and optionally movie title which is currently playing in theaters.\").\n  * `parameters` (object): Defines the input parameters the function expects. \n    * `type` (string): Specifies the overall data type, such as `object`.\n    * `properties` (object): Lists individual parameters, each with: \n      * `type` (string): The data type of the parameter, such as `string`, `integer`, `boolean, array`.\n      * `description` (string): A description of the parameter's purpose and format. Provide examples and constraints (\"The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.\").\n      * `enum` (array, optional): If the parameter values are from a fixed set, use \"enum\" to list the allowed values instead of just describing them in the description. This improves accuracy (\"enum\": [\"daylight\", \"cool\", \"warm\"]).\n    * `required` (array): An array of strings listing the parameter names that are mandatory for the function to operate.\n\n\n\n## Parallel Function Calling\n\nIn addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco.\n\n### Python\n    \n    \n    power_disco_ball = {\n        \"name\": \"power_disco_ball\",\n        \"description\": \"Powers the spinning disco ball.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"power\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether to turn the disco ball on or off.\",\n                }\n            },\n            \"required\": [\"power\"],\n        },\n    }\n    \n    start_music = {\n        \"name\": \"start_music\",\n        \"description\": \"Play some music matching the specified parameters.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"energetic\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is energetic or not.\",\n                },\n                \"loud\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is loud or not.\",\n                },\n            },\n            \"required\": [\"energetic\", \"loud\"],\n        },\n    }\n    \n    dim_lights = {\n        \"name\": \"dim_lights\",\n        \"description\": \"Dim the lights.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"number\",\n                    \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n                }\n            },\n            \"required\": [\"brightness\"],\n        },\n    }\n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    const powerDiscoBall = {\n      name: 'power_disco_ball',\n      description: 'Powers the spinning disco ball.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          power: {\n            type: Type.BOOLEAN,\n            description: 'Whether to turn the disco ball on or off.'\n          }\n        },\n        required: ['power']\n      }\n    };\n    \n    const startMusic = {\n      name: 'start_music',\n      description: 'Play some music matching the specified parameters.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          energetic: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is energetic or not.'\n          },\n          loud: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is loud or not.'\n          }\n        },\n        required: ['energetic', 'loud']\n      }\n    };\n    \n    const dimLights = {\n      name: 'dim_lights',\n      description: 'Dim the lights.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'\n          }\n        },\n        required: ['brightness']\n      }\n    };\n    \n\nCall the model with an instruction that could use all of the specified tools. This example uses a `tool_config`. To learn more you can read about [configuring function calling](/gemini-api/docs/function-calling#function_calling_modes).\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Set up function declarations\n    house_tools = [\n        types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])\n    ]\n    \n    config = {\n        \"tools\": house_tools,\n        \"automatic_function_calling\": {\"disable\": True},\n        # Force the model to call 'any' function, instead of chatting.\n        \"tool_config\": {\"function_calling_config\": {\"mode\": \"any\"}},\n    }\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Turn this place into a party!\")\n    \n    # Print out each of the function calls requested from this single call\n    print(\"Example 1: Forced function calling\")\n    for fn in response.function_calls:\n        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n        print(f\"{fn.name}({args})\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Set up function declarations\n    const houseFns = [powerDiscoBall, startMusic, dimLights];\n    \n    const config = {\n        tools: [{\n            functionDeclarations: houseFns\n        }],\n        // Force the model to call 'any' function, instead of chatting.\n        toolConfig: {\n            functionCallingConfig: {\n            mode: 'any'\n            }\n        }\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Create a chat session\n    const chat = ai.chats.create({\n        model: 'gemini-2.0-flash',\n        config: config\n    });\n    const response = await chat.sendMessage({message: 'Turn this place into a party!'});\n    \n    // Print out each of the function calls requested from this single call\n    console.log(\"Example 1: Forced function calling\");\n    for (const fn of response.functionCalls) {\n        const args = Object.entries(fn.args)\n            .map(([key, val]) => `${key}=${val}`)\n            .join(', ');\n        console.log(`${fn.name}(${args})`);\n    }\n    \n\nEach of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested. \n\nThe Python SDK supports a feature called [automatic function calling](/gemini-api/docs/function-calling#automatic_function_calling_python_only) which converts the Python function to declarations, handles the function call execution and response cycle for you. Following is an example for our disco use case.\n\n**Note:** Automatic Function Calling is a Python SDK only feature at the moment.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Actual implementation functions\n    def power_disco_ball_impl(power: bool) -> dict:\n        \"\"\"Powers the spinning disco ball.\n    \n        Args:\n            power: Whether to turn the disco ball on or off.\n    \n        Returns:\n            A status dictionary indicating the current state.\n        \"\"\"\n        return {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n    \n    def start_music_impl(energetic: bool, loud: bool) -> dict:\n        \"\"\"Play some music matching the specified parameters.\n    \n        Args:\n            energetic: Whether the music is energetic or not.\n            loud: Whether the music is loud or not.\n    \n        Returns:\n            A dictionary containing the music settings.\n        \"\"\"\n        music_type = \"energetic\" if energetic else \"chill\"\n        volume = \"loud\" if loud else \"quiet\"\n        return {\"music_type\": music_type, \"volume\": volume}\n    \n    def dim_lights_impl(brightness: float) -> dict:\n        \"\"\"Dim the lights.\n    \n        Args:\n            brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n    \n        Returns:\n            A dictionary containing the new brightness setting.\n        \"\"\"\n        return {\"brightness\": brightness}\n    \n    config = {\n        \"tools\": [power_disco_ball_impl, start_music_impl, dim_lights_impl],\n    }\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Do everything you need to this place into party!\")\n    \n    print(\"\\nExample 2: Automatic function calling\")\n    print(response.text)\n    # I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!\n    \n\n## Compositional Function Calling\n\nGemini 2.0 supports compositional function calling, meaning the model can chain multiple function calls together. For example, to answer \"Get the temperature in my current location\", the Gemini API might invoke both a `get_current_location()` function and a `get_weather()` function that takes the location as a parameter.\n\n**Note:** Compositional function calling is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Light control schemas\n    turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}\n    turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}\n    \n    prompt = \"\"\"\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n      \"\"\"\n    \n    tools = [\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}\n    ]\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Light control schemas\n    const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };\n    const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };\n    \n    const prompt = `\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n    `;\n    \n    const tools = [\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }\n    ];\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n## Function calling modes\n\nThe Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the `function_calling_config`.\n\n  * `AUTO (Default)`: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n  * `ANY`: The model is constrained to always predict a function call and guarantee function schema adherence. If `allowed_function_names` is not specified, the model can choose from any of the provided function declarations. If `allowed_function_names` is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call in response to every prompt (if applicable).\n  * `NONE`: The model is _prohibited_ from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.\n\n\n\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Configure function calling mode\n    tool_config = types.ToolConfig(\n        function_calling_config=types.FunctionCallingConfig(\n            mode=\"ANY\", allowed_function_names=[\"get_current_temperature\"]\n        )\n    )\n    \n    # Create the generation config\n    config = types.GenerateContentConfig(\n        temperature=0,\n        tools=[tools],  # not defined here.\n        tool_config=tool_config,\n    )\n    \n\n### JavaScript\n    \n    \n    import { FunctionCallingConfigMode } from '@google/genai';\n    \n    // Configure function calling mode\n    const toolConfig = {\n      functionCallingConfig: {\n        mode: FunctionCallingConfigMode.ANY,\n        allowedFunctionNames: ['get_current_temperature']\n      }\n    };\n    \n    // Create the generation config\n    const config = {\n      temperature: 0,\n      tools: tools, // not defined here.\n      toolConfig: toolConfig,\n    };\n    \n\n## Automatic Function Calling (Python Only)\n\nWhen using the Python SDK, you can provide Python functions directly as tools. The SDK automatically converts the Python function to declarations, handles the function call execution and response cycle for you. The Python SDK then automatically:\n\n  1. Detects function call responses from the model.\n  2. Call the corresponding Python function in your code.\n  3. Sends the function response back to the model.\n  4. Returns the model's final text response.\n\n\n\nTo use this, define your function with type hints and a docstring, and then pass the function itself (not a JSON declaration) as a tool:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Define the function with type hints and docstring\n    def get_current_temperature(location: str) -> dict:\n        \"\"\"Gets the current temperature for a given location.\n    \n        Args:\n            location: The city and state, e.g. San Francisco, CA\n    \n        Returns:\n            A dictionary containing the temperature and unit.\n        \"\"\"\n        # ... (implementation) ...\n        return {\"temperature\": 25, \"unit\": \"Celsius\"}\n    \n    # Configure the client and model\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))  # Replace with your actual API key setup\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature]\n    )  # Pass the function itself\n    \n    # Make the request\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=\"What's the temperature in London?\",\n        config=config,\n    )\n    \n    print(response.text)  # The SDK handles the function call and returns the final text\n    \n\nYou can disable automatic function calling with:\n\n### Python\n    \n    \n    # To disable automatic function calling:\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature],\n        automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n    )\n    \n\n### Automatic Function schema declaration\n\nAutomatic schema extraction from Python functions doesn't work in all cases. For example: it doesn't handle cases where you describe the fields of a nested dictionary-object. The API is able to describe any of the following types:\n\n### Python\n    \n    \n    AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])\n    \n\nTo see what the inferred schema looks like, you can convert it using [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable):\n\n### Python\n    \n    \n    def multiply(a: float, b: float):\n        \"\"\"Returns a * b.\"\"\"\n        return a * b\n    \n    fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)\n    \n    # to_json_dict() provides a clean JSON representation.\n    print(fn_decl.to_json_dict())\n    \n\n## Multi-tool use: Combine Native Tools with Function Calling\n\nWith Gemini 2.0, you can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, [Grounding with Google Search](/gemini-api/docs/grounding) and [code execution](/gemini-api/docs/code-execution), in a request using the [Live API](/gemini-api/docs/live).\n\n**Note:** Multi-tool use is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Multiple tasks example - combining lights, code execution, and search\n    prompt = \"\"\"\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n      \"\"\"\n    \n    tools = [\n        {'google_search': {}},\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.\n    ]\n    \n    # Execute the prompt with specified tools in audio modality\n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Multiple tasks example - combining lights, code execution, and search\n    const prompt = `\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n    `;\n    \n    const tools = [\n      { googleSearch: {} },\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.\n    ];\n    \n    // Execute the prompt with specified tools in audio modality\n    await run(prompt, {tools: tools, modality: \"AUDIO\"});\n    \n\nPython developers can try this out in the [Live API Tool Use notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).\n\n## Use Model Context Protocol (MCP)\n\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard to connect AI applications with external tools, data sources, and systems. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts. You can use models with MCP server using their tool calling capabilities.\n\nMCP servers expose the tools as JSON schema definitions, which can be used with Gemini compatible function declarations. This lets you to use a MCP server with Gemini models directly. Here, you can find an example of how to use a local MCP server with Gemini SDK and the `mcp` SDK.\n\n### Python\n    \n    \n    import asyncio\n    import os\n    from datetime import datetime\n    from google import genai\n    from google.genai import types\n    from mcp import ClientSession, StdioServerParameters\n    from mcp.client.stdio import stdio_client\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Create server parameters for stdio connection\n    server_params = StdioServerParameters(\n        command=\"npx\",  # Executable\n        args=[\"-y\", \"@philschmid/weather-mcp\"],  # Weather MCP Server\n        env=None,  # Optional environment variables\n    )\n    \n    async def run():\n        async with stdio_client(server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                # Prompt to get the weather for the current day in London.\n                prompt = f\"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?\"\n                # Initialize the connection between client and server\n                await session.initialize()\n    \n                # Get tools from MCP session and convert to Gemini Tool objects\n                mcp_tools = await session.list_tools()\n                tools = [\n                    types.Tool(\n                        function_declarations=[\n                            {\n                                \"name\": tool.name,\n                                \"description\": tool.description,\n                                \"parameters\": {\n                                    k: v\n                                    for k, v in tool.inputSchema.items()\n                                    if k not in [\"additionalProperties\", \"$schema\"]\n                                },\n                            }\n                        ]\n                    )\n                    for tool in mcp_tools.tools\n                ]\n    \n                # Send request to the model with MCP function declarations\n                response = client.models.generate_content(\n                    model=\"gemini-2.0-flash\",\n                    contents=prompt,\n                    config=types.GenerateContentConfig(\n                        temperature=0,\n                        tools=tools,\n                    ),\n                )\n    \n                # Check for a function call\n                if response.candidates[0].content.parts[0].function_call:\n                    function_call = response.candidates[0].content.parts[0].function_call\n                    print(function_call)\n                    # Call the MCP server with the predicted tool\n                    result = await session.call_tool(\n                        function_call.name, arguments=function_call.args\n                    )\n                    print(result.content[0].text)\n                    # Continue as shown in step 4 of \"How Function Calling Works\"\n                    # and create a user friendly response\n                else:\n                    print(\"No function call found in the response.\")\n                    print(response.text)\n    \n    # Start the asyncio event loop and run the main function\n    asyncio.run(run())\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    import { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\n    import { StdioClientTransport } from \"@modelcontextprotocol/sdk/client/stdio.js\";\n    \n    // Create server parameters for stdio connection\n    const serverParams = new StdioClientTransport({\n      command: \"npx\",\n      args: [\"-y\", \"@philschmid/weather-mcp\"]\n    });\n    \n    const client = new Client(\n      {\n        name: \"example-client\",\n        version: \"1.0.0\"\n      }\n    );\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Initialize the connection between client and server\n    await client.connect(serverParams);\n    \n    // Get tools from MCP session and convert to Gemini Tool objects\n    const mcpTools = await client.listTools();\n    const tools = mcpTools.tools.map((tool) => {\n      // Filter the parameters to exclude not supported keys\n      const parameters = Object.fromEntries(\n        Object.entries(tool.inputSchema).filter(([key]) => ![\"additionalProperties\", \"$schema\"].includes(key))\n      );\n      return {\n        name: tool.name,\n        description: tool.description,\n        parameters: parameters\n      };\n    });\n    \n    // Send request to the model with MCP function declarations\n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: \"What is the weather in London in the UK on 2024-04-04?\",\n      config: {\n        tools: [{\n          functionDeclarations: tools\n        }],\n      },\n    });\n    \n    // Check for function calls in the response\n    if (response.functionCalls && response.functionCalls.length > 0) {\n      const functionCall = response.functionCalls[0]; // Assuming one function call\n      console.log(`Function to call: ${functionCall.name}`);\n      console.log(`Arguments: ${JSON.stringify(functionCall.args)}`);\n      // Call the MCP server with the predicted tool\n      const result = await client.callTool({name: functionCall.name, arguments: functionCall.args});\n      console.log(result.content[0].text);\n      // Continue as shown in step 4 of \"How Function Calling Works\"\n      // and create a user friendly response\n    } else {\n      console.log(\"No function call found in the response.\");\n      console.log(response.text);\n    }\n    \n    // Close the connection\n    await client.close();\n    \n\n## Supported Models\n\nExperimental models are not included. You can find their capabilities on the [model overview](https://ai.google.dev/gemini-api/docs/models) page.\n\nModel | Function Calling | Parallel Function Calling | Compositional Function Calling(Live API only)  \n---|---|---|---  \nGemini 2.0 Flash | ✔️ | ✔️ | ✔️  \nGemini 2.0 Flash-Lite | X | X | X  \nGemini 1.5 Flash | ✔️ | ✔️ | ✔️  \nGemini 1.5 Pro | ✔️ | ✔️ | ✔️  \n  \n## Best Practices\n\n  * **Function and Parameter Descriptions:** Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.\n  * **Naming:** Use descriptive function names (without spaces, periods, or dashes).\n  * **Strong Typing:** Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.\n  * **Tool Selection:** While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.\n  * **Prompt Engineering:**\n    * Provide context: Tell the model its role (e.g., \"You are a helpful weather assistant.\").\n    * Give instructions: Specify how and when to use functions (e.g., \"Don't guess dates; always use a future date for forecasts.\").\n    * Encourage clarification: Instruct the model to ask clarifying questions if needed.\n  * **Temperature:** Use a low temperature (e.g., 0) for more deterministic and reliable function calls.\n  * **Validation:** If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.\n  * **Error Handling** : Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.\n  * **Security:** Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.\n  * **Token Limits:** Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.\n\n\n\n## Notes and Limitations\n\n  * Only a [subset of the OpenAPI schema](https://ai.google.dev/api/caching#FunctionDeclaration) is supported.\n  * Supported parameter types in Python are limited.\n  * Automatic function calling is a Python SDK feature only.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-07 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-07 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/function-calling#automatic_function_calling_python_only",
    "title": "Function Calling with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Function Calling with the Gemini API \n\nFunction calling lets you connect models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:\n\n  * **Augment Knowledge:** Access information from external sources like databases, APIs, and knowledge bases.\n  * **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.\n  * **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices\n\n\n\nGet Weather Schedule Meeting Create Chart\n\n## How Function Calling Works\n\nFunction calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:\n\n  1. **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.\n  2. **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\n  3. **Execute Function Code (Your Responsibility):** The Model _does not_ execute the function itself. It's your application's responsibility to process the response and check for Function Call, if \n     * **Yes** : Extract the name and args of the function and execute the corresponding function in your application.\n     * **No:** The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).\n  4. **Create User friendly response:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.\n\n\n\nThis process can be repeated over multiple turns, allowing for complex interactions and workflows. The model also supports calling multiple functions in a single turn ([parallel function calling](/gemini-api/docs/function-calling#parallel_function_calling)) and in sequence ([compositional function calling](/gemini-api/docs/function-calling#compositional_function_calling)).\n\n### Step 1: Define Function Declaration\n\nDefine a function and its declaration within your application code that allows users to set light values and make an API request. This function could call external services or APIs.\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Define a function that the model can call to control smart lights\n    set_light_values_declaration = {\n        \"name\": \"set_light_values\",\n        \"description\": \"Sets the brightness and color temperature of a light.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n                },\n                \"color_temp\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"daylight\", \"cool\", \"warm\"],\n                    \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n                },\n            },\n            \"required\": [\"brightness\", \"color_temp\"],\n        },\n    }\n    \n    # This is the actual function that would be called based on the model's suggestion\n    def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n        \"\"\"Set the brightness and color temperature of a room light. (mock API).\n    \n        Args:\n            brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n            color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    \n        Returns:\n            A dictionary containing the set brightness and color temperature.\n        \"\"\"\n        return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n    \n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    // Define a function that the model can call to control smart lights\n    const setLightValuesFunctionDeclaration = {\n      name: 'set_light_values',\n      description: 'Sets the brightness and color temperature of a light.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',\n          },\n          color_temp: {\n            type: Type.STRING,\n            enum: ['daylight', 'cool', 'warm'],\n            description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',\n          },\n        },\n        required: ['brightness', 'color_temp'],\n      },\n    };\n    \n    /**\n    * Set the brightness and color temperature of a room light. (mock API)\n    * @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness\n    * @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    * @return {Object} A dictionary containing the set brightness and color temperature.\n    */\n    function setLightValues(brightness, color_temp) {\n      return {\n        brightness: brightness,\n        colorTemperature: color_temp\n      };\n    }\n    \n\n### Step 2: Call the model with function declarations\n\nOnce you have defined your function declarations, you can prompt the model to use the function. It analyzes the prompt and function declarations and decides to respond directly or to call a function. If a function is called the response object will contain a function call suggestion.\n\n### Python\n    \n    \n    from google import genai\n    \n    # Generation Config with Function Declaration\n    tools = types.Tool(function_declarations=[set_light_values_declaration])\n    config = types.GenerateContentConfig(tools=[tools])\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Define user prompt\n    contents = [\n        types.Content(\n            role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n        )\n    ]\n    \n    # Send request with function declarations\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\", config=config, contents=contents\n    )\n    \n    print(response.candidates[0].content.parts[0].function_call)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Generation Config with Function Declaration\n    const config = {\n      tools: [{\n        functionDeclarations: [setLightValuesFunctionDeclaration]\n      }]\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Define user prompt\n    const contents = [\n      {\n        role: 'user',\n        parts: [{ text: 'Turn the lights down to a romantic level' }]\n      }\n    ];\n    \n    // Send request with function declarations\n    const response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(response.functionCalls[0]); \n    \n\nThe model then returns a `functionCall` object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.\n\n### Python\n    \n    \n    id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'\n    \n\n### JavaScript\n    \n    \n    {\n      name: 'set_light_values',\n      args: { brightness: 25, color_temp: 'warm' }\n    }\n    \n\n### Step 3: Execute set_light_values function code\n\nExtract the function call details from the model's response, parse the arguments , and execute the `set_light_values` function in our code.\n\n### Python\n    \n    \n    # Extract tool call details\n    tool_call = response.candidates[0].content.parts[0].function_call\n    \n    if tool_call.name == \"set_light_values\":\n        result = set_light_values(**tool_call.args)\n        print(f\"Function execution result: {result}\")\n    \n\n### JavaScript\n    \n    \n    // Extract tool call details\n    const tool_call = response.functionCalls[0]\n    \n    let result;\n    if (tool_call.name === 'set_light_values') {\n      result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);\n      console.log(`Function execution result: ${JSON.stringify(result)}`);\n    }\n    \n\n### Step 4: Create User friendly response with function result and call the model again\n\nFinally, send the result of the function execution back to the model so it can incorporate this information into its final response to the user.\n\n### Python\n    \n    \n    # Create a function response part\n    function_response_part = types.Part.from_function_response(\n        name=tool_call.name,\n        response={\"result\": result},\n    )\n    \n    # Append function call and result of the function execution to contents\n    contents.append(types.Content(role=\"model\", parts=[types.Part(function_call=tool_call)])) # Append the model's function call message\n    contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n    \n    final_response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=config,\n        contents=contents,\n    )\n    \n    print(final_response.text)\n    \n\n### JavaScript\n    \n    \n    // Create a function response part\n    const function_response_part = {\n      name: tool_call.name,\n      response: { result }\n    }\n    \n    // Append function call and result of the function execution to contents\n    contents.push({ role: 'model', parts: [{ functionCall: tool_call }] });\n    contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });\n    \n    // Get the final response from the model\n    const final_response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(final_response.text);\n    \n\nThis completes the function calling flow. The Model successfully used the `set_light_values` function to perform the request action of the user.\n\n## Function declarations\n\nWhen you implement function calling in a prompt, you create a `tools` object, which contains one or more _`function declarations`_. You define functions using JSON, specifically with a [select subset](https://ai.google.dev/api/caching#Schema) of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schemawr) format. A single function declaration can include the following parameters:\n\n  * `name` (string): A unique name for the function (`get_weather_forecast`, `send_email`). Use descriptive names without spaces or special characters (use underscores or camelCase).\n  * `description` (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful (\"Finds theaters based on location and optionally movie title which is currently playing in theaters.\").\n  * `parameters` (object): Defines the input parameters the function expects. \n    * `type` (string): Specifies the overall data type, such as `object`.\n    * `properties` (object): Lists individual parameters, each with: \n      * `type` (string): The data type of the parameter, such as `string`, `integer`, `boolean, array`.\n      * `description` (string): A description of the parameter's purpose and format. Provide examples and constraints (\"The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.\").\n      * `enum` (array, optional): If the parameter values are from a fixed set, use \"enum\" to list the allowed values instead of just describing them in the description. This improves accuracy (\"enum\": [\"daylight\", \"cool\", \"warm\"]).\n    * `required` (array): An array of strings listing the parameter names that are mandatory for the function to operate.\n\n\n\n## Parallel Function Calling\n\nIn addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco.\n\n### Python\n    \n    \n    power_disco_ball = {\n        \"name\": \"power_disco_ball\",\n        \"description\": \"Powers the spinning disco ball.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"power\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether to turn the disco ball on or off.\",\n                }\n            },\n            \"required\": [\"power\"],\n        },\n    }\n    \n    start_music = {\n        \"name\": \"start_music\",\n        \"description\": \"Play some music matching the specified parameters.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"energetic\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is energetic or not.\",\n                },\n                \"loud\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is loud or not.\",\n                },\n            },\n            \"required\": [\"energetic\", \"loud\"],\n        },\n    }\n    \n    dim_lights = {\n        \"name\": \"dim_lights\",\n        \"description\": \"Dim the lights.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"number\",\n                    \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n                }\n            },\n            \"required\": [\"brightness\"],\n        },\n    }\n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    const powerDiscoBall = {\n      name: 'power_disco_ball',\n      description: 'Powers the spinning disco ball.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          power: {\n            type: Type.BOOLEAN,\n            description: 'Whether to turn the disco ball on or off.'\n          }\n        },\n        required: ['power']\n      }\n    };\n    \n    const startMusic = {\n      name: 'start_music',\n      description: 'Play some music matching the specified parameters.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          energetic: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is energetic or not.'\n          },\n          loud: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is loud or not.'\n          }\n        },\n        required: ['energetic', 'loud']\n      }\n    };\n    \n    const dimLights = {\n      name: 'dim_lights',\n      description: 'Dim the lights.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'\n          }\n        },\n        required: ['brightness']\n      }\n    };\n    \n\nCall the model with an instruction that could use all of the specified tools. This example uses a `tool_config`. To learn more you can read about [configuring function calling](/gemini-api/docs/function-calling#function_calling_modes).\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Set up function declarations\n    house_tools = [\n        types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])\n    ]\n    \n    config = {\n        \"tools\": house_tools,\n        \"automatic_function_calling\": {\"disable\": True},\n        # Force the model to call 'any' function, instead of chatting.\n        \"tool_config\": {\"function_calling_config\": {\"mode\": \"any\"}},\n    }\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Turn this place into a party!\")\n    \n    # Print out each of the function calls requested from this single call\n    print(\"Example 1: Forced function calling\")\n    for fn in response.function_calls:\n        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n        print(f\"{fn.name}({args})\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Set up function declarations\n    const houseFns = [powerDiscoBall, startMusic, dimLights];\n    \n    const config = {\n        tools: [{\n            functionDeclarations: houseFns\n        }],\n        // Force the model to call 'any' function, instead of chatting.\n        toolConfig: {\n            functionCallingConfig: {\n            mode: 'any'\n            }\n        }\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Create a chat session\n    const chat = ai.chats.create({\n        model: 'gemini-2.0-flash',\n        config: config\n    });\n    const response = await chat.sendMessage({message: 'Turn this place into a party!'});\n    \n    // Print out each of the function calls requested from this single call\n    console.log(\"Example 1: Forced function calling\");\n    for (const fn of response.functionCalls) {\n        const args = Object.entries(fn.args)\n            .map(([key, val]) => `${key}=${val}`)\n            .join(', ');\n        console.log(`${fn.name}(${args})`);\n    }\n    \n\nEach of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested. \n\nThe Python SDK supports a feature called [automatic function calling](/gemini-api/docs/function-calling#automatic_function_calling_python_only) which converts the Python function to declarations, handles the function call execution and response cycle for you. Following is an example for our disco use case.\n\n**Note:** Automatic Function Calling is a Python SDK only feature at the moment.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Actual implementation functions\n    def power_disco_ball_impl(power: bool) -> dict:\n        \"\"\"Powers the spinning disco ball.\n    \n        Args:\n            power: Whether to turn the disco ball on or off.\n    \n        Returns:\n            A status dictionary indicating the current state.\n        \"\"\"\n        return {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n    \n    def start_music_impl(energetic: bool, loud: bool) -> dict:\n        \"\"\"Play some music matching the specified parameters.\n    \n        Args:\n            energetic: Whether the music is energetic or not.\n            loud: Whether the music is loud or not.\n    \n        Returns:\n            A dictionary containing the music settings.\n        \"\"\"\n        music_type = \"energetic\" if energetic else \"chill\"\n        volume = \"loud\" if loud else \"quiet\"\n        return {\"music_type\": music_type, \"volume\": volume}\n    \n    def dim_lights_impl(brightness: float) -> dict:\n        \"\"\"Dim the lights.\n    \n        Args:\n            brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n    \n        Returns:\n            A dictionary containing the new brightness setting.\n        \"\"\"\n        return {\"brightness\": brightness}\n    \n    config = {\n        \"tools\": [power_disco_ball_impl, start_music_impl, dim_lights_impl],\n    }\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Do everything you need to this place into party!\")\n    \n    print(\"\\nExample 2: Automatic function calling\")\n    print(response.text)\n    # I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!\n    \n\n## Compositional Function Calling\n\nGemini 2.0 supports compositional function calling, meaning the model can chain multiple function calls together. For example, to answer \"Get the temperature in my current location\", the Gemini API might invoke both a `get_current_location()` function and a `get_weather()` function that takes the location as a parameter.\n\n**Note:** Compositional function calling is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Light control schemas\n    turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}\n    turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}\n    \n    prompt = \"\"\"\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n      \"\"\"\n    \n    tools = [\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}\n    ]\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Light control schemas\n    const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };\n    const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };\n    \n    const prompt = `\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n    `;\n    \n    const tools = [\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }\n    ];\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n## Function calling modes\n\nThe Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the `function_calling_config`.\n\n  * `AUTO (Default)`: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n  * `ANY`: The model is constrained to always predict a function call and guarantee function schema adherence. If `allowed_function_names` is not specified, the model can choose from any of the provided function declarations. If `allowed_function_names` is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call in response to every prompt (if applicable).\n  * `NONE`: The model is _prohibited_ from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.\n\n\n\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Configure function calling mode\n    tool_config = types.ToolConfig(\n        function_calling_config=types.FunctionCallingConfig(\n            mode=\"ANY\", allowed_function_names=[\"get_current_temperature\"]\n        )\n    )\n    \n    # Create the generation config\n    config = types.GenerateContentConfig(\n        temperature=0,\n        tools=[tools],  # not defined here.\n        tool_config=tool_config,\n    )\n    \n\n### JavaScript\n    \n    \n    import { FunctionCallingConfigMode } from '@google/genai';\n    \n    // Configure function calling mode\n    const toolConfig = {\n      functionCallingConfig: {\n        mode: FunctionCallingConfigMode.ANY,\n        allowedFunctionNames: ['get_current_temperature']\n      }\n    };\n    \n    // Create the generation config\n    const config = {\n      temperature: 0,\n      tools: tools, // not defined here.\n      toolConfig: toolConfig,\n    };\n    \n\n## Automatic Function Calling (Python Only)\n\nWhen using the Python SDK, you can provide Python functions directly as tools. The SDK automatically converts the Python function to declarations, handles the function call execution and response cycle for you. The Python SDK then automatically:\n\n  1. Detects function call responses from the model.\n  2. Call the corresponding Python function in your code.\n  3. Sends the function response back to the model.\n  4. Returns the model's final text response.\n\n\n\nTo use this, define your function with type hints and a docstring, and then pass the function itself (not a JSON declaration) as a tool:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Define the function with type hints and docstring\n    def get_current_temperature(location: str) -> dict:\n        \"\"\"Gets the current temperature for a given location.\n    \n        Args:\n            location: The city and state, e.g. San Francisco, CA\n    \n        Returns:\n            A dictionary containing the temperature and unit.\n        \"\"\"\n        # ... (implementation) ...\n        return {\"temperature\": 25, \"unit\": \"Celsius\"}\n    \n    # Configure the client and model\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))  # Replace with your actual API key setup\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature]\n    )  # Pass the function itself\n    \n    # Make the request\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=\"What's the temperature in London?\",\n        config=config,\n    )\n    \n    print(response.text)  # The SDK handles the function call and returns the final text\n    \n\nYou can disable automatic function calling with:\n\n### Python\n    \n    \n    # To disable automatic function calling:\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature],\n        automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n    )\n    \n\n### Automatic Function schema declaration\n\nAutomatic schema extraction from Python functions doesn't work in all cases. For example: it doesn't handle cases where you describe the fields of a nested dictionary-object. The API is able to describe any of the following types:\n\n### Python\n    \n    \n    AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])\n    \n\nTo see what the inferred schema looks like, you can convert it using [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable):\n\n### Python\n    \n    \n    def multiply(a: float, b: float):\n        \"\"\"Returns a * b.\"\"\"\n        return a * b\n    \n    fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)\n    \n    # to_json_dict() provides a clean JSON representation.\n    print(fn_decl.to_json_dict())\n    \n\n## Multi-tool use: Combine Native Tools with Function Calling\n\nWith Gemini 2.0, you can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, [Grounding with Google Search](/gemini-api/docs/grounding) and [code execution](/gemini-api/docs/code-execution), in a request using the [Live API](/gemini-api/docs/live).\n\n**Note:** Multi-tool use is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Multiple tasks example - combining lights, code execution, and search\n    prompt = \"\"\"\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n      \"\"\"\n    \n    tools = [\n        {'google_search': {}},\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.\n    ]\n    \n    # Execute the prompt with specified tools in audio modality\n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Multiple tasks example - combining lights, code execution, and search\n    const prompt = `\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n    `;\n    \n    const tools = [\n      { googleSearch: {} },\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.\n    ];\n    \n    // Execute the prompt with specified tools in audio modality\n    await run(prompt, {tools: tools, modality: \"AUDIO\"});\n    \n\nPython developers can try this out in the [Live API Tool Use notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).\n\n## Use Model Context Protocol (MCP)\n\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard to connect AI applications with external tools, data sources, and systems. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts. You can use models with MCP server using their tool calling capabilities.\n\nMCP servers expose the tools as JSON schema definitions, which can be used with Gemini compatible function declarations. This lets you to use a MCP server with Gemini models directly. Here, you can find an example of how to use a local MCP server with Gemini SDK and the `mcp` SDK.\n\n### Python\n    \n    \n    import asyncio\n    import os\n    from datetime import datetime\n    from google import genai\n    from google.genai import types\n    from mcp import ClientSession, StdioServerParameters\n    from mcp.client.stdio import stdio_client\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Create server parameters for stdio connection\n    server_params = StdioServerParameters(\n        command=\"npx\",  # Executable\n        args=[\"-y\", \"@philschmid/weather-mcp\"],  # Weather MCP Server\n        env=None,  # Optional environment variables\n    )\n    \n    async def run():\n        async with stdio_client(server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                # Prompt to get the weather for the current day in London.\n                prompt = f\"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?\"\n                # Initialize the connection between client and server\n                await session.initialize()\n    \n                # Get tools from MCP session and convert to Gemini Tool objects\n                mcp_tools = await session.list_tools()\n                tools = [\n                    types.Tool(\n                        function_declarations=[\n                            {\n                                \"name\": tool.name,\n                                \"description\": tool.description,\n                                \"parameters\": {\n                                    k: v\n                                    for k, v in tool.inputSchema.items()\n                                    if k not in [\"additionalProperties\", \"$schema\"]\n                                },\n                            }\n                        ]\n                    )\n                    for tool in mcp_tools.tools\n                ]\n    \n                # Send request to the model with MCP function declarations\n                response = client.models.generate_content(\n                    model=\"gemini-2.0-flash\",\n                    contents=prompt,\n                    config=types.GenerateContentConfig(\n                        temperature=0,\n                        tools=tools,\n                    ),\n                )\n    \n                # Check for a function call\n                if response.candidates[0].content.parts[0].function_call:\n                    function_call = response.candidates[0].content.parts[0].function_call\n                    print(function_call)\n                    # Call the MCP server with the predicted tool\n                    result = await session.call_tool(\n                        function_call.name, arguments=function_call.args\n                    )\n                    print(result.content[0].text)\n                    # Continue as shown in step 4 of \"How Function Calling Works\"\n                    # and create a user friendly response\n                else:\n                    print(\"No function call found in the response.\")\n                    print(response.text)\n    \n    # Start the asyncio event loop and run the main function\n    asyncio.run(run())\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    import { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\n    import { StdioClientTransport } from \"@modelcontextprotocol/sdk/client/stdio.js\";\n    \n    // Create server parameters for stdio connection\n    const serverParams = new StdioClientTransport({\n      command: \"npx\",\n      args: [\"-y\", \"@philschmid/weather-mcp\"]\n    });\n    \n    const client = new Client(\n      {\n        name: \"example-client\",\n        version: \"1.0.0\"\n      }\n    );\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Initialize the connection between client and server\n    await client.connect(serverParams);\n    \n    // Get tools from MCP session and convert to Gemini Tool objects\n    const mcpTools = await client.listTools();\n    const tools = mcpTools.tools.map((tool) => {\n      // Filter the parameters to exclude not supported keys\n      const parameters = Object.fromEntries(\n        Object.entries(tool.inputSchema).filter(([key]) => ![\"additionalProperties\", \"$schema\"].includes(key))\n      );\n      return {\n        name: tool.name,\n        description: tool.description,\n        parameters: parameters\n      };\n    });\n    \n    // Send request to the model with MCP function declarations\n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: \"What is the weather in London in the UK on 2024-04-04?\",\n      config: {\n        tools: [{\n          functionDeclarations: tools\n        }],\n      },\n    });\n    \n    // Check for function calls in the response\n    if (response.functionCalls && response.functionCalls.length > 0) {\n      const functionCall = response.functionCalls[0]; // Assuming one function call\n      console.log(`Function to call: ${functionCall.name}`);\n      console.log(`Arguments: ${JSON.stringify(functionCall.args)}`);\n      // Call the MCP server with the predicted tool\n      const result = await client.callTool({name: functionCall.name, arguments: functionCall.args});\n      console.log(result.content[0].text);\n      // Continue as shown in step 4 of \"How Function Calling Works\"\n      // and create a user friendly response\n    } else {\n      console.log(\"No function call found in the response.\");\n      console.log(response.text);\n    }\n    \n    // Close the connection\n    await client.close();\n    \n\n## Supported Models\n\nExperimental models are not included. You can find their capabilities on the [model overview](https://ai.google.dev/gemini-api/docs/models) page.\n\nModel | Function Calling | Parallel Function Calling | Compositional Function Calling(Live API only)  \n---|---|---|---  \nGemini 2.0 Flash | ✔️ | ✔️ | ✔️  \nGemini 2.0 Flash-Lite | X | X | X  \nGemini 1.5 Flash | ✔️ | ✔️ | ✔️  \nGemini 1.5 Pro | ✔️ | ✔️ | ✔️  \n  \n## Best Practices\n\n  * **Function and Parameter Descriptions:** Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.\n  * **Naming:** Use descriptive function names (without spaces, periods, or dashes).\n  * **Strong Typing:** Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.\n  * **Tool Selection:** While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.\n  * **Prompt Engineering:**\n    * Provide context: Tell the model its role (e.g., \"You are a helpful weather assistant.\").\n    * Give instructions: Specify how and when to use functions (e.g., \"Don't guess dates; always use a future date for forecasts.\").\n    * Encourage clarification: Instruct the model to ask clarifying questions if needed.\n  * **Temperature:** Use a low temperature (e.g., 0) for more deterministic and reliable function calls.\n  * **Validation:** If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.\n  * **Error Handling** : Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.\n  * **Security:** Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.\n  * **Token Limits:** Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.\n\n\n\n## Notes and Limitations\n\n  * Only a [subset of the OpenAPI schema](https://ai.google.dev/api/caching#FunctionDeclaration) is supported.\n  * Supported parameter types in Python are limited.\n  * Automatic function calling is a Python SDK feature only.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-07 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-07 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/text-generation#multi-turn-conversations",
    "title": "Text generation  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Text generation \n\nThe Gemini API can generate text output in response to various inputs, including text, images, video, and audio. This guide shows you how to generate text using text and image inputs. It also covers streaming, chat, and system instructions.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Text input\n\nThe simplest way to generate text using the Gemini API is to provide the model with a single text-only input, as shown in this example:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[\"How does AI work?\"]\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"How does AI work?\",\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    // import packages here\n    \n    func main() {\n      ctx := context.Background()\n      client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n      if err != nil {\n        log.Fatal(err)\n      }\n      defer client.Close()\n    \n      model := client.GenerativeModel(\"gemini-2.0-flash\")\n      resp, err := model.GenerateContent(ctx, genai.Text(\"How does AI work?\"))\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp) // helper function for printing content parts\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"How does AI work?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { text: 'How AI does work?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Image input\n\nThe Gemini API supports multimodal inputs that combine text and media files. The following example shows how to generate text from text and image input:\n\n### Python\n    \n    \n    from PIL import Image\n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    image = Image.open(\"/path/to/organ.png\")\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[image, \"Tell me about this instrument\"]\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import {\n      GoogleGenAI,\n      createUserContent,\n      createPartFromUri,\n    } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const image = await ai.files.upload({\n        file: \"/path/to/organ.png\",\n      });\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: [\n          createUserContent([\n            \"Tell me about this instrument\",\n            createPartFromUri(image.uri, image.mimeType),\n          ]),\n        ],\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-2.0-flash\")\n    \n    imgData, err := os.ReadFile(filepath.Join(testDataDir, \"organ.jpg\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    \n    resp, err := model.GenerateContent(ctx,\n      genai.Text(\"Tell me about this instrument\"),\n      genai.ImageData(\"jpeg\", imgData))\n    if err != nil {\n      log.Fatal(err)\n    }\n    \n    printResponse(resp)\n    \n\n### REST\n    \n    \n    # Use a temporary file to hold the base64 encoded image data\n    TEMP_B64=$(mktemp)\n    trap 'rm -f \"$TEMP_B64\"' EXIT\n    base64 $B64FLAGS $IMG_PATH > \"$TEMP_B64\"\n    \n    # Use a temporary file to hold the JSON payload\n    TEMP_JSON=$(mktemp)\n    trap 'rm -f \"$TEMP_JSON\"' EXIT\n    \n    cat > \"$TEMP_JSON\" << EOF\n    {\n      \"contents\": [\n        {\n          \"parts\": [\n            {\n              \"text\": \"Tell me about this instrument\"\n            },\n            {\n              \"inline_data\": {\n                \"mime_type\": \"image/jpeg\",\n                \"data\": \"$(cat \"$TEMP_B64\")\"\n              }\n            }\n          ]\n        }\n      ]\n    }\n    EOF\n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d \"@$TEMP_JSON\"\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const imageUrl = 'http://image/url';\n      const image = getImageData(imageUrl);\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { image },\n              { text: 'Tell me about this instrument' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n    function getImageData(url) {\n      const blob = UrlFetchApp.fetch(url).getBlob();\n    \n      return {\n        mimeType: blob.getContentType(),\n        data: Utilities.base64Encode(blob.getBytes())\n      };\n    }\n    \n\n## Streaming output\n\nBy default, the model returns a response after completing the entire text generation process. You can achieve faster interactions by using streaming to return instances of [`GenerateContentResponse`](/api/generate-content#v1beta.GenerateContentResponse) as they're generated.\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content_stream(\n        model=\"gemini-2.0-flash\",\n        contents=[\"Explain how AI works\"]\n    )\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContentStream({\n        model: \"gemini-2.0-flash\",\n        contents: \"Explain how AI works\",\n      });\n    \n      for await (const chunk of response) {\n        console.log(chunk.text);\n      }\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    iter := model.GenerateContentStream(ctx, genai.Text(\"Write a story about a magic backpack.\"))\n    for {\n      resp, err := iter.Next()\n      if err == iterator.Done {\n        break\n      }\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp)\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse&key=${GEMINI_API_KEY}\" \\\n      -H 'Content-Type: application/json' \\\n      --no-buffer \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Explain how AI works\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { text: 'Explain how AI works' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Multi-turn conversations\n\nThe Gemini SDK lets you collect multiple rounds of questions and responses into a chat. The chat format enables users to step incrementally toward answers and to get help with multipart problems. This SDK implementation of chat provides an interface to keep track of conversation history, but behind the scenes it uses the same [`generateContent`](/api/generate-content#method:-models.generatecontent) method to create the response.\n\nThe following code example shows a basic chat implementation:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    chat = client.chats.create(model=\"gemini-2.0-flash\")\n    \n    response = chat.send_message(\"I have 2 dogs in my house.\")\n    print(response.text)\n    \n    response = chat.send_message(\"How many paws are in my house?\")\n    print(response.text)\n    \n    for message in chat.get_history():\n        print(f'role - {message.role}',end=\": \")\n        print(message.parts[0].text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const chat = ai.chats.create({\n        model: \"gemini-2.0-flash\",\n        history: [\n          {\n            role: \"user\",\n            parts: [{ text: \"Hello\" }],\n          },\n          {\n            role: \"model\",\n            parts: [{ text: \"Great to meet you. What would you like to know?\" }],\n          },\n        ],\n      });\n    \n      const response1 = await chat.sendMessage({\n        message: \"I have 2 dogs in my house.\",\n      });\n      console.log(\"Chat response 1:\", response1.text);\n    \n      const response2 = await chat.sendMessage({\n        message: \"How many paws are in my house?\",\n      });\n      console.log(\"Chat response 2:\", response2.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    cs := model.StartChat()\n    \n    cs.History = []*genai.Content{\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Hello, I have 2 dogs in my house.\"),\n        },\n        Role: \"user\",\n      },\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Great to meet you. What would you like to know?\"),\n        },\n        Role: \"model\",\n      },\n    }\n    \n    res, err := cs.SendMessage(ctx, genai.Text(\"How many paws are in my house?\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    printResponse(res)\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"Hello\"\n              }\n            ]\n          },\n          {\n            \"role\": \"model\",\n            \"parts\": [\n              {\n                \"text\": \"Great to meet you. What would you like to know?\"\n              }\n            ]\n          },\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"I have two dogs in my house. How many paws are in my house?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            role: 'user',\n            parts: [\n              { text: 'Hello' },\n            ],\n          },\n          {\n            role: 'model',\n            parts: [\n              { text: 'Great to meet you. What would you like to know?' },\n            ],\n          },\n          {\n            role: 'user',\n            parts: [\n              { text: 'I have two dogs in my house. How many paws are in my house?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nYou can also use streaming with chat, as shown in the following example:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    chat = client.chats.create(model=\"gemini-2.0-flash\")\n    \n    response = chat.send_message_stream(\"I have 2 dogs in my house.\")\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n    response = chat.send_message_stream(\"How many paws are in my house?\")\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n    for message in chat.get_history():\n        print(f'role - {message.role}', end=\": \")\n        print(message.parts[0].text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const chat = ai.chats.create({\n        model: \"gemini-2.0-flash\",\n        history: [\n          {\n            role: \"user\",\n            parts: [{ text: \"Hello\" }],\n          },\n          {\n            role: \"model\",\n            parts: [{ text: \"Great to meet you. What would you like to know?\" }],\n          },\n        ],\n      });\n    \n      const stream1 = await chat.sendMessageStream({\n        message: \"I have 2 dogs in my house.\",\n      });\n      for await (const chunk of stream1) {\n        console.log(chunk.text);\n        console.log(\"_\".repeat(80));\n      }\n    \n      const stream2 = await chat.sendMessageStream({\n        message: \"How many paws are in my house?\",\n      });\n      for await (const chunk of stream2) {\n        console.log(chunk.text);\n        console.log(\"_\".repeat(80));\n      }\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    cs := model.StartChat()\n    \n    cs.History = []*genai.Content{\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Hello, I have 2 dogs in my house.\"),\n        },\n        Role: \"user\",\n      },\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Great to meet you. What would you like to know?\"),\n        },\n        Role: \"model\",\n      },\n    }\n    \n    iter := cs.SendMessageStream(ctx, genai.Text(\"How many paws are in my house?\"))\n    for {\n      resp, err := iter.Next()\n      if err == iterator.Done {\n        break\n      }\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp)\n    }\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse&key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"Hello\"\n              }\n            ]\n          },\n          {\n            \"role\": \"model\",\n            \"parts\": [\n              {\n                \"text\": \"Great to meet you. What would you like to know?\"\n              }\n            ]\n          },\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"I have two dogs in my house. How many paws are in my house?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            role: 'user',\n            parts: [\n              { text: 'Hello' },\n            ],\n          },\n          {\n            role: 'model',\n            parts: [\n              { text: 'Great to meet you. What would you like to know?' },\n            ],\n          },\n          {\n            role: 'user',\n            parts: [\n              { text: 'I have two dogs in my house. How many paws are in my house?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Configuration parameters\n\nEvery prompt you send to the model includes parameters that control how the model generates responses. You can configure these parameters, or let the model use the default options.\n\nThe following example shows how to configure model parameters:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[\"Explain how AI works\"],\n        config=types.GenerateContentConfig(\n            max_output_tokens=500,\n            temperature=0.1\n        )\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Explain how AI works\",\n        config: {\n          maxOutputTokens: 500,\n          temperature: 0.1,\n        },\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-pro-latest\")\n    model.SetTemperature(0.9)\n    model.SetTopP(0.5)\n    model.SetTopK(20)\n    model.SetMaxOutputTokens(100)\n    model.SystemInstruction = genai.NewUserContent(genai.Text(\"You are Yoda from Star Wars.\"))\n    model.ResponseMIMEType = \"application/json\"\n    resp, err := model.GenerateContent(ctx, genai.Text(\"What is the average size of a swallow?\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    printResponse(resp)\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Explain how AI works\"\n              }\n            ]\n          }\n        ],\n        \"generationConfig\": {\n          \"stopSequences\": [\n            \"Title\"\n          ],\n          \"temperature\": 1.0,\n          \"maxOutputTokens\": 800,\n          \"topP\": 0.8,\n          \"topK\": 10\n        }\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const generationConfig = {\n        temperature: 1,\n        topP: 0.95,\n        topK: 40,\n        maxOutputTokens: 8192,\n        responseMimeType: 'text/plain',\n      };\n    \n      const payload = {\n        generationConfig,\n        contents: [\n          {\n            parts: [\n              { text: 'Explain how AI works in a few words' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nHere are some of the model parameters you can configure. (Naming conventions vary by programming language.)\n\n  * `stopSequences`: Specifies the set of character sequences (up to 5) that will stop output generation. If specified, the API will stop at the first appearance of a `stop_sequence`. The stop sequence won't be included as part of the response.\n  * `temperature`: Controls the randomness of the output. Use higher values for more creative responses, and lower values for more deterministic responses. Values can range from [0.0, 2.0].\n  * `maxOutputTokens`: Sets the maximum number of tokens to include in a candidate.\n  * `topP`: Changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the `topP` value. The default `topP` value is 0.95.\n  * `topK`: Changes how the model selects tokens for output. A `topK` of 1 means the selected token is the most probable among all the tokens in the model's vocabulary, while a `topK` of 3 means that the next token is selected from among the 3 most probable using the temperature. Tokens are further filtered based on `topP` with the final token selected using temperature sampling.\n\n\n\n## System instructions\n\nSystem instructions let you steer the behavior of a model based on your specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full interaction with the user, enabling you to specify product-level behavior separate from the prompts provided by end users.\n\nYou can set system instructions when you initialize your model:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=types.GenerateContentConfig(\n            system_instruction=\"You are a cat. Your name is Neko.\"),\n        contents=\"Hello there\"\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Hello there\",\n        config: {\n          systemInstruction: \"You are a cat. Your name is Neko.\",\n        },\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    // import packages here\n    \n    func main() {\n      ctx := context.Background()\n      client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n      if err != nil {\n        log.Fatal(err)\n      }\n      defer client.Close()\n    \n      model := client.GenerativeModel(\"gemini-2.0-flash\")\n      model.SystemInstruction = &genai.Content{\n        Parts: []genai.Part{genai.Text(`\n          You are a cat. Your name is Neko.\n        `)},\n      }\n      resp, err := model.GenerateContent(ctx, genai.Text(\"Hello there\"))\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp) // helper function for printing content parts\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -d '{\n        \"system_instruction\": {\n          \"parts\": [\n            {\n              \"text\": \"You are a cat. Your name is Neko.\"\n            }\n          ]\n        },\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Hello there\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const systemInstruction = {\n        parts: [{\n          text: 'You are a cat. Your name is Neko.'\n        }]\n      };\n    \n      const payload = {\n        systemInstruction,\n        contents: [\n          {\n            parts: [\n              { text: 'Hello there' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nThen, you can send requests to the model as usual.\n\n## Supported models\n\nThe entire Gemini family of models supports text generation. To learn more about the models and their capabilities, see [Models](/gemini-api/docs/models).\n\n## Prompting tips\n\nFor basic text generation use cases, your prompt might not need to include any output examples, system instructions, or formatting information. This is a [zero-shot](/gemini-api/docs/models/generative-models#zero-shot-prompts) approach. For some use cases, a [one-shot](/gemini-api/docs/models/generative-models#one-shot-prompts) or [few-shot](/gemini-api/docs/models/generative-models#few-shot-prompts) prompt might produce output that's more aligned with user expectations. In some cases, you might also want to provide system instructions to help the model understand the task or follow specific guidelines.\n\n## What's next\n\n  * Try the [Gemini API getting started Colab](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb).\n  * Learn how to use Gemini's [vision understanding](/gemini-api/docs/vision) to process images and videos.\n  * Learn how to use Gemini's [audio understanding](/gemini-api/docs/audio) to process audio files.\n  * Learn about multimodal [file prompting strategies](/gemini-api/docs/file-prompting-strategies).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/grounding#configure-search-tool",
    "title": "Grounding with Google Search  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Grounding with Google Search \n\nPython JavaScript REST\n\nThe Grounding with Google Search feature in the Gemini API and AI Studio can be used to improve the accuracy and recency of responses from the model. In addition to more factual responses, when Grounding with Google Search is enabled, the Gemini API returns grounding sources (in-line supporting links) and Google Search Suggestions along with the response content. The Search Suggestions point users to the search results corresponding to the grounded response.\n\nThis guide will help you get started with Grounding with Google Search.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Google Search Suggestions\n\nTo use Grounding with Google Search, you have to display Google Search Suggestions, which are suggested queries included in the metadata of the grounded response. To learn more about the display requirements, see [Use Google Search Suggestions](/gemini-api/docs/grounding/search-suggestions).\n\n## Google Search retrieval\n\n**Note:** Google Search retrieval is only compatible with Gemini 1.5 models. For Gemini 2.0 models, you should use Search as a tool.\n\n### Dynamic retrieval\n\n**Note:** Dynamic retrieval is only compatible with Gemini 1.5 Flash. For Gemini 2.0, you should use Search as a tool, as shown above.\n\nSome queries are likely to benefit more from Grounding with Google Search than others. The _dynamic retrieval_ feature gives you additional control over when to use Grounding with Google Search.\n\nIf the dynamic retrieval mode is unspecified, Grounding with Google Search is always triggered. If the mode is set to dynamic, the model decides when to use grounding based on a threshold that you can configure. The threshold is a floating-point value in the range [0,1] and defaults to 0.3. If the threshold value is 0, the response is always grounded with Google Search; if it's 1, it never is.\n\n#### How dynamic retrieval works\n\nYou can use dynamic retrieval in your request to choose when to turn on Grounding with Google Search. This is useful when the prompt doesn't require an answer grounded in Google Search and the model can provide an answer based on its own knowledge without grounding. This helps you manage latency, quality, and cost more effectively.\n\nBefore you invoke the dynamic retrieval configuration in your request, understand the following terminology:\n\n  * **Prediction score** : When you request a grounded answer, Gemini assigns a _prediction score_ to the prompt. The prediction score is a floating point value in the range [0,1]. Its value depends on whether the prompt can benefit from grounding the answer with the most up-to-date information from Google Search. Thus, if a prompt requires an answer grounded in the most recent facts on the web, it has a higher prediction score. A prompt for which a model-generated answer is sufficient has a lower prediction score.\n\nHere are examples of some prompts and their prediction scores.\n\n**Note:** The prediction scores are assigned by Gemini and can vary over time depending on several factors. Prompt | Prediction score | Comment  \n---|---|---  \n\"Write a poem about peonies\" | 0.13 | The model can rely on its knowledge and the answer doesn't need grounding.  \n\"Suggest a toy for a 2yo child\" | 0.36 | The model can rely on its knowledge and the answer doesn't need grounding.  \n\"Can you give a recipe for an asian-inspired guacamole?\" | 0.55 | Google Search can give a grounded answer, but grounding isn't strictly required; the model knowledge might be sufficient.  \n\"What's Agent Builder? How is grounding billed in Agent Builder?\" | 0.72 | Requires Google Search to generate a well-grounded answer.  \n\"Who won the latest F1 grand prix?\" | 0.97 | Requires Google Search to generate a well-grounded answer.  \n  * **Threshold** : In your API request, you can specify a dynamic retrieval configuration with a threshold. The threshold is a floating point value in the range [0,1] and defaults to 0.3. If the threshold value is zero, the response is always grounded with Google Search. For all other values of threshold, the following is applicable:\n\n    * If the prediction score is greater than or equal to the threshold, the answer is grounded with Google Search. A lower threshold implies that more prompts have responses that are generated using Grounding with Google Search.\n    * If the prediction score is less than the threshold, the model might still generate the answer, but it isn't grounded with Google Search.\n\n\n\nTo learn how to set the dynamic retrieval threshold using an SDK or the REST API, see the appropriate code example.\n\nTo find a good threshold that suits your business needs, you can create a representative set of queries that you expect to encounter. Then you can sort the queries according to the prediction score in the response and select a good threshold for your use case.\n\n## A grounded response\n\nIf your prompt successfully grounds to Google Search, the response will include `groundingMetadata`. A grounded response might look something like this (parts of the response have been omitted for brevity):\n    \n    \n    {\n      \"candidates\": [\n        {\n          \"content\": {\n            \"parts\": [\n              {\n                \"text\": \"Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships. He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall. \\n\"\n              }\n            ],\n            \"role\": \"model\"\n          },\n          ...\n          \"groundingMetadata\": {\n            \"searchEntryPoint\": {\n              \"renderedContent\": \"\\u003cstyle\\u003e\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n\\u003c/style\\u003e\\n\\u003cdiv class=\\\"container\\\"\\u003e\\n  \\u003cdiv class=\\\"headline\\\"\\u003e\\n    \\u003csvg class=\\\"logo-light\\\" width=\\\"18\\\" height=\\\"18\\\" viewBox=\\\"9 9 35 35\\\" fill=\\\"none\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\"\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\\\" fill=\\\"#4285F4\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\\\" fill=\\\"#34A853\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\\\" fill=\\\"#FBBC05\\\"/\\u003e\\n      \\u003cpath fill-rule=\\\"evenodd\\\" clip-rule=\\\"evenodd\\\" d=\\\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\\\" fill=\\\"#EA4335\\\"/\\u003e\\n    \\u003c/svg\\u003e\\n    \\u003csvg class=\\\"logo-dark\\\" width=\\\"18\\\" height=\\\"18\\\" viewBox=\\\"0 0 48 48\\\" xmlns=\\\"http://www.w3.org/2000/svg\\\"\\u003e\\n      \\u003ccircle cx=\\\"24\\\" cy=\\\"23\\\" fill=\\\"#FFF\\\" r=\\\"22\\\"/\\u003e\\n      \\u003cpath d=\\\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\\\" fill=\\\"#4285F4\\\"/\\u003e\\n      \\u003cpath d=\\\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\\\" fill=\\\"#34A853\\\"/\\u003e\\n      \\u003cpath d=\\\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\\\" fill=\\\"#FBBC05\\\"/\\u003e\\n      \\u003cpath d=\\\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\\\" fill=\\\"#EA4335\\\"/\\u003e\\n    \\u003c/svg\\u003e\\n    \\u003cdiv class=\\\"gradient-container\\\"\\u003e\\u003cdiv class=\\\"gradient\\\"\\u003e\\u003c/div\\u003e\\u003c/div\\u003e\\n  \\u003c/div\\u003e\\n  \\u003cdiv class=\\\"carousel\\\"\\u003e\\n    \\u003ca class=\\\"chip\\\" href=\\\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4x8Epe-gzpwRBvp7o3RZh2m1ygq1EHktn0OWCtvTXjad4bb1zSuqfJd6OEuZZ9_SXZ_P2SvCpJM7NaFfQfiZs6064MeqXego0vSbV9LlAZoxTdbxWK1hFeqTG6kA13YJf7Fbu1SqBYM0cFM4zo0G_sD9NKYWcOCQMvDLDEJFhjrC9DM_QobBIAMq-gWN95G5tvt6_z6EuPN8QY=\\\"\\u003ewho won wimbledon 2024\\u003c/a\\u003e\\n  \\u003c/div\\u003e\\n\\u003c/div\\u003e\\n\"\n            },\n            \"groundingChunks\": [\n              {\n                \"web\": {\n                  \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4whET1ta3sDETZvcicd8FeNe4z0VuduVsxrT677KQRp2rYghXI0VpfYbIMVI3THcTuMwggRCbFXS_wVvW0UmGzMe9h2fyrkvsnQPJyikJasNIbjJLPX0StM4Bd694-ZVle56MmRA4YiUvwSqad1w6O2opmWnw==\",\n                  \"title\": \"wikipedia.org\"\n                }\n              },\n              {\n                \"web\": {\n                  \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wR1M-9-yMPUr_KdHlnoAmQ8ZX90DtQ_vDYTjtP2oR5RH4tRP04uqKPLmesvo64BBkPeYLC2EpVDxv9ngO3S1fs2xh-e78fY4m0GAtgNlahUkm_tBm_sih5kFPc7ill9u2uwesNGUkwrQlmP2mfWNU5lMMr23HGktr6t0sV0QYlzQq7odVoBxYWlQ_sqWFH\",\n                  \"title\": \"wikipedia.org\"\n                }\n              },\n              {\n                \"web\": {\n                  \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4wsDmROzbP-tmt8GdwCW_pqISTZ4IRbBuoaMyaHfcQg8WW-yKRQQvMDTPAuLxJh-8_U8_iw_6JKFbQ8M9oVYtaFdWFK4gOtL4RrC9Jyqc5BNpuxp6uLEKgL5-9TggtNvO97PyCfziDFXPsxylwI1HcfQdrz3Jy7ZdOL4XM-S5rC0lF2S3VWW0IEAEtS7WX861meBYVjIuuF_mIr3spYPqWLhbAY2Spj-4_ba8DjRvmevIFUhRuESTKvBfmpxNSM\",\n                  \"title\": \"cbssports.com\"\n                }\n              },\n              {\n                \"web\": {\n                  \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4yzjLkorHiUKjhOPkWaZ9b4cO-cLG-02vlEl6xTBjMUjyhK04qSIclAa7heR41JQ6AAVXmNdS3WDrLOV4Wli-iezyzW8QPQ4vgnmO_egdsuxhcGk3-Fp8-yfqNLvgXFwY5mPo6QRhvplOFv0_x9mAcka18QuAXtj0SPvJfZhUEgYLCtCrucDS5XFc5HmRBcG1tqFdKSE1ihnp8KLdaWMhrUQI21hHS9\",\n                  \"title\": \"jagranjosh.com\"\n                }\n              },\n              {\n                \"web\": {\n                  \"uri\": \"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AWhgh4y9L4oeNGWCatFz63b9PpP3ys-Wi_zwnkUT5ji9lY7gPUJQcsmmE87q88GSdZqzcx5nZG9usot5FYk2yK-FAGvCRE6JsUQJB_W11_kJU2HVV1BTPiZ4SAgm8XDFIxpCZXnXmEx5HUfRqQm_zav7CvS2qjA2x3__qLME6Jy7R5oza1C5_aqjQu422le9CaigThS5bvJoMo-ZGcXdBUCj2CqoXNVjMA==\",\n                  \"title\": \"apnews.com\"\n                }\n              }\n            ],\n            \"groundingSupports\": [\n              {\n                \"segment\": {\n                  \"endIndex\": 85,\n                  \"text\": \"Carlos Alcaraz won the Gentlemen's Singles title at the 2024 Wimbledon Championships.\"\n                },\n                \"groundingChunkIndices\": [\n                  0,\n                  1,\n                  2,\n                  3\n                ],\n                \"confidenceScores\": [\n                  0.97380733,\n                  0.97380733,\n                  0.97380733,\n                  0.97380733\n                ]\n              },\n              {\n                \"segment\": {\n                  \"startIndex\": 86,\n                  \"endIndex\": 210,\n                  \"text\": \"He defeated Novak Djokovic in the final, winning his second consecutive Wimbledon title and fourth Grand Slam title overall.\"\n                },\n                \"groundingChunkIndices\": [\n                  1,\n                  0,\n                  4\n                ],\n                \"confidenceScores\": [\n                  0.96145374,\n                  0.96145374,\n                  0.96145374\n                ]\n              }\n            ],\n            \"webSearchQueries\": [\n              \"who won wimbledon 2024\"\n            ]\n          }\n        }\n      ],\n      ...\n    }\n    \n\nIf the response doesn't include `groundingMetadata`, this means the response wasn't successfully grounded. There are several reasons this could happen, including low source relevance or incomplete information within the model response.\n\nWhen a grounded result is generated, the metadata contains URIs that redirect to the publishers of the content that was used to generate the grounded result. These URIs contain the `vertexaisearch` subdomain, as in this truncated example: `https://vertexaisearch.cloud.google.com/grounding-api-redirect/...`. The metadata also contains the publishers' domains. The provided URIs remain accessible for 30 days after the grounded result is generated.\n\n**Important:** The provided URIs must be directly accessible by the end users and must not be queried programmatically through automated means. If automated access is detected, the grounded answer generation service might stop providing the redirection URIs.\n\nThe `renderedContent` field within `searchEntryPoint` is the provided code for implementing Google Search Suggestions. See [Use Google Search Suggestions](/gemini-api/docs/grounding/search-suggestions) to learn more.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-11 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-11 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/code-execution#enable-on-model",
    "title": "Code execution  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Code execution \n\nPython JavaScript Go REST\n\nThe Gemini API code execution feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n\n**Note:** Gemini is only able to execute code in Python. You can still ask Gemini to generate code in another language, but the model can't use the code execution tool to run it.\n\nCode execution is available in both AI Studio and the Gemini API. In AI Studio, you can enable code execution in the right panel under **Tools**. The Gemini API provides code execution as a tool, similar to [function calling](/gemini-api/docs/function-calling). After you add code execution as a tool, the model decides when to use it.\n\nThe code execution environment includes the following libraries: `altair`, `chess`, `cv2`, `matplotlib`, `mpmath`, `numpy`, `pandas`, `pdfminer`, `reportlab`, `seaborn`, `sklearn`, `statsmodels`, `striprtf`, `sympy`, and `tabulate`. You can't install your own libraries.\n\n**Note:** Only `matplotlib` is supported for graph rendering using code execution.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Get started with code execution\n\n## Input/output (I/O)\n\nStarting with [Gemini 2.0 Flash](/gemini-api/docs/models/gemini#gemini-2.0-flash), code execution supports file input and graph output. Using these new input and output capabilities, you can upload CSV and text files, ask questions about the files, and have [Matplotlib](https://matplotlib.org/) graphs generated as part of the response.\n\n### I/O pricing\n\nWhen using code execution I/O, you're charged for input tokens and output tokens:\n\n**Input tokens:**\n\n  * User prompt\n\n\n\n**Output tokens:**\n\n  * Code generated by the model\n  * Code execution output in the code environment\n  * Summary generated by the model\n\n\n\n### I/O details\n\nWhen you're working with code execution I/O, be aware of the following technical details:\n\n  * The maximum runtime of the code environment is 30 seconds.\n  * If the code environment generates an error, the model may decide to regenerate the code output. This can happen up to 5 times.\n  * The maximum file input size is limited by the model token window. In AI Studio, using Gemini Flash 2.0, the maximum input file size is 1 million tokens (roughly 2MB for text files of the supported input types). If you upload a file that's too large, AI Studio won't let you send it.\n\n| Single turn | Bidirectional (Multimodal Live API)  \n---|---|---  \nModels supported | All Gemini 2.0 models | Only Flash experimental models  \nFile input types supported | .png, .jpeg, .csv, .xml, .cpp, .java, .py, .js, .ts | .png, .jpeg, .csv, .xml, .cpp, .java, .py, .js, .ts  \nPlotting libraries supported | Matplotlib | Matplotlib  \n[Multi-tool use](/gemini-api/docs/function-calling#multi-tool-use) | No | Yes  \n  \n## Billing\n\nThere's no additional charge for enabling code execution from the Gemini API. You'll be billed at the current rate of input and output tokens based on the Gemini model you're using.\n\nHere are a few other things to know about billing for code execution:\n\n  * You're only billed once for the input tokens you pass to the model, and you're billed for the final output tokens returned to you by the model.\n  * Tokens representing generated code are counted as output tokens. Generated code can include text and multimodal output like images.\n  * Code execution results are also counted as output tokens.\n\n\n\nThe billing model is shown in the following diagram:\n\n  * You're billed at the current rate of input and output tokens based on the Gemini model you're using.\n  * If Gemini uses code execution when generating your response, the original prompt, the generated code, and the result of the executed code are labeled _intermediate tokens_ and are billed as _input tokens_.\n  * Gemini then generates a summary and returns the generated code, the result of the executed code, and the final summary. These are billed as _output tokens_.\n  * The Gemini API includes an intermediate token count in the API response, so you know why you're getting additional input tokens beyond your initial prompt.\n\n\n\n## Limitations\n\n  * The model can only generate and execute code. It can't return other artifacts like media files.\n  * In some cases, enabling code execution can lead to regressions in other areas of model output (for example, writing a story).\n  * There is some variation in the ability of the different models to use code execution successfully.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/function-calling#multi-tool-use",
    "title": "Function Calling with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Function Calling with the Gemini API \n\nFunction calling lets you connect models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:\n\n  * **Augment Knowledge:** Access information from external sources like databases, APIs, and knowledge bases.\n  * **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.\n  * **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices\n\n\n\nGet Weather Schedule Meeting Create Chart\n\n## How Function Calling Works\n\nFunction calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:\n\n  1. **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.\n  2. **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\n  3. **Execute Function Code (Your Responsibility):** The Model _does not_ execute the function itself. It's your application's responsibility to process the response and check for Function Call, if \n     * **Yes** : Extract the name and args of the function and execute the corresponding function in your application.\n     * **No:** The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).\n  4. **Create User friendly response:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.\n\n\n\nThis process can be repeated over multiple turns, allowing for complex interactions and workflows. The model also supports calling multiple functions in a single turn ([parallel function calling](/gemini-api/docs/function-calling#parallel_function_calling)) and in sequence ([compositional function calling](/gemini-api/docs/function-calling#compositional_function_calling)).\n\n### Step 1: Define Function Declaration\n\nDefine a function and its declaration within your application code that allows users to set light values and make an API request. This function could call external services or APIs.\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Define a function that the model can call to control smart lights\n    set_light_values_declaration = {\n        \"name\": \"set_light_values\",\n        \"description\": \"Sets the brightness and color temperature of a light.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n                },\n                \"color_temp\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"daylight\", \"cool\", \"warm\"],\n                    \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n                },\n            },\n            \"required\": [\"brightness\", \"color_temp\"],\n        },\n    }\n    \n    # This is the actual function that would be called based on the model's suggestion\n    def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n        \"\"\"Set the brightness and color temperature of a room light. (mock API).\n    \n        Args:\n            brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n            color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    \n        Returns:\n            A dictionary containing the set brightness and color temperature.\n        \"\"\"\n        return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n    \n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    // Define a function that the model can call to control smart lights\n    const setLightValuesFunctionDeclaration = {\n      name: 'set_light_values',\n      description: 'Sets the brightness and color temperature of a light.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',\n          },\n          color_temp: {\n            type: Type.STRING,\n            enum: ['daylight', 'cool', 'warm'],\n            description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',\n          },\n        },\n        required: ['brightness', 'color_temp'],\n      },\n    };\n    \n    /**\n    * Set the brightness and color temperature of a room light. (mock API)\n    * @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness\n    * @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    * @return {Object} A dictionary containing the set brightness and color temperature.\n    */\n    function setLightValues(brightness, color_temp) {\n      return {\n        brightness: brightness,\n        colorTemperature: color_temp\n      };\n    }\n    \n\n### Step 2: Call the model with function declarations\n\nOnce you have defined your function declarations, you can prompt the model to use the function. It analyzes the prompt and function declarations and decides to respond directly or to call a function. If a function is called the response object will contain a function call suggestion.\n\n### Python\n    \n    \n    from google import genai\n    \n    # Generation Config with Function Declaration\n    tools = types.Tool(function_declarations=[set_light_values_declaration])\n    config = types.GenerateContentConfig(tools=[tools])\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Define user prompt\n    contents = [\n        types.Content(\n            role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n        )\n    ]\n    \n    # Send request with function declarations\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\", config=config, contents=contents\n    )\n    \n    print(response.candidates[0].content.parts[0].function_call)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Generation Config with Function Declaration\n    const config = {\n      tools: [{\n        functionDeclarations: [setLightValuesFunctionDeclaration]\n      }]\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Define user prompt\n    const contents = [\n      {\n        role: 'user',\n        parts: [{ text: 'Turn the lights down to a romantic level' }]\n      }\n    ];\n    \n    // Send request with function declarations\n    const response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(response.functionCalls[0]); \n    \n\nThe model then returns a `functionCall` object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.\n\n### Python\n    \n    \n    id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'\n    \n\n### JavaScript\n    \n    \n    {\n      name: 'set_light_values',\n      args: { brightness: 25, color_temp: 'warm' }\n    }\n    \n\n### Step 3: Execute set_light_values function code\n\nExtract the function call details from the model's response, parse the arguments , and execute the `set_light_values` function in our code.\n\n### Python\n    \n    \n    # Extract tool call details\n    tool_call = response.candidates[0].content.parts[0].function_call\n    \n    if tool_call.name == \"set_light_values\":\n        result = set_light_values(**tool_call.args)\n        print(f\"Function execution result: {result}\")\n    \n\n### JavaScript\n    \n    \n    // Extract tool call details\n    const tool_call = response.functionCalls[0]\n    \n    let result;\n    if (tool_call.name === 'set_light_values') {\n      result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);\n      console.log(`Function execution result: ${JSON.stringify(result)}`);\n    }\n    \n\n### Step 4: Create User friendly response with function result and call the model again\n\nFinally, send the result of the function execution back to the model so it can incorporate this information into its final response to the user.\n\n### Python\n    \n    \n    # Create a function response part\n    function_response_part = types.Part.from_function_response(\n        name=tool_call.name,\n        response={\"result\": result},\n    )\n    \n    # Append function call and result of the function execution to contents\n    contents.append(types.Content(role=\"model\", parts=[types.Part(function_call=tool_call)])) # Append the model's function call message\n    contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n    \n    final_response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=config,\n        contents=contents,\n    )\n    \n    print(final_response.text)\n    \n\n### JavaScript\n    \n    \n    // Create a function response part\n    const function_response_part = {\n      name: tool_call.name,\n      response: { result }\n    }\n    \n    // Append function call and result of the function execution to contents\n    contents.push({ role: 'model', parts: [{ functionCall: tool_call }] });\n    contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });\n    \n    // Get the final response from the model\n    const final_response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(final_response.text);\n    \n\nThis completes the function calling flow. The Model successfully used the `set_light_values` function to perform the request action of the user.\n\n## Function declarations\n\nWhen you implement function calling in a prompt, you create a `tools` object, which contains one or more _`function declarations`_. You define functions using JSON, specifically with a [select subset](https://ai.google.dev/api/caching#Schema) of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schemawr) format. A single function declaration can include the following parameters:\n\n  * `name` (string): A unique name for the function (`get_weather_forecast`, `send_email`). Use descriptive names without spaces or special characters (use underscores or camelCase).\n  * `description` (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful (\"Finds theaters based on location and optionally movie title which is currently playing in theaters.\").\n  * `parameters` (object): Defines the input parameters the function expects. \n    * `type` (string): Specifies the overall data type, such as `object`.\n    * `properties` (object): Lists individual parameters, each with: \n      * `type` (string): The data type of the parameter, such as `string`, `integer`, `boolean, array`.\n      * `description` (string): A description of the parameter's purpose and format. Provide examples and constraints (\"The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.\").\n      * `enum` (array, optional): If the parameter values are from a fixed set, use \"enum\" to list the allowed values instead of just describing them in the description. This improves accuracy (\"enum\": [\"daylight\", \"cool\", \"warm\"]).\n    * `required` (array): An array of strings listing the parameter names that are mandatory for the function to operate.\n\n\n\n## Parallel Function Calling\n\nIn addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco.\n\n### Python\n    \n    \n    power_disco_ball = {\n        \"name\": \"power_disco_ball\",\n        \"description\": \"Powers the spinning disco ball.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"power\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether to turn the disco ball on or off.\",\n                }\n            },\n            \"required\": [\"power\"],\n        },\n    }\n    \n    start_music = {\n        \"name\": \"start_music\",\n        \"description\": \"Play some music matching the specified parameters.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"energetic\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is energetic or not.\",\n                },\n                \"loud\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is loud or not.\",\n                },\n            },\n            \"required\": [\"energetic\", \"loud\"],\n        },\n    }\n    \n    dim_lights = {\n        \"name\": \"dim_lights\",\n        \"description\": \"Dim the lights.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"number\",\n                    \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n                }\n            },\n            \"required\": [\"brightness\"],\n        },\n    }\n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    const powerDiscoBall = {\n      name: 'power_disco_ball',\n      description: 'Powers the spinning disco ball.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          power: {\n            type: Type.BOOLEAN,\n            description: 'Whether to turn the disco ball on or off.'\n          }\n        },\n        required: ['power']\n      }\n    };\n    \n    const startMusic = {\n      name: 'start_music',\n      description: 'Play some music matching the specified parameters.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          energetic: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is energetic or not.'\n          },\n          loud: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is loud or not.'\n          }\n        },\n        required: ['energetic', 'loud']\n      }\n    };\n    \n    const dimLights = {\n      name: 'dim_lights',\n      description: 'Dim the lights.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'\n          }\n        },\n        required: ['brightness']\n      }\n    };\n    \n\nCall the model with an instruction that could use all of the specified tools. This example uses a `tool_config`. To learn more you can read about [configuring function calling](/gemini-api/docs/function-calling#function_calling_modes).\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Set up function declarations\n    house_tools = [\n        types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])\n    ]\n    \n    config = {\n        \"tools\": house_tools,\n        \"automatic_function_calling\": {\"disable\": True},\n        # Force the model to call 'any' function, instead of chatting.\n        \"tool_config\": {\"function_calling_config\": {\"mode\": \"any\"}},\n    }\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Turn this place into a party!\")\n    \n    # Print out each of the function calls requested from this single call\n    print(\"Example 1: Forced function calling\")\n    for fn in response.function_calls:\n        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n        print(f\"{fn.name}({args})\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Set up function declarations\n    const houseFns = [powerDiscoBall, startMusic, dimLights];\n    \n    const config = {\n        tools: [{\n            functionDeclarations: houseFns\n        }],\n        // Force the model to call 'any' function, instead of chatting.\n        toolConfig: {\n            functionCallingConfig: {\n            mode: 'any'\n            }\n        }\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Create a chat session\n    const chat = ai.chats.create({\n        model: 'gemini-2.0-flash',\n        config: config\n    });\n    const response = await chat.sendMessage({message: 'Turn this place into a party!'});\n    \n    // Print out each of the function calls requested from this single call\n    console.log(\"Example 1: Forced function calling\");\n    for (const fn of response.functionCalls) {\n        const args = Object.entries(fn.args)\n            .map(([key, val]) => `${key}=${val}`)\n            .join(', ');\n        console.log(`${fn.name}(${args})`);\n    }\n    \n\nEach of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested. \n\nThe Python SDK supports a feature called [automatic function calling](/gemini-api/docs/function-calling#automatic_function_calling_python_only) which converts the Python function to declarations, handles the function call execution and response cycle for you. Following is an example for our disco use case.\n\n**Note:** Automatic Function Calling is a Python SDK only feature at the moment.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Actual implementation functions\n    def power_disco_ball_impl(power: bool) -> dict:\n        \"\"\"Powers the spinning disco ball.\n    \n        Args:\n            power: Whether to turn the disco ball on or off.\n    \n        Returns:\n            A status dictionary indicating the current state.\n        \"\"\"\n        return {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n    \n    def start_music_impl(energetic: bool, loud: bool) -> dict:\n        \"\"\"Play some music matching the specified parameters.\n    \n        Args:\n            energetic: Whether the music is energetic or not.\n            loud: Whether the music is loud or not.\n    \n        Returns:\n            A dictionary containing the music settings.\n        \"\"\"\n        music_type = \"energetic\" if energetic else \"chill\"\n        volume = \"loud\" if loud else \"quiet\"\n        return {\"music_type\": music_type, \"volume\": volume}\n    \n    def dim_lights_impl(brightness: float) -> dict:\n        \"\"\"Dim the lights.\n    \n        Args:\n            brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n    \n        Returns:\n            A dictionary containing the new brightness setting.\n        \"\"\"\n        return {\"brightness\": brightness}\n    \n    config = {\n        \"tools\": [power_disco_ball_impl, start_music_impl, dim_lights_impl],\n    }\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Do everything you need to this place into party!\")\n    \n    print(\"\\nExample 2: Automatic function calling\")\n    print(response.text)\n    # I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!\n    \n\n## Compositional Function Calling\n\nGemini 2.0 supports compositional function calling, meaning the model can chain multiple function calls together. For example, to answer \"Get the temperature in my current location\", the Gemini API might invoke both a `get_current_location()` function and a `get_weather()` function that takes the location as a parameter.\n\n**Note:** Compositional function calling is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Light control schemas\n    turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}\n    turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}\n    \n    prompt = \"\"\"\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n      \"\"\"\n    \n    tools = [\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}\n    ]\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Light control schemas\n    const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };\n    const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };\n    \n    const prompt = `\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n    `;\n    \n    const tools = [\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }\n    ];\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n## Function calling modes\n\nThe Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the `function_calling_config`.\n\n  * `AUTO (Default)`: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n  * `ANY`: The model is constrained to always predict a function call and guarantee function schema adherence. If `allowed_function_names` is not specified, the model can choose from any of the provided function declarations. If `allowed_function_names` is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call in response to every prompt (if applicable).\n  * `NONE`: The model is _prohibited_ from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.\n\n\n\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Configure function calling mode\n    tool_config = types.ToolConfig(\n        function_calling_config=types.FunctionCallingConfig(\n            mode=\"ANY\", allowed_function_names=[\"get_current_temperature\"]\n        )\n    )\n    \n    # Create the generation config\n    config = types.GenerateContentConfig(\n        temperature=0,\n        tools=[tools],  # not defined here.\n        tool_config=tool_config,\n    )\n    \n\n### JavaScript\n    \n    \n    import { FunctionCallingConfigMode } from '@google/genai';\n    \n    // Configure function calling mode\n    const toolConfig = {\n      functionCallingConfig: {\n        mode: FunctionCallingConfigMode.ANY,\n        allowedFunctionNames: ['get_current_temperature']\n      }\n    };\n    \n    // Create the generation config\n    const config = {\n      temperature: 0,\n      tools: tools, // not defined here.\n      toolConfig: toolConfig,\n    };\n    \n\n## Automatic Function Calling (Python Only)\n\nWhen using the Python SDK, you can provide Python functions directly as tools. The SDK automatically converts the Python function to declarations, handles the function call execution and response cycle for you. The Python SDK then automatically:\n\n  1. Detects function call responses from the model.\n  2. Call the corresponding Python function in your code.\n  3. Sends the function response back to the model.\n  4. Returns the model's final text response.\n\n\n\nTo use this, define your function with type hints and a docstring, and then pass the function itself (not a JSON declaration) as a tool:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Define the function with type hints and docstring\n    def get_current_temperature(location: str) -> dict:\n        \"\"\"Gets the current temperature for a given location.\n    \n        Args:\n            location: The city and state, e.g. San Francisco, CA\n    \n        Returns:\n            A dictionary containing the temperature and unit.\n        \"\"\"\n        # ... (implementation) ...\n        return {\"temperature\": 25, \"unit\": \"Celsius\"}\n    \n    # Configure the client and model\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))  # Replace with your actual API key setup\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature]\n    )  # Pass the function itself\n    \n    # Make the request\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=\"What's the temperature in London?\",\n        config=config,\n    )\n    \n    print(response.text)  # The SDK handles the function call and returns the final text\n    \n\nYou can disable automatic function calling with:\n\n### Python\n    \n    \n    # To disable automatic function calling:\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature],\n        automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n    )\n    \n\n### Automatic Function schema declaration\n\nAutomatic schema extraction from Python functions doesn't work in all cases. For example: it doesn't handle cases where you describe the fields of a nested dictionary-object. The API is able to describe any of the following types:\n\n### Python\n    \n    \n    AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])\n    \n\nTo see what the inferred schema looks like, you can convert it using [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable):\n\n### Python\n    \n    \n    def multiply(a: float, b: float):\n        \"\"\"Returns a * b.\"\"\"\n        return a * b\n    \n    fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)\n    \n    # to_json_dict() provides a clean JSON representation.\n    print(fn_decl.to_json_dict())\n    \n\n## Multi-tool use: Combine Native Tools with Function Calling\n\nWith Gemini 2.0, you can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, [Grounding with Google Search](/gemini-api/docs/grounding) and [code execution](/gemini-api/docs/code-execution), in a request using the [Live API](/gemini-api/docs/live).\n\n**Note:** Multi-tool use is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Multiple tasks example - combining lights, code execution, and search\n    prompt = \"\"\"\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n      \"\"\"\n    \n    tools = [\n        {'google_search': {}},\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.\n    ]\n    \n    # Execute the prompt with specified tools in audio modality\n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Multiple tasks example - combining lights, code execution, and search\n    const prompt = `\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n    `;\n    \n    const tools = [\n      { googleSearch: {} },\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.\n    ];\n    \n    // Execute the prompt with specified tools in audio modality\n    await run(prompt, {tools: tools, modality: \"AUDIO\"});\n    \n\nPython developers can try this out in the [Live API Tool Use notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).\n\n## Use Model Context Protocol (MCP)\n\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard to connect AI applications with external tools, data sources, and systems. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts. You can use models with MCP server using their tool calling capabilities.\n\nMCP servers expose the tools as JSON schema definitions, which can be used with Gemini compatible function declarations. This lets you to use a MCP server with Gemini models directly. Here, you can find an example of how to use a local MCP server with Gemini SDK and the `mcp` SDK.\n\n### Python\n    \n    \n    import asyncio\n    import os\n    from datetime import datetime\n    from google import genai\n    from google.genai import types\n    from mcp import ClientSession, StdioServerParameters\n    from mcp.client.stdio import stdio_client\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Create server parameters for stdio connection\n    server_params = StdioServerParameters(\n        command=\"npx\",  # Executable\n        args=[\"-y\", \"@philschmid/weather-mcp\"],  # Weather MCP Server\n        env=None,  # Optional environment variables\n    )\n    \n    async def run():\n        async with stdio_client(server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                # Prompt to get the weather for the current day in London.\n                prompt = f\"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?\"\n                # Initialize the connection between client and server\n                await session.initialize()\n    \n                # Get tools from MCP session and convert to Gemini Tool objects\n                mcp_tools = await session.list_tools()\n                tools = [\n                    types.Tool(\n                        function_declarations=[\n                            {\n                                \"name\": tool.name,\n                                \"description\": tool.description,\n                                \"parameters\": {\n                                    k: v\n                                    for k, v in tool.inputSchema.items()\n                                    if k not in [\"additionalProperties\", \"$schema\"]\n                                },\n                            }\n                        ]\n                    )\n                    for tool in mcp_tools.tools\n                ]\n    \n                # Send request to the model with MCP function declarations\n                response = client.models.generate_content(\n                    model=\"gemini-2.0-flash\",\n                    contents=prompt,\n                    config=types.GenerateContentConfig(\n                        temperature=0,\n                        tools=tools,\n                    ),\n                )\n    \n                # Check for a function call\n                if response.candidates[0].content.parts[0].function_call:\n                    function_call = response.candidates[0].content.parts[0].function_call\n                    print(function_call)\n                    # Call the MCP server with the predicted tool\n                    result = await session.call_tool(\n                        function_call.name, arguments=function_call.args\n                    )\n                    print(result.content[0].text)\n                    # Continue as shown in step 4 of \"How Function Calling Works\"\n                    # and create a user friendly response\n                else:\n                    print(\"No function call found in the response.\")\n                    print(response.text)\n    \n    # Start the asyncio event loop and run the main function\n    asyncio.run(run())\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    import { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\n    import { StdioClientTransport } from \"@modelcontextprotocol/sdk/client/stdio.js\";\n    \n    // Create server parameters for stdio connection\n    const serverParams = new StdioClientTransport({\n      command: \"npx\",\n      args: [\"-y\", \"@philschmid/weather-mcp\"]\n    });\n    \n    const client = new Client(\n      {\n        name: \"example-client\",\n        version: \"1.0.0\"\n      }\n    );\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Initialize the connection between client and server\n    await client.connect(serverParams);\n    \n    // Get tools from MCP session and convert to Gemini Tool objects\n    const mcpTools = await client.listTools();\n    const tools = mcpTools.tools.map((tool) => {\n      // Filter the parameters to exclude not supported keys\n      const parameters = Object.fromEntries(\n        Object.entries(tool.inputSchema).filter(([key]) => ![\"additionalProperties\", \"$schema\"].includes(key))\n      );\n      return {\n        name: tool.name,\n        description: tool.description,\n        parameters: parameters\n      };\n    });\n    \n    // Send request to the model with MCP function declarations\n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: \"What is the weather in London in the UK on 2024-04-04?\",\n      config: {\n        tools: [{\n          functionDeclarations: tools\n        }],\n      },\n    });\n    \n    // Check for function calls in the response\n    if (response.functionCalls && response.functionCalls.length > 0) {\n      const functionCall = response.functionCalls[0]; // Assuming one function call\n      console.log(`Function to call: ${functionCall.name}`);\n      console.log(`Arguments: ${JSON.stringify(functionCall.args)}`);\n      // Call the MCP server with the predicted tool\n      const result = await client.callTool({name: functionCall.name, arguments: functionCall.args});\n      console.log(result.content[0].text);\n      // Continue as shown in step 4 of \"How Function Calling Works\"\n      // and create a user friendly response\n    } else {\n      console.log(\"No function call found in the response.\");\n      console.log(response.text);\n    }\n    \n    // Close the connection\n    await client.close();\n    \n\n## Supported Models\n\nExperimental models are not included. You can find their capabilities on the [model overview](https://ai.google.dev/gemini-api/docs/models) page.\n\nModel | Function Calling | Parallel Function Calling | Compositional Function Calling(Live API only)  \n---|---|---|---  \nGemini 2.0 Flash | ✔️ | ✔️ | ✔️  \nGemini 2.0 Flash-Lite | X | X | X  \nGemini 1.5 Flash | ✔️ | ✔️ | ✔️  \nGemini 1.5 Pro | ✔️ | ✔️ | ✔️  \n  \n## Best Practices\n\n  * **Function and Parameter Descriptions:** Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.\n  * **Naming:** Use descriptive function names (without spaces, periods, or dashes).\n  * **Strong Typing:** Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.\n  * **Tool Selection:** While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.\n  * **Prompt Engineering:**\n    * Provide context: Tell the model its role (e.g., \"You are a helpful weather assistant.\").\n    * Give instructions: Specify how and when to use functions (e.g., \"Don't guess dates; always use a future date for forecasts.\").\n    * Encourage clarification: Instruct the model to ask clarifying questions if needed.\n  * **Temperature:** Use a low temperature (e.g., 0) for more deterministic and reliable function calls.\n  * **Validation:** If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.\n  * **Error Handling** : Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.\n  * **Security:** Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.\n  * **Token Limits:** Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.\n\n\n\n## Notes and Limitations\n\n  * Only a [subset of the OpenAPI schema](https://ai.google.dev/api/caching#FunctionDeclaration) is supported.\n  * Supported parameter types in Python are limited.\n  * Automatic function calling is a Python SDK feature only.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-07 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-07 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/structured-output#supply-schema-in-config",
    "title": "Generate structured output with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Generate structured output with the Gemini API \n\nPython JavaScript Go REST\n\n  \n\n\nGemini generates unstructured text by default, but some applications require structured text. For these use cases, you can constrain Gemini to respond with JSON, a structured data format suitable for automated processing. You can also constrain the model to respond with one of the options specified in an enum.\n\nHere are a few use cases that might require structured output from the model:\n\n  * Build a database of companies by pulling company information out of newspaper articles.\n  * Pull standardized information out of resumes.\n  * Extract ingredients from recipes and display a link to a grocery website for each ingredient.\n\n\n\nIn your prompt, you can ask Gemini to produce JSON-formatted output, but note that the model is not guaranteed to produce JSON and nothing but JSON. For a more deterministic response, you can pass a specific JSON schema in a [`responseSchema`](/api/rest/v1beta/GenerationConfig#FIELDS.response_schema) field so that Gemini always responds with an expected structure. To learn more about working with schemas, see More about JSON schemas.\n\nThis guide shows you how to generate JSON using the [`generateContent`](/api/rest/v1/models/generateContent) method through the SDK of your choice or using the REST API directly. The examples show text-only input, although Gemini can also produce JSON responses to multimodal requests that include [images](/gemini-api/docs/vision), [videos](/gemini-api/docs/vision), and [audio](/gemini-api/docs/audio).\n\n## More about JSON schemas\n\nWhen you configure the model to return a JSON response, you can use a `Schema` object to define the shape of the JSON data. The `Schema` represents a select subset of the [OpenAPI 3.0 Schema object](https://spec.openapis.org/oas/v3.0.3#schema-object).\n\nHere's a pseudo-JSON representation of all the `Schema` fields:\n    \n    \n    {\n      \"type\": enum (Type),\n      \"format\": string,\n      \"description\": string,\n      \"nullable\": boolean,\n      \"enum\": [\n        string\n      ],\n      \"maxItems\": string,\n      \"minItems\": string,\n      \"properties\": {\n        string: {\n          object (Schema)\n        },\n        ...\n      },\n      \"required\": [\n        string\n      ],\n      \"propertyOrdering\": [\n        string\n      ],\n      \"items\": {\n        object (Schema)\n      }\n    }\n    \n\nThe `Type` of the schema must be one of the OpenAPI [Data Types](https://spec.openapis.org/oas/v3.0.3#data-types). Only a subset of fields is valid for each `Type`. The following list maps each `Type` to valid fields for that type:\n\n  * `string` -> enum, format\n  * `integer` -> format\n  * `number` -> format\n  * `boolean`\n  * `array` -> minItems, maxItems, items\n  * `object` -> properties, required, propertyOrdering, nullable\n\n\n\nHere are some example schemas showing valid type-and-field combinations:\n    \n    \n    { \"type\": \"string\", \"enum\": [\"a\", \"b\", \"c\"] }\n    \n    { \"type\": \"string\", \"format\": \"date-time\" }\n    \n    { \"type\": \"integer\", \"format\": \"int64\" }\n    \n    { \"type\": \"number\", \"format\": \"double\" }\n    \n    { \"type\": \"boolean\" }\n    \n    { \"type\": \"array\", \"minItems\": 3, \"maxItems\": 3, \"items\": { \"type\": ... } }\n    \n    { \"type\": \"object\",\n      \"properties\": {\n        \"a\": { \"type\": ... },\n        \"b\": { \"type\": ... },\n        \"c\": { \"type\": ... }\n      },\n      \"nullable\": true,\n      \"required\": [\"c\"],\n      \"propertyOrdering\": [\"c\", \"b\", \"a\"]\n    }\n    \n\nFor complete documentation of the Schema fields as they're used in the Gemini API, see the [Schema reference](/api/caching#Schema).\n\n### Property ordering\n\nWhen you're working with JSON schemas in the Gemini API, the order of properties is important. By default, the API orders properties alphabetically and does not preserve the order in which the properties are defined (although the [Google Gen AI SDKs](/gemini-api/docs/sdks) may preserve this order). If you're providing examples to the model with a schema configured, and the property ordering of the examples is not consistent with the property ordering of the schema, the output could be rambling or unexpected.\n\nTo ensure a consistent, predictable ordering of properties, you can use the optional `propertyOrdering[]` field.\n    \n    \n    \"propertyOrdering\": [\"recipe_name\", \"ingredients\"]\n    \n\n`propertyOrdering[]` – not a standard field in the OpenAPI specification – is an array of strings used to determine the order of properties in the response. By specifying the order of properties and then providing examples with properties in that same order, you can potentially improve the quality of results.\n\n**Key Point:** To improve results when you're using a JSON schema, set `propertyOrdering[]` and provide examples with a matching property ordering.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/tokens#media-token",
    "title": "Understand and count tokens  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Understand and count tokens \n\nPython JavaScript Go\n\n  \n\n\nGemini and other generative AI models process input and output at a granularity called a _token_.\n\n## About tokens\n\nTokens can be single characters like `z` or whole words like `cat`. Long words are broken up into several tokens. The set of all tokens used by the model is called the vocabulary, and the process of splitting text into tokens is called _tokenization_.\n\nFor Gemini models, a token is equivalent to about 4 characters. 100 tokens is equal to about 60-80 English words.\n\nWhen billing is enabled, the [cost of a call to the Gemini API](/pricing) is determined in part by the number of input and output tokens, so knowing how to count tokens can be helpful.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-01 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-01 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/prompting_with_media?lang=python#prompting-with-videos",
    "title": "File prompting strategies  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  File prompting strategies \n\nThe Gemini family of artificial intelligence (AI) models is built to handle various types of input data, including text, images, and audio. Since these models can handle more than one type or _mode_ of data, the Gemini models are called _multimodal models_ or explained as having _multimodal capabilities_.\n\n**Note:** Prompting with media files is supported by specific Gemini models. For more information about which models support this feature, see the [Vision](/gemini-api/docs/vision) and [Audio](/gemini-api/docs/audio) guides.\n\nThis page provides guidance and best practices for using media files with prompts for the Gemini API. For information on using the Gemini Files API for uploading media files and including them in your prompts, see the [Vision](/gemini-api/docs/vision), [Audio](/gemini-api/docs/audio), and [Document processing](/gemini-api/docs/document-processing) guides.\n\n## Using media files with prompts\n\nBeing able to use various types of data in your prompts gives you more flexibility in terms of what tasks you can tackle with the Gemini API. For example, you can send the model a photo of a delicious meal and ask it to write a short blog about the meal.\n\n**Prompt** | **Response**  \n---|---  \nWrite a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.   \n|  Meal prepping is a great way to save time and money, and it can also help you to eat healthier. This meal is a great example of a healthy and delicious meal that can be easily prepped ahead of time. This meal features brown rice, roasted vegetables, and chicken teriyaki. The brown rice is a healthy and complex carbohydrate that will give you sustained energy. The roasted vegetables are a great way to get your daily dose of vitamins and minerals, and the chicken teriyaki is a delicious and protein-rich option. This meal is also very easy to prepare. Simply cook the brown rice, roast the vegetables, and cook the chicken teriyaki. Once everything is cooked, divide it into meal prep containers and store them in the refrigerator. You can then grab a container and go on busy mornings! If you're looking for a healthy and delicious meal that can be easily prepped ahead of time, this meal is a great option. It's packed with nutrients and flavor, and it's sure to keep you feeling full and satisfied. Here's to healthy and delicious meal prepping!  \n  \n## Strategies for prompting with media\n\nIf you are having trouble getting the output you want from prompts that use media files, there are some strategies that can help you get the results you want. The following sections provide design approaches and troubleshooting tips for improving prompts that use multimodal input.\n\nYou can improve your multimodal prompts by following these best practices:\n\n  * ### Prompt design fundamentals\n\n    * **Be specific in your instructions** : Craft clear and concise instructions that leave minimal room for misinterpretation.\n    * **Add a few examples to your prompt:** Use realistic few-shot examples to illustrate what you want to achieve.\n    * **Break it down step-by-step** : Divide complex tasks into manageable sub-goals, guiding the model through the process.\n    * **Specify the output format** : In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more. \n    * **Put your image first for single-image prompts** : While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image is placed before the text prompt.\n  * ### Troubleshooting your multimodal prompt\n\n    * **If the model is not drawing information from the relevant part of the image:** Drop hints with which aspects of the image you want the prompt to draw information from.\n    * **If the model output is too generic (not tailored enough to the image):** At the start of the prompt, try asking the model to describe the image(s) before providing the task instruction, or try asking the model to refer to what's in the image.\n\n\n\n## Prompt design fundamentals\n\nThis section expands on the best practices listed in the previous section.\n\nNote: Model results from the following Gemini examples can vary from run to run.\n\n### Be specific in your instructions\n\nPrompts have the most success when they are clear and detailed. If you have a specific output in mind, it's better to include that requirement in the prompt to ensure you get the output you want. \n\nSometimes, a prompt's intent might seem clear to the person who wrote it, but it is actually under-specified. To customize model behavior, consider how your prompt could be (mis)interpreted, and ensure that the instructions you give are specific and clear. Remember to leave minimal room for misinterpretation\n\nFor this image of an airport board, asking the model to just \"describe this image\" could generate a general description. If you need the model to parse the time and city from the image, you can include that request directly in your prompt.\n\nPrompt | Model response  \n---|---  \nDescribe this image. | The image shows an airport arrivals and departures board.  \n**Updated prompt** | **Improved response**  \n---|---  \nParse the time and city from the airport board shown in this image into a list. | \n* 10:50 Moscow\n* 11:05 Edinburgh\n* 11:05 London\n* 11:10 Bucharest\n* 11:30 Kiev\n* 11:35 Dublin\n* 11:45 East Midlands\n* 12:15 Sofia\n* 12:30 London\n* 12:30 Newcastle\n* 12:40 St Petersburg\n* 12:40 London\n* 12:45 Manchester  \n  \n### Add a few examples\n\nThe Gemini model can accept multiple inputs which it can use as examples to understand the output you want. Adding these examples can help the model identify the patterns and apply the relationship between the given images and responses to the new example. This is also called \"few-shot\" learning.\n\nIn the example below, the initial output is written in sentence form, and also contains the country (Brazil). Suppose you want the output in a different format or style, and you want only the city, not the country. Adding few-shot examples to your prompt can steer the model to respond in the way you want.\n\n**Prompt** | **Model response**  \n---|---  \nDetermine the city along with the landmark. | The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil.  \n**Updated prompt** | **Improved response**  \n---|---  \nDetermine the city along with the landmark. |  city: Rome, landmark: the Colosseum.  \n|  city: Beijing, landmark: Forbidden City  \n|  city: Rio de Janeiro, landmark: Christ the Redeemer statue  \n  \n### Break it down step-by-step\n\nFor complex tasks like the ones that require both visual understanding and reasoning, it can be helpful to split the task into smaller, more straightforward steps. Alternatively, it could also be effective if you directly ask the model to “think step by step” in your prompt.\n\nPrompt | Model response  \n---|---  \nWhen will I run out of toilet paper? | Soon, you only have 3 rolls left.  \n**Updated prompt** | **Improved response**  \n---|---  \n  \n1\\. First, count how many toilet paper rolls are in this picture.\n2\\. Then, determine how much toilet paper a typical person uses per day.\n3\\. Calculate how long these rolls of toilet paper will last.\n| \n\n1\\. There are 3 rolls of toilet paper in this picture.\n2\\. A typical person uses about 20 sheets of toilet paper per day.\n3\\. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month.\n  \n  \nMath problems or other types of word problems are great candidates for asking the model to think step-by-step.\n\n**Prompt** | **Response**  \n---|---  \nWhat is the 4th term in the sequence? | -135   \n  \nThe response from the model is incorrect. Some ways to improve this is to ask the model to parse the problem before answering the prompt.\n\n**Prompt** | **Response**  \n---|---  \nParse the formula in the image first. Then based on the formula, answer \"what's the 4th term in the sequence?\". |  _Click to expand the result_ $$ b(n) = b(n-1) * (-3) $$ $$ b(1) = 15 $$\n\n  1. Substitute n = 4 into the formula:\n$$ b(4) = b(4-1) * (-3) $$ $$ b(4) = b(3) * (-3) $$\n  2. We don't know the value of b(3), so substitute n = 3 into the formula:\n$$ b(3) = b(3-1) * (-3) $$ $$ b(3) = b(2) * (-3) $$\n  3. We don't know the value of b(2), so substitute n = 2 into the formula:\n$$ b(2) = b(2-1) * (-3) $$ $$ b(2) = b(1) * (-3) $$\n  4. We know the value of b(1) is 15, so substitute it into the formula:\n$$ b(2) = 15 * (-3) $$ $$ b(2) = -45 $$\n  5. Now that we know the value of b(2), substitute it into the formula for b(3):\n$$ b(3) = b(2) * (-3) $$ $$ b(3) = -45 * (-3) $$ $$ b(3) = 135 $$\n  6. Now that we know the value of b(3), substitute it into the formula for b(4):\n$$ b(4) = b(3) * (-3) $$ $$ b(4) = 135 * (-3) $$ $$ b(4) = -405 $$ \nTherefore, the 4th term in the sequence is -405.  \n  \nThis model response is correct - now to make the response even more streamlined, you can ask the model to to think step-by-step.\n\n**Updated prompt** | **Improved response**  \n---|---  \nWhat is the 4th term in the sequence? Think step by step. | The first term is 15. The second term is b(1) * (-3) = 15 * (-3) = -45. The third term is b(2) * (-3) = -45 * (-3) = 135. The fourth term is b(3) * (-3) = 135 * (-3) = -405.  \n  \n### Try specifying the output format\n\nA common problem is the need for model output to be in a certain format (e.g. markdown, JSON, HTML), especially if the model output needs to be ingested by a downstream task. You can try to produce output in that format by instructing the model to do so within the prompt. Below are two examples:\n\nPrompt | Model response  \n---|---  \nParse the table in this image into markdown format |  | Subject | Status | Time | Start date | Due on | | ------- | ------ | ---- | --------- | ------ | | Math | Done | 1.5 hrs | 11/29/2023 | 12/1/2023 | | Geography | Done | 30 min | 12/2/2023 | 12/4/2023 | | Physics | In progress | 2 hrs | 12/3/2023 | 12/5/2023 | | Biology | Not started | 6 hrs | 11/2/2016 | 12/6/2023 |  \n**Prompt** | **Model response**  \n---|---  \nProvide a list of all the following attributes: ingredients, type of cuisine, vegetarian or not, in JSON format | ` ```json``` { \"ingredients\": [ \"rice\", \"shrimp\", \"clams\", \"mussels\", \"peas\", \"tomatoes\", \"onions\", \"garlic\", \"olive oil\", \"paprika\", \"salt\", \"pepper\" ], \"type of cuisine\": \"Spanish\", \"vegetarian\": false `  \n  \n### Put your image first for single-image prompts\n\nWhile Gemini can interpret image and text in any order within a prompt, placing a single image before the text prompt might lead to better results. In the following samples, you'll notice the image comes first before the text in each prompt.\n\n## Troubleshooting your multimodal prompt\n\nYou might need to troubleshoot your prompt if you are not getting a helpful response. Here are a few strategies you could try.\n\n### If the model is not drawing information from the relevant part of the image\n\nTo get a more specific response, you can point out which aspects of the image you want the prompt to draw information from to inform its response.\n\nPrompt | Model response  \n---|---  \nHow many days will these diapers last a baby? | The diapers will last for 198 days before they run out.  \n**Updated prompt** | **Improved response**  \n---|---  \nHow long will these diapers last before I run out? Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box. Divide the total number by how many diapers the child goes through per day. |  The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs. The average child in this weight range is likely between 1 and 2 years old. A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6).  \n  \n### If the model output is too generic and not tailored enough to the image input\n\nTo help the model tailor its response to the image(s), try asking it to describe the images before performing its reasoning task.\n\nPrompt | Model response |  What is in common between these images? | All four images are examples of landscape photography.  \n---|---  \n**Updated prompt** | **Improved response**  \n---|---  \nFirst, describe what's in each image in detail. What's in common between these images? |  The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea. The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity. The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private and public realms. It's a place where we prepare ourselves to enter or leave the outside world. The fourth image shows the sea in between being calm and being stormy. Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action.  \n  \nYou can also try explicitly asking the model to refer to what's in the image.\n\n**Updated prompt** | **Improved response**  \n---|---  \nWhat is in common between these images? Refer to what's in the images in your response. |  All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a dark and stormy sea. These images are all related to travel because they are all things that people need or use when they travel. The first image shows a person packing a suitcase, which is something that people do when they are going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people often go to buy things for their trips. The third image shows a stack of toilet paper, which is something that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is something that people often encounter when they are traveling.  \n  \nThe same tips work for creative output from images. In the following example with the rabbit, explicitly asking the model to include aspects from the image can improve the output.\n\nPrompt | Model response |  Can you write me a poem about this image? |  Little bunny, so soft and brown,  \nHopping through the field, up and down.  \nYour ears so long, your tail so small,  \nYou're the cutest creature of them all. You nibble on the grass, so green and sweet,  \nAnd hop away when you see me.  \nBut I'll always be your friend,  \nLittle bunny, until the very end.  \n---|---  \n**Updated prompt** | **Improved response**  \n---|---  \nCan you write me a descriptive and dramatic poem about this image and include the location? |  Little bunny on the green,  \nEars so long, so keen.  \nWhat do you see out there,  \nIn the distance, where you stare? Is it the ocean blue,  \nSparkling in the sun's hue?  \nOr the waves that crash and roar,  \nA rhythm that you adore? Little bunny, tell me true,  \nWhat captures your attention so?  \nIs it the salty breeze,  \nOr the seagulls' cries that please? Perhaps it's the vastness of it all,  \nA world so big and small.  \nA place where dreams can roam,  \nAnd adventures can be sown. Little bunny, I wonder what you dream,  \nAs you sit on the grass, so serene.  \nDo you yearn to explore the deep,  \nOr stay on land, where you can leap? Whatever it may be, little bunny,  \nKeep that spark of wonder burning brightly.  \nFor in your dreams and aspirations,  \nLies a world waiting for your creation.  \n  \n### Troubleshooting which part of the prompt failed\n\nIt can be hard to know whether a prompt failed because the model didn't **understand the image** to begin with, or if it did understand the image but did not perform the correct**reasoning steps** afterward.\n\nTo disambiguate between those reasons, ask the model to describe what's in the image.\n\nIn this example below, if the model responds with a snack that seems surprising when paired with tea (e.g. popcorn), you can first troubleshoot to determine whether the model correctly recognized that the image contains tea.\n\nPrompt | Prompt for troubleshooting  \n---|---  \nWhat's a snack I can make in 1 minute that would go well with this? | Describe what's in this image.   \n  \nAnother strategy is to ask the model to explain its reasoning. That can help you narrow down which part of the reasoning broke down, if any.\n\nPrompt | Prompt for troubleshooting  \n---|---  \nWhat's a snack I can make in 1 minute that would go well with this? | What's a snack I can make in 1 minute that would go well with this? Please explain why.  \n  \n### Tuning the sampling parameters\n\nIn each request, you send not only the multimodal prompt but a set of sampling parameters to the model. The model can generate different results for different parameter values. Experiment with the different parameters to get the best values for the task. The most commonly adjusted parameters are the following:\n\n  * Temperature\n  * top-P\n  * top-K\n\n\n\n#### Temperature\n\nTemperature is used for sampling during response generation, which occurs when top-P and top-K are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic and less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected.  \n\n\nFor most use cases, try starting with a temperature of 0.4. If you need more creative results, try increasing the temperature. If you observe clear hallucinations, try reducing the temperature.\n\n#### Top-K\n\nTop-K changes how the model selects tokens for output. A top-K of 1 means the next selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-K of 3 means that the next token is selected from among the three most probable tokens by using temperature.  \n\n\nFor each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further filtered based on top-P with the final token selected using temperature sampling.\n\nSpecify a lower value for less random responses and a higher value for more random responses. The default value of top-K is 32.\n\n#### Top-P\n\nTop-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K) to least probable until the sum of their probabilities equals the top-P value. For example, if tokens A, B, and C have a probability of 0.6, 0.3, 0.1 and the top-P value is 0.9, then the model will select either A or B as the next token by using temperature and excludes C as a candidate.  \n\n\nSpecify a lower value for less random responses and a higher value for more random responses. The default value of top-P is 1.0.\n\n## Next steps\n\n  * Try writing your own multimodal prompts using [Google AI Studio](http://aistudio.google.com).\n  * For more guidance on prompt design, see the [Prompt strategies](/gemini-api/docs/prompting-strategies) page.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/prompting_with_media?lang=python#audio_formats",
    "title": "File prompting strategies  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  File prompting strategies \n\nThe Gemini family of artificial intelligence (AI) models is built to handle various types of input data, including text, images, and audio. Since these models can handle more than one type or _mode_ of data, the Gemini models are called _multimodal models_ or explained as having _multimodal capabilities_.\n\n**Note:** Prompting with media files is supported by specific Gemini models. For more information about which models support this feature, see the [Vision](/gemini-api/docs/vision) and [Audio](/gemini-api/docs/audio) guides.\n\nThis page provides guidance and best practices for using media files with prompts for the Gemini API. For information on using the Gemini Files API for uploading media files and including them in your prompts, see the [Vision](/gemini-api/docs/vision), [Audio](/gemini-api/docs/audio), and [Document processing](/gemini-api/docs/document-processing) guides.\n\n## Using media files with prompts\n\nBeing able to use various types of data in your prompts gives you more flexibility in terms of what tasks you can tackle with the Gemini API. For example, you can send the model a photo of a delicious meal and ask it to write a short blog about the meal.\n\n**Prompt** | **Response**  \n---|---  \nWrite a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.   \n|  Meal prepping is a great way to save time and money, and it can also help you to eat healthier. This meal is a great example of a healthy and delicious meal that can be easily prepped ahead of time. This meal features brown rice, roasted vegetables, and chicken teriyaki. The brown rice is a healthy and complex carbohydrate that will give you sustained energy. The roasted vegetables are a great way to get your daily dose of vitamins and minerals, and the chicken teriyaki is a delicious and protein-rich option. This meal is also very easy to prepare. Simply cook the brown rice, roast the vegetables, and cook the chicken teriyaki. Once everything is cooked, divide it into meal prep containers and store them in the refrigerator. You can then grab a container and go on busy mornings! If you're looking for a healthy and delicious meal that can be easily prepped ahead of time, this meal is a great option. It's packed with nutrients and flavor, and it's sure to keep you feeling full and satisfied. Here's to healthy and delicious meal prepping!  \n  \n## Strategies for prompting with media\n\nIf you are having trouble getting the output you want from prompts that use media files, there are some strategies that can help you get the results you want. The following sections provide design approaches and troubleshooting tips for improving prompts that use multimodal input.\n\nYou can improve your multimodal prompts by following these best practices:\n\n  * ### Prompt design fundamentals\n\n    * **Be specific in your instructions** : Craft clear and concise instructions that leave minimal room for misinterpretation.\n    * **Add a few examples to your prompt:** Use realistic few-shot examples to illustrate what you want to achieve.\n    * **Break it down step-by-step** : Divide complex tasks into manageable sub-goals, guiding the model through the process.\n    * **Specify the output format** : In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more. \n    * **Put your image first for single-image prompts** : While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image is placed before the text prompt.\n  * ### Troubleshooting your multimodal prompt\n\n    * **If the model is not drawing information from the relevant part of the image:** Drop hints with which aspects of the image you want the prompt to draw information from.\n    * **If the model output is too generic (not tailored enough to the image):** At the start of the prompt, try asking the model to describe the image(s) before providing the task instruction, or try asking the model to refer to what's in the image.\n\n\n\n## Prompt design fundamentals\n\nThis section expands on the best practices listed in the previous section.\n\nNote: Model results from the following Gemini examples can vary from run to run.\n\n### Be specific in your instructions\n\nPrompts have the most success when they are clear and detailed. If you have a specific output in mind, it's better to include that requirement in the prompt to ensure you get the output you want. \n\nSometimes, a prompt's intent might seem clear to the person who wrote it, but it is actually under-specified. To customize model behavior, consider how your prompt could be (mis)interpreted, and ensure that the instructions you give are specific and clear. Remember to leave minimal room for misinterpretation\n\nFor this image of an airport board, asking the model to just \"describe this image\" could generate a general description. If you need the model to parse the time and city from the image, you can include that request directly in your prompt.\n\nPrompt | Model response  \n---|---  \nDescribe this image. | The image shows an airport arrivals and departures board.  \n**Updated prompt** | **Improved response**  \n---|---  \nParse the time and city from the airport board shown in this image into a list. | \n* 10:50 Moscow\n* 11:05 Edinburgh\n* 11:05 London\n* 11:10 Bucharest\n* 11:30 Kiev\n* 11:35 Dublin\n* 11:45 East Midlands\n* 12:15 Sofia\n* 12:30 London\n* 12:30 Newcastle\n* 12:40 St Petersburg\n* 12:40 London\n* 12:45 Manchester  \n  \n### Add a few examples\n\nThe Gemini model can accept multiple inputs which it can use as examples to understand the output you want. Adding these examples can help the model identify the patterns and apply the relationship between the given images and responses to the new example. This is also called \"few-shot\" learning.\n\nIn the example below, the initial output is written in sentence form, and also contains the country (Brazil). Suppose you want the output in a different format or style, and you want only the city, not the country. Adding few-shot examples to your prompt can steer the model to respond in the way you want.\n\n**Prompt** | **Model response**  \n---|---  \nDetermine the city along with the landmark. | The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil.  \n**Updated prompt** | **Improved response**  \n---|---  \nDetermine the city along with the landmark. |  city: Rome, landmark: the Colosseum.  \n|  city: Beijing, landmark: Forbidden City  \n|  city: Rio de Janeiro, landmark: Christ the Redeemer statue  \n  \n### Break it down step-by-step\n\nFor complex tasks like the ones that require both visual understanding and reasoning, it can be helpful to split the task into smaller, more straightforward steps. Alternatively, it could also be effective if you directly ask the model to “think step by step” in your prompt.\n\nPrompt | Model response  \n---|---  \nWhen will I run out of toilet paper? | Soon, you only have 3 rolls left.  \n**Updated prompt** | **Improved response**  \n---|---  \n  \n1\\. First, count how many toilet paper rolls are in this picture.\n2\\. Then, determine how much toilet paper a typical person uses per day.\n3\\. Calculate how long these rolls of toilet paper will last.\n| \n\n1\\. There are 3 rolls of toilet paper in this picture.\n2\\. A typical person uses about 20 sheets of toilet paper per day.\n3\\. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month.\n  \n  \nMath problems or other types of word problems are great candidates for asking the model to think step-by-step.\n\n**Prompt** | **Response**  \n---|---  \nWhat is the 4th term in the sequence? | -135   \n  \nThe response from the model is incorrect. Some ways to improve this is to ask the model to parse the problem before answering the prompt.\n\n**Prompt** | **Response**  \n---|---  \nParse the formula in the image first. Then based on the formula, answer \"what's the 4th term in the sequence?\". |  _Click to expand the result_ $$ b(n) = b(n-1) * (-3) $$ $$ b(1) = 15 $$\n\n  1. Substitute n = 4 into the formula:\n$$ b(4) = b(4-1) * (-3) $$ $$ b(4) = b(3) * (-3) $$\n  2. We don't know the value of b(3), so substitute n = 3 into the formula:\n$$ b(3) = b(3-1) * (-3) $$ $$ b(3) = b(2) * (-3) $$\n  3. We don't know the value of b(2), so substitute n = 2 into the formula:\n$$ b(2) = b(2-1) * (-3) $$ $$ b(2) = b(1) * (-3) $$\n  4. We know the value of b(1) is 15, so substitute it into the formula:\n$$ b(2) = 15 * (-3) $$ $$ b(2) = -45 $$\n  5. Now that we know the value of b(2), substitute it into the formula for b(3):\n$$ b(3) = b(2) * (-3) $$ $$ b(3) = -45 * (-3) $$ $$ b(3) = 135 $$\n  6. Now that we know the value of b(3), substitute it into the formula for b(4):\n$$ b(4) = b(3) * (-3) $$ $$ b(4) = 135 * (-3) $$ $$ b(4) = -405 $$ \nTherefore, the 4th term in the sequence is -405.  \n  \nThis model response is correct - now to make the response even more streamlined, you can ask the model to to think step-by-step.\n\n**Updated prompt** | **Improved response**  \n---|---  \nWhat is the 4th term in the sequence? Think step by step. | The first term is 15. The second term is b(1) * (-3) = 15 * (-3) = -45. The third term is b(2) * (-3) = -45 * (-3) = 135. The fourth term is b(3) * (-3) = 135 * (-3) = -405.  \n  \n### Try specifying the output format\n\nA common problem is the need for model output to be in a certain format (e.g. markdown, JSON, HTML), especially if the model output needs to be ingested by a downstream task. You can try to produce output in that format by instructing the model to do so within the prompt. Below are two examples:\n\nPrompt | Model response  \n---|---  \nParse the table in this image into markdown format |  | Subject | Status | Time | Start date | Due on | | ------- | ------ | ---- | --------- | ------ | | Math | Done | 1.5 hrs | 11/29/2023 | 12/1/2023 | | Geography | Done | 30 min | 12/2/2023 | 12/4/2023 | | Physics | In progress | 2 hrs | 12/3/2023 | 12/5/2023 | | Biology | Not started | 6 hrs | 11/2/2016 | 12/6/2023 |  \n**Prompt** | **Model response**  \n---|---  \nProvide a list of all the following attributes: ingredients, type of cuisine, vegetarian or not, in JSON format | ` ```json``` { \"ingredients\": [ \"rice\", \"shrimp\", \"clams\", \"mussels\", \"peas\", \"tomatoes\", \"onions\", \"garlic\", \"olive oil\", \"paprika\", \"salt\", \"pepper\" ], \"type of cuisine\": \"Spanish\", \"vegetarian\": false `  \n  \n### Put your image first for single-image prompts\n\nWhile Gemini can interpret image and text in any order within a prompt, placing a single image before the text prompt might lead to better results. In the following samples, you'll notice the image comes first before the text in each prompt.\n\n## Troubleshooting your multimodal prompt\n\nYou might need to troubleshoot your prompt if you are not getting a helpful response. Here are a few strategies you could try.\n\n### If the model is not drawing information from the relevant part of the image\n\nTo get a more specific response, you can point out which aspects of the image you want the prompt to draw information from to inform its response.\n\nPrompt | Model response  \n---|---  \nHow many days will these diapers last a baby? | The diapers will last for 198 days before they run out.  \n**Updated prompt** | **Improved response**  \n---|---  \nHow long will these diapers last before I run out? Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box. Divide the total number by how many diapers the child goes through per day. |  The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs. The average child in this weight range is likely between 1 and 2 years old. A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6).  \n  \n### If the model output is too generic and not tailored enough to the image input\n\nTo help the model tailor its response to the image(s), try asking it to describe the images before performing its reasoning task.\n\nPrompt | Model response |  What is in common between these images? | All four images are examples of landscape photography.  \n---|---  \n**Updated prompt** | **Improved response**  \n---|---  \nFirst, describe what's in each image in detail. What's in common between these images? |  The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea. The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity. The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private and public realms. It's a place where we prepare ourselves to enter or leave the outside world. The fourth image shows the sea in between being calm and being stormy. Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action.  \n  \nYou can also try explicitly asking the model to refer to what's in the image.\n\n**Updated prompt** | **Improved response**  \n---|---  \nWhat is in common between these images? Refer to what's in the images in your response. |  All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a dark and stormy sea. These images are all related to travel because they are all things that people need or use when they travel. The first image shows a person packing a suitcase, which is something that people do when they are going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people often go to buy things for their trips. The third image shows a stack of toilet paper, which is something that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is something that people often encounter when they are traveling.  \n  \nThe same tips work for creative output from images. In the following example with the rabbit, explicitly asking the model to include aspects from the image can improve the output.\n\nPrompt | Model response |  Can you write me a poem about this image? |  Little bunny, so soft and brown,  \nHopping through the field, up and down.  \nYour ears so long, your tail so small,  \nYou're the cutest creature of them all. You nibble on the grass, so green and sweet,  \nAnd hop away when you see me.  \nBut I'll always be your friend,  \nLittle bunny, until the very end.  \n---|---  \n**Updated prompt** | **Improved response**  \n---|---  \nCan you write me a descriptive and dramatic poem about this image and include the location? |  Little bunny on the green,  \nEars so long, so keen.  \nWhat do you see out there,  \nIn the distance, where you stare? Is it the ocean blue,  \nSparkling in the sun's hue?  \nOr the waves that crash and roar,  \nA rhythm that you adore? Little bunny, tell me true,  \nWhat captures your attention so?  \nIs it the salty breeze,  \nOr the seagulls' cries that please? Perhaps it's the vastness of it all,  \nA world so big and small.  \nA place where dreams can roam,  \nAnd adventures can be sown. Little bunny, I wonder what you dream,  \nAs you sit on the grass, so serene.  \nDo you yearn to explore the deep,  \nOr stay on land, where you can leap? Whatever it may be, little bunny,  \nKeep that spark of wonder burning brightly.  \nFor in your dreams and aspirations,  \nLies a world waiting for your creation.  \n  \n### Troubleshooting which part of the prompt failed\n\nIt can be hard to know whether a prompt failed because the model didn't **understand the image** to begin with, or if it did understand the image but did not perform the correct**reasoning steps** afterward.\n\nTo disambiguate between those reasons, ask the model to describe what's in the image.\n\nIn this example below, if the model responds with a snack that seems surprising when paired with tea (e.g. popcorn), you can first troubleshoot to determine whether the model correctly recognized that the image contains tea.\n\nPrompt | Prompt for troubleshooting  \n---|---  \nWhat's a snack I can make in 1 minute that would go well with this? | Describe what's in this image.   \n  \nAnother strategy is to ask the model to explain its reasoning. That can help you narrow down which part of the reasoning broke down, if any.\n\nPrompt | Prompt for troubleshooting  \n---|---  \nWhat's a snack I can make in 1 minute that would go well with this? | What's a snack I can make in 1 minute that would go well with this? Please explain why.  \n  \n### Tuning the sampling parameters\n\nIn each request, you send not only the multimodal prompt but a set of sampling parameters to the model. The model can generate different results for different parameter values. Experiment with the different parameters to get the best values for the task. The most commonly adjusted parameters are the following:\n\n  * Temperature\n  * top-P\n  * top-K\n\n\n\n#### Temperature\n\nTemperature is used for sampling during response generation, which occurs when top-P and top-K are applied. Temperature controls the degree of randomness in token selection. Lower temperatures are good for prompts that require a more deterministic and less open-ended or creative response, while higher temperatures can lead to more diverse or creative results. A temperature of 0 is deterministic, meaning that the highest probability response is always selected.  \n\n\nFor most use cases, try starting with a temperature of 0.4. If you need more creative results, try increasing the temperature. If you observe clear hallucinations, try reducing the temperature.\n\n#### Top-K\n\nTop-K changes how the model selects tokens for output. A top-K of 1 means the next selected token is the most probable among all tokens in the model's vocabulary (also called greedy decoding), while a top-K of 3 means that the next token is selected from among the three most probable tokens by using temperature.  \n\n\nFor each token selection step, the top-K tokens with the highest probabilities are sampled. Then tokens are further filtered based on top-P with the final token selected using temperature sampling.\n\nSpecify a lower value for less random responses and a higher value for more random responses. The default value of top-K is 32.\n\n#### Top-P\n\nTop-P changes how the model selects tokens for output. Tokens are selected from the most (see top-K) to least probable until the sum of their probabilities equals the top-P value. For example, if tokens A, B, and C have a probability of 0.6, 0.3, 0.1 and the top-P value is 0.9, then the model will select either A or B as the next token by using temperature and excludes C as a candidate.  \n\n\nSpecify a lower value for less random responses and a higher value for more random responses. The default value of top-P is 1.0.\n\n## Next steps\n\n  * Try writing your own multimodal prompts using [Google AI Studio](http://aistudio.google.com).\n  * For more guidance on prompt design, see the [Prompt strategies](/gemini-api/docs/prompting-strategies) page.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-02-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-02-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/vision#image-input",
    "title": "Explore vision capabilities with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Explore vision capabilities with the Gemini API \n\nPython Node.js Go REST\n\nGemini models are able to process images and videos, enabling many frontier developer use cases that would have historically required domain specific models. Some of Gemini's vision capabilities include the ability to:\n\n  * Caption and answer questions about images\n  * Transcribe and reason over PDFs, including up to 2 million tokens\n  * Describe, segment, and extract information from videos up to 90 minutes long\n  * Detect objects in an image and return bounding box coordinates for them\n\n\n\nGemini was built to be multimodal from the ground up and we continue to push the frontier of what is possible.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## What's next\n\nThis guide shows how to upload image and video files using the File API and then generate text outputs from image and video inputs. To learn more, see the following resources:\n\n  * [File prompting strategies](/gemini-api/docs/file-prompting-strategies): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.\n  * [System instructions](/gemini-api/docs/system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.\n  * [Safety guidance](/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/billing#enable-cloud-billing",
    "title": "Billing  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Billing \n\nThis guide provides an overview of different Gemini API billing options, explains how to enable billing and monitor usage, and provides answers to frequently asked questions (FAQs) about billing.\n\n[Upgrade to the Gemini API paid tier](https://aistudio.google.com/plan_information)\n\n## About billing\n\nBilling for the Gemini API is based on two pricing tiers: _free of charge_ (or _free_) and _pay-as-you-go_ (or _paid_). Pricing and rate limits differ between these tiers and also vary by model. You can check out the [rate limits](/gemini-api/docs/rate-limits) and [pricing](/gemini-api/docs/pricing) pages for more into. For a model-by-model breakdown of capabilities, see the [Gemini models page](/gemini-api/docs/models/gemini).\n\n#### How to request an upgrade\n\nThe Gemini API uses Cloud Billing for all billing services. To transition from the Free tier to a paid tier, you must first enable Cloud Billing for your Google Cloud project.\n\nWhen your project meets the specified criteria, it becomes eligible for an upgrade to the next tier. To request an upgrade, follow these steps:\n\n  1. Go to the [AI Studio API keys page](https://aistudio.google.com/app/apikey).\n  2. Find the project you want to upgrade and click **Upgrade**.\n  3. The system will automatically verify your project's eligibility. This process takes a few seconds.\n  4. If your project meets all the requirements, it will be instantly upgraded to the next tier.\n\n\n\n### Why use the paid tier?\n\nWhen you enable billing and use the paid tier, you benefit from [higher rate limits](/gemini-api/docs/rate-limits), and your prompts and responses aren't used to improve Google products. For more information on data use for paid services, see the [terms of service](/gemini-api/terms#data-use-paid).\n\n### Cloud Billing\n\nThe Gemini API uses [Cloud Billing](https://cloud.google.com/billing/docs/concepts) for billing services. To use the paid tier, you must set up Cloud Billing on your cloud project. After you've enabled Cloud Billing, you can use Cloud Billing tools to track spending, understand costs, make payments, and access Cloud Billing support.\n\n## Enable billing\n\nYou can enable Cloud Billing starting from Google AI Studio:\n\n  1. Open [Google AI Studio](https://aistudio.google.com/).\n\n  2. In the bottom of the left sidebar, select **Settings** > **Plan information**.\n\n  3. Click **Set up Billing** for your chosen project to enable Cloud Billing.\n\n\n\n\n## Monitor usage\n\nAfter you enable Cloud Billing, you can monitor your usage of the Gemini API in the [Google Cloud console](https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com). The service name for the API is `generativelanguage.googleapis.com`, and in the console the Gemini API is also referred to as the **Generative Language API**.\n\nTo learn more, see the Google Cloud documentation on [monitoring API usage](https://cloud.google.com/apis/docs/monitoring).\n\n## Frequently asked questions\n\nThis section provides answers to frequently asked questions.\n\n### What am I billed for?\n\nGemini API pricing is based on the following:\n\n  * Input token count\n  * Output token count\n  * Cached token count\n  * Cached token storage duration\n\n\n\nFor pricing information, see the [pricing page](/pricing).\n\n### Where can I view my quota?\n\nYou can view your quota and system limits in the [Google Cloud console](https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/quotas).\n\n### How do I request more quota?\n\nTo request more quota, follow the instructions at How to request an upgrade.\n\n### Can I use the Gemini API for free in EEA (including EU), the UK, and CH?\n\nYes, we make the free tier and paid tier available in [many regions](/gemini-api/docs/available-regions).\n\n### If I set up billing with the Gemini API, will I be charged for my Google AI Studio usage?\n\nNo, Google AI Studio usage remains free of charge regardless of if you set up billing across all supported regions.\n\n### Can I use 1M tokens in the free tier?\n\nThe free tier for Gemini API differs based on the model selected. For now, you can try the 1M token context window in the following ways:\n\n  * In Google AI Studio\n  * With pay-as-you-go plans\n  * With free-of-charge plans for select models\n\n\n\nSee the latest free-of-charge rate limits per model on [rate limits page](/gemini-api/docs/rate-limits).\n\n### How can I calculate the number of tokens I'm using?\n\nUse the [`GenerativeModel.count_tokens`](/api/python/google/generativeai/GenerativeModel#count_tokens) method to count the number of tokens. Refer to the [Tokens guide](/gemini-api/docs/tokens) to learn more about tokens.\n\n### Can I use my Google Cloud credits with the Gemini API?\n\nYes, Google Cloud credits can be used towards Gemini API usage.\n\n### How is billing handled?\n\nBilling for the Gemini API is handled by the [Cloud Billing](https://cloud.google.com/billing/docs/concepts) system.\n\n### Am I charged for failed requests?\n\nIf your request fails with a 400 or 500 error, you won't be charged for the tokens used. However, the request will still count against your quota.\n\n### Is there a charge for fine-tuning the models?\n\n[Model tuning](/gemini-api/docs/model-tuning) is free, but inference on tuned models is charged at the same rate as the base models.\n\n### Is GetTokens billed?\n\nRequests to the GetTokens API are not billed, and they don't count against inference quota.\n\n### How is my Google AI Studio data handled if I have a paid API account?\n\nRefer to the [terms](/gemini-api/terms#paid-services) for details on how data is handled when Cloud billing is enabled (see \"How Google Uses Your Data\" under \"Paid Services\"). Note that your Google AI Studio prompts are treated under the same \"Paid Services\" terms so long as at least 1 API project has billing enabled, which you can validate on the [Gemini API Key page](https://aistudio.google.com/apikey) if you see any projects marked as \"Paid\" under \"Plan\".\n\n### Where can I get help with billing?\n\nTo get help with billing, see [Get Cloud Billing support](https://cloud.google.com/support/billing).\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-03-25 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-03-25 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/rate-limits#usage-tiers",
    "title": "Rate limits  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Rate limits \n\nRate limits regulate the number of requests you can make to the Gemini API within a given timeframe. These limits help ensure fair usage, protect against abuse, and help maintain system performance for all users.\n\n## How rate limits work\n\nRate limits are measured across three dimensions:\n\n  * Requests per minute (**RPM**)\n  * Requests per day (**RPD**)\n  * Tokens per minute (**TPM**)\n\n\n\nYour usage is evaluated against each limit, and exceeding any of them will trigger a rate limit error. For example, if your RPM limit is 20, making 21 requests within a minute will result in an error, even if you haven't exceeded your TPM or other limits.\n\nRate limits are applied per project, not per API key.\n\nLimits vary depending on the specific model being used, and some limits only apply to specific models. For example, Images per minute, or IPM, is only calculated for models capable of generating images (Imagen 3), but is conceptually similar to TPM.\n\n## Usage tiers\n\nRate limits are tied to the projects usage tier (which we will soon be rolling out). As your API usage and spending increase, you'll have an option to upgrade to a higher tier with increased rate limits.\n\nTier | Qualifications  \n---|---  \nFree | Users in [eligible countries](/gemini-api/docs/available-regions)  \nTier 1 | Billing account [linked to the project](/gemini-api/docs/billing#enable-cloud-billing)  \nTier 2 | Total spend: $250 + at least 30 days since successful payment  \nTier 3 | Total spend: $1,000 + at least 30 days since successful payment  \n  \nWhen you request an upgrade, our automated abuse protection system performs additional checks. While meeting the stated qualification criteria is generally sufficient for approval, in rare cases an upgrade request may be denied based on other factors identified during the review process.\n\nThis system helps ensure the security and integrity of the Gemini API platform for all users.\n\n## Current rate limits\n\n### Free Tier\n\nModel | RPM | TPM | RPD  \n---|---|---|---  \nGemini 2.5 Pro Experimental | 5 | 1,000,000 | 25  \nGemini 2.5 Pro Preview | \\-- | \\-- | \\--  \nGemini 2.0 Flash | 15 | 1,000,000 | 1,500  \nGemini 2.0 Flash Experimental (including image generation) | 10 | 1,000,000 | 1,500  \nGemini 2.0 Flash-Lite | 30 | 1,000,000 | 1,500  \nGemini 2.0 Flash Thinking Experimental 01-21 | 10 | 4,000,000 | 1,500  \nGemini 1.5 Flash | 15 | 1,000,000 | 1,500  \nGemini 1.5 Flash-8B | 15 | 1,000,000 | 1,500  \nGemini 1.5 Pro | 2 | 32,000 | 50  \nVeo 2 | \\-- | \\-- | \\--  \nImagen 3 | \\-- | \\-- | \\--  \nGemma 3 | 30 | 15,000 | 14,400  \nGemini Embedding Experimental 03-07 | 5 | \\-- | 100  \n  \n### Tier 1\n\nModel | RPM | TPM | RPD  \n---|---|---|---  \nGemini 2.5 Pro Experimental | \\-- | \\-- | \\--  \nGemini 2.5 Pro Preview | 150 | 2,000,000 | 1,000  \nGemini 2.0 Flash | 2,000 | 4,000,000 | \\--  \nGemini 2.0 Flash Experimental (including image generation) | 10 | 4,000,000 | \\--  \nGemini 2.0 Flash-Lite | 4,000 | 4,000,000 | \\--  \nGemini 2.0 Flash Thinking Experimental 01-21 | 10 | 4,000,000 | \\--  \nGemini 1.5 Flash | 2,000 | 4,000,000 | \\--  \nGemini 1.5 Flash-8B | 4,000 | 4,000,000 | \\--  \nGemini 1.5 Pro | 1,000 | 4,000,000 | \\--  \nImagen 3 | \\-- | 20 images per minute (IPM) | \\--  \nVeo 2 | 2 | \\-- | 50  \nGemma 3 | 30 | 15,000 | 14,400  \nGemini Embedding Experimental 03-07 | 10 | \\-- | 1,000  \n  \n### Tier 2\n\nModel | RPM | TPM | RPD  \n---|---|---|---  \nGemini 2.5 Pro Experimental | \\-- | \\-- | \\--  \nGemini 2.5 Pro Preview | 1,000 | 5,000,000 | 50,000  \nGemini 2.0 Flash | 10,000 | 10,000,000 | \\--  \nGemini 2.0 Flash Experimental (including image generation) | 10 | 4,000,000 | \\--  \nGemini 2.0 Flash-Lite | 4,000 | 4,000,000 | \\--  \nGemini 2.0 Flash Thinking Experimental 01-21 | 10 | 4,000,000 | \\--  \nGemini 1.5 Flash | 2,000 | 4,000,000 | \\--  \nGemini 1.5 Flash-8B | 4,000 | 4,000,000 | \\--  \nGemini 1.5 Pro | 1,000 | 4,000,000 | \\--  \nImagen 3 | \\-- | 20 images per minute (IPM) | \\--  \nVeo 2 | \\-- | \\-- | \\--  \nGemma 3 | 30 | 15,000 | 14,400  \nGemini Embedding Experimental 03-07 | 10 | \\-- | 1,000  \n  \n### Tier 3\n\nModel | RPM | TPM | RPD  \n---|---|---|---  \nGemini 2.5 Pro Preview | 2,000 | 8,000,000 | \\--  \n  \nSpecified rate limits are not guaranteed and actual capacity may vary.\n\n### Live API rate limits\n\n### Free Tier\n\nNumber of concurrent sessions | TPM  \n---|---  \n3 | 1,000,000  \n  \n### Tier 1\n\nNumber of concurrent sessions | TPM  \n---|---  \n50 | 4,000,000  \n  \n### Tier 2\n\nNumber of concurrent sessions | TPM  \n---|---  \n1000 | 10,000,000  \n  \n### Tier 3\n\nNumber of concurrent sessions | TPM  \n---|---  \nNot yet available | Not yet available  \n  \nSpecified rate limits are not guaranteed and actual capacity may vary.\n\n## How to upgrade to the next tier\n\nThe Gemini API uses Cloud Billing for all billing services. To transition from the Free tier to a paid tier, you must first enable Cloud Billing for your Google Cloud project.\n\nOnce your project meets the specified criteria, it becomes eligible for an upgrade to the next tier. To request an upgrade, follow these steps:\n\n  * Navigate to the [API keys page](https://aistudio.google.com/app/apikey) in AI Studio.\n  * Locate the project you want to upgrade and click \"Upgrade\". The \"Upgrade\" option will only show up for projects that meet [next tier qualifications](/gemini-api/docs/rate-limits#usage-tiers).\n\n\n\nAfter a quick validation, the project will be upgraded to the next tier.\n\n## Request a rate limit increase\n\nEach model variation has an associated rate limit (requests per minute, RPM). For details on those rate limits, see [Gemini models](/models/gemini).\n\n[Request paid tier rate limit increase](https://forms.gle/ETzX94k8jf7iSotH9)\n\nWe offer no guarantees about increasing your rate limit, but we'll do our best to review your request and reach out to you if we're able to accommodate your capacity needs.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/text-generation?lang=python#generate-a-text-stream",
    "title": "Text generation  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Text generation \n\nThe Gemini API can generate text output in response to various inputs, including text, images, video, and audio. This guide shows you how to generate text using text and image inputs. It also covers streaming, chat, and system instructions.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Text input\n\nThe simplest way to generate text using the Gemini API is to provide the model with a single text-only input, as shown in this example:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[\"How does AI work?\"]\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"How does AI work?\",\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    // import packages here\n    \n    func main() {\n      ctx := context.Background()\n      client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n      if err != nil {\n        log.Fatal(err)\n      }\n      defer client.Close()\n    \n      model := client.GenerativeModel(\"gemini-2.0-flash\")\n      resp, err := model.GenerateContent(ctx, genai.Text(\"How does AI work?\"))\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp) // helper function for printing content parts\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"How does AI work?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { text: 'How AI does work?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Image input\n\nThe Gemini API supports multimodal inputs that combine text and media files. The following example shows how to generate text from text and image input:\n\n### Python\n    \n    \n    from PIL import Image\n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    image = Image.open(\"/path/to/organ.png\")\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[image, \"Tell me about this instrument\"]\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import {\n      GoogleGenAI,\n      createUserContent,\n      createPartFromUri,\n    } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const image = await ai.files.upload({\n        file: \"/path/to/organ.png\",\n      });\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: [\n          createUserContent([\n            \"Tell me about this instrument\",\n            createPartFromUri(image.uri, image.mimeType),\n          ]),\n        ],\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-2.0-flash\")\n    \n    imgData, err := os.ReadFile(filepath.Join(testDataDir, \"organ.jpg\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    \n    resp, err := model.GenerateContent(ctx,\n      genai.Text(\"Tell me about this instrument\"),\n      genai.ImageData(\"jpeg\", imgData))\n    if err != nil {\n      log.Fatal(err)\n    }\n    \n    printResponse(resp)\n    \n\n### REST\n    \n    \n    # Use a temporary file to hold the base64 encoded image data\n    TEMP_B64=$(mktemp)\n    trap 'rm -f \"$TEMP_B64\"' EXIT\n    base64 $B64FLAGS $IMG_PATH > \"$TEMP_B64\"\n    \n    # Use a temporary file to hold the JSON payload\n    TEMP_JSON=$(mktemp)\n    trap 'rm -f \"$TEMP_JSON\"' EXIT\n    \n    cat > \"$TEMP_JSON\" << EOF\n    {\n      \"contents\": [\n        {\n          \"parts\": [\n            {\n              \"text\": \"Tell me about this instrument\"\n            },\n            {\n              \"inline_data\": {\n                \"mime_type\": \"image/jpeg\",\n                \"data\": \"$(cat \"$TEMP_B64\")\"\n              }\n            }\n          ]\n        }\n      ]\n    }\n    EOF\n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d \"@$TEMP_JSON\"\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const imageUrl = 'http://image/url';\n      const image = getImageData(imageUrl);\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { image },\n              { text: 'Tell me about this instrument' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n    function getImageData(url) {\n      const blob = UrlFetchApp.fetch(url).getBlob();\n    \n      return {\n        mimeType: blob.getContentType(),\n        data: Utilities.base64Encode(blob.getBytes())\n      };\n    }\n    \n\n## Streaming output\n\nBy default, the model returns a response after completing the entire text generation process. You can achieve faster interactions by using streaming to return instances of [`GenerateContentResponse`](/api/generate-content#v1beta.GenerateContentResponse) as they're generated.\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content_stream(\n        model=\"gemini-2.0-flash\",\n        contents=[\"Explain how AI works\"]\n    )\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContentStream({\n        model: \"gemini-2.0-flash\",\n        contents: \"Explain how AI works\",\n      });\n    \n      for await (const chunk of response) {\n        console.log(chunk.text);\n      }\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    iter := model.GenerateContentStream(ctx, genai.Text(\"Write a story about a magic backpack.\"))\n    for {\n      resp, err := iter.Next()\n      if err == iterator.Done {\n        break\n      }\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp)\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse&key=${GEMINI_API_KEY}\" \\\n      -H 'Content-Type: application/json' \\\n      --no-buffer \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Explain how AI works\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            parts: [\n              { text: 'Explain how AI works' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Multi-turn conversations\n\nThe Gemini SDK lets you collect multiple rounds of questions and responses into a chat. The chat format enables users to step incrementally toward answers and to get help with multipart problems. This SDK implementation of chat provides an interface to keep track of conversation history, but behind the scenes it uses the same [`generateContent`](/api/generate-content#method:-models.generatecontent) method to create the response.\n\nThe following code example shows a basic chat implementation:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    chat = client.chats.create(model=\"gemini-2.0-flash\")\n    \n    response = chat.send_message(\"I have 2 dogs in my house.\")\n    print(response.text)\n    \n    response = chat.send_message(\"How many paws are in my house?\")\n    print(response.text)\n    \n    for message in chat.get_history():\n        print(f'role - {message.role}',end=\": \")\n        print(message.parts[0].text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const chat = ai.chats.create({\n        model: \"gemini-2.0-flash\",\n        history: [\n          {\n            role: \"user\",\n            parts: [{ text: \"Hello\" }],\n          },\n          {\n            role: \"model\",\n            parts: [{ text: \"Great to meet you. What would you like to know?\" }],\n          },\n        ],\n      });\n    \n      const response1 = await chat.sendMessage({\n        message: \"I have 2 dogs in my house.\",\n      });\n      console.log(\"Chat response 1:\", response1.text);\n    \n      const response2 = await chat.sendMessage({\n        message: \"How many paws are in my house?\",\n      });\n      console.log(\"Chat response 2:\", response2.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    cs := model.StartChat()\n    \n    cs.History = []*genai.Content{\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Hello, I have 2 dogs in my house.\"),\n        },\n        Role: \"user\",\n      },\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Great to meet you. What would you like to know?\"),\n        },\n        Role: \"model\",\n      },\n    }\n    \n    res, err := cs.SendMessage(ctx, genai.Text(\"How many paws are in my house?\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    printResponse(res)\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"Hello\"\n              }\n            ]\n          },\n          {\n            \"role\": \"model\",\n            \"parts\": [\n              {\n                \"text\": \"Great to meet you. What would you like to know?\"\n              }\n            ]\n          },\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"I have two dogs in my house. How many paws are in my house?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            role: 'user',\n            parts: [\n              { text: 'Hello' },\n            ],\n          },\n          {\n            role: 'model',\n            parts: [\n              { text: 'Great to meet you. What would you like to know?' },\n            ],\n          },\n          {\n            role: 'user',\n            parts: [\n              { text: 'I have two dogs in my house. How many paws are in my house?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nYou can also use streaming with chat, as shown in the following example:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    chat = client.chats.create(model=\"gemini-2.0-flash\")\n    \n    response = chat.send_message_stream(\"I have 2 dogs in my house.\")\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n    response = chat.send_message_stream(\"How many paws are in my house?\")\n    for chunk in response:\n        print(chunk.text, end=\"\")\n    \n    for message in chat.get_history():\n        print(f'role - {message.role}', end=\": \")\n        print(message.parts[0].text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const chat = ai.chats.create({\n        model: \"gemini-2.0-flash\",\n        history: [\n          {\n            role: \"user\",\n            parts: [{ text: \"Hello\" }],\n          },\n          {\n            role: \"model\",\n            parts: [{ text: \"Great to meet you. What would you like to know?\" }],\n          },\n        ],\n      });\n    \n      const stream1 = await chat.sendMessageStream({\n        message: \"I have 2 dogs in my house.\",\n      });\n      for await (const chunk of stream1) {\n        console.log(chunk.text);\n        console.log(\"_\".repeat(80));\n      }\n    \n      const stream2 = await chat.sendMessageStream({\n        message: \"How many paws are in my house?\",\n      });\n      for await (const chunk of stream2) {\n        console.log(chunk.text);\n        console.log(\"_\".repeat(80));\n      }\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    cs := model.StartChat()\n    \n    cs.History = []*genai.Content{\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Hello, I have 2 dogs in my house.\"),\n        },\n        Role: \"user\",\n      },\n      {\n        Parts: []genai.Part{\n          genai.Text(\"Great to meet you. What would you like to know?\"),\n        },\n        Role: \"model\",\n      },\n    }\n    \n    iter := cs.SendMessageStream(ctx, genai.Text(\"How many paws are in my house?\"))\n    for {\n      resp, err := iter.Next()\n      if err == iterator.Done {\n        break\n      }\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp)\n    }\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?alt=sse&key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"Hello\"\n              }\n            ]\n          },\n          {\n            \"role\": \"model\",\n            \"parts\": [\n              {\n                \"text\": \"Great to meet you. What would you like to know?\"\n              }\n            ]\n          },\n          {\n            \"role\": \"user\",\n            \"parts\": [\n              {\n                \"text\": \"I have two dogs in my house. How many paws are in my house?\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const payload = {\n        contents: [\n          {\n            role: 'user',\n            parts: [\n              { text: 'Hello' },\n            ],\n          },\n          {\n            role: 'model',\n            parts: [\n              { text: 'Great to meet you. What would you like to know?' },\n            ],\n          },\n          {\n            role: 'user',\n            parts: [\n              { text: 'I have two dogs in my house. How many paws are in my house?' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:streamGenerateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\n## Configuration parameters\n\nEvery prompt you send to the model includes parameters that control how the model generates responses. You can configure these parameters, or let the model use the default options.\n\nThe following example shows how to configure model parameters:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=[\"Explain how AI works\"],\n        config=types.GenerateContentConfig(\n            max_output_tokens=500,\n            temperature=0.1\n        )\n    )\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Explain how AI works\",\n        config: {\n          maxOutputTokens: 500,\n          temperature: 0.1,\n        },\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-pro-latest\")\n    model.SetTemperature(0.9)\n    model.SetTopP(0.5)\n    model.SetTopK(20)\n    model.SetMaxOutputTokens(100)\n    model.SystemInstruction = genai.NewUserContent(genai.Text(\"You are Yoda from Star Wars.\"))\n    model.ResponseMIMEType = \"application/json\"\n    resp, err := model.GenerateContent(ctx, genai.Text(\"What is the average size of a swallow?\"))\n    if err != nil {\n      log.Fatal(err)\n    }\n    printResponse(resp)\n    \n\n### REST\n    \n    \n    curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY \\\n      -H 'Content-Type: application/json' \\\n      -X POST \\\n      -d '{\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Explain how AI works\"\n              }\n            ]\n          }\n        ],\n        \"generationConfig\": {\n          \"stopSequences\": [\n            \"Title\"\n          ],\n          \"temperature\": 1.0,\n          \"maxOutputTokens\": 800,\n          \"topP\": 0.8,\n          \"topK\": 10\n        }\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const generationConfig = {\n        temperature: 1,\n        topP: 0.95,\n        topK: 40,\n        maxOutputTokens: 8192,\n        responseMimeType: 'text/plain',\n      };\n    \n      const payload = {\n        generationConfig,\n        contents: [\n          {\n            parts: [\n              { text: 'Explain how AI works in a few words' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nHere are some of the model parameters you can configure. (Naming conventions vary by programming language.)\n\n  * `stopSequences`: Specifies the set of character sequences (up to 5) that will stop output generation. If specified, the API will stop at the first appearance of a `stop_sequence`. The stop sequence won't be included as part of the response.\n  * `temperature`: Controls the randomness of the output. Use higher values for more creative responses, and lower values for more deterministic responses. Values can range from [0.0, 2.0].\n  * `maxOutputTokens`: Sets the maximum number of tokens to include in a candidate.\n  * `topP`: Changes how the model selects tokens for output. Tokens are selected from the most to least probable until the sum of their probabilities equals the `topP` value. The default `topP` value is 0.95.\n  * `topK`: Changes how the model selects tokens for output. A `topK` of 1 means the selected token is the most probable among all the tokens in the model's vocabulary, while a `topK` of 3 means that the next token is selected from among the 3 most probable using the temperature. Tokens are further filtered based on `topP` with the final token selected using temperature sampling.\n\n\n\n## System instructions\n\nSystem instructions let you steer the behavior of a model based on your specific use case. When you provide system instructions, you give the model additional context to help it understand the task and generate more customized responses. The model should adhere to the system instructions over the full interaction with the user, enabling you to specify product-level behavior separate from the prompts provided by end users.\n\nYou can set system instructions when you initialize your model:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=types.GenerateContentConfig(\n            system_instruction=\"You are a cat. Your name is Neko.\"),\n        contents=\"Hello there\"\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Hello there\",\n        config: {\n          systemInstruction: \"You are a cat. Your name is Neko.\",\n        },\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Go\n    \n    \n    // import packages here\n    \n    func main() {\n      ctx := context.Background()\n      client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n      if err != nil {\n        log.Fatal(err)\n      }\n      defer client.Close()\n    \n      model := client.GenerativeModel(\"gemini-2.0-flash\")\n      model.SystemInstruction = &genai.Content{\n        Parts: []genai.Part{genai.Text(`\n          You are a cat. Your name is Neko.\n        `)},\n      }\n      resp, err := model.GenerateContent(ctx, genai.Text(\"Hello there\"))\n      if err != nil {\n        log.Fatal(err)\n      }\n      printResponse(resp) // helper function for printing content parts\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n      -H 'Content-Type: application/json' \\\n      -d '{\n        \"system_instruction\": {\n          \"parts\": [\n            {\n              \"text\": \"You are a cat. Your name is Neko.\"\n            }\n          ]\n        },\n        \"contents\": [\n          {\n            \"parts\": [\n              {\n                \"text\": \"Hello there\"\n              }\n            ]\n          }\n        ]\n      }'\n    \n\n### Apps Script\n    \n    \n    // See https://developers.google.com/apps-script/guides/properties\n    // for instructions on how to set the API key.\n    const apiKey = PropertiesService.getScriptProperties().getProperty('GEMINI_API_KEY');\n    \n    function main() {\n      const systemInstruction = {\n        parts: [{\n          text: 'You are a cat. Your name is Neko.'\n        }]\n      };\n    \n      const payload = {\n        systemInstruction,\n        contents: [\n          {\n            parts: [\n              { text: 'Hello there' },\n            ],\n          },\n        ],\n      };\n    \n      const url = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=${apiKey}`;\n      const options = {\n        method: 'POST',\n        contentType: 'application/json',\n        payload: JSON.stringify(payload)\n      };\n    \n      const response = UrlFetchApp.fetch(url, options);\n      const data = JSON.parse(response);\n      const content = data['candidates'][0]['content']['parts'][0]['text'];\n      console.log(content);\n    }\n    \n\nThen, you can send requests to the model as usual.\n\n## Supported models\n\nThe entire Gemini family of models supports text generation. To learn more about the models and their capabilities, see [Models](/gemini-api/docs/models).\n\n## Prompting tips\n\nFor basic text generation use cases, your prompt might not need to include any output examples, system instructions, or formatting information. This is a [zero-shot](/gemini-api/docs/models/generative-models#zero-shot-prompts) approach. For some use cases, a [one-shot](/gemini-api/docs/models/generative-models#one-shot-prompts) or [few-shot](/gemini-api/docs/models/generative-models#few-shot-prompts) prompt might produce output that's more aligned with user expectations. In some cases, you might also want to provide system instructions to help the model understand the task or follow specific guidelines.\n\n## What's next\n\n  * Try the [Gemini API getting started Colab](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started.ipynb).\n  * Learn how to use Gemini's [vision understanding](/gemini-api/docs/vision) to process images and videos.\n  * Learn how to use Gemini's [audio understanding](/gemini-api/docs/audio) to process audio files.\n  * Learn about multimodal [file prompting strategies](/gemini-api/docs/file-prompting-strategies).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/function-calling/tutorial",
    "title": "Function Calling with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Function Calling with the Gemini API \n\nFunction calling lets you connect models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:\n\n  * **Augment Knowledge:** Access information from external sources like databases, APIs, and knowledge bases.\n  * **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.\n  * **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices\n\n\n\nGet Weather Schedule Meeting Create Chart\n\n## How Function Calling Works\n\nFunction calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:\n\n  1. **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.\n  2. **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\n  3. **Execute Function Code (Your Responsibility):** The Model _does not_ execute the function itself. It's your application's responsibility to process the response and check for Function Call, if \n     * **Yes** : Extract the name and args of the function and execute the corresponding function in your application.\n     * **No:** The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).\n  4. **Create User friendly response:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.\n\n\n\nThis process can be repeated over multiple turns, allowing for complex interactions and workflows. The model also supports calling multiple functions in a single turn ([parallel function calling](/gemini-api/docs/function-calling#parallel_function_calling)) and in sequence ([compositional function calling](/gemini-api/docs/function-calling#compositional_function_calling)).\n\n### Step 1: Define Function Declaration\n\nDefine a function and its declaration within your application code that allows users to set light values and make an API request. This function could call external services or APIs.\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Define a function that the model can call to control smart lights\n    set_light_values_declaration = {\n        \"name\": \"set_light_values\",\n        \"description\": \"Sets the brightness and color temperature of a light.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n                },\n                \"color_temp\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"daylight\", \"cool\", \"warm\"],\n                    \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n                },\n            },\n            \"required\": [\"brightness\", \"color_temp\"],\n        },\n    }\n    \n    # This is the actual function that would be called based on the model's suggestion\n    def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n        \"\"\"Set the brightness and color temperature of a room light. (mock API).\n    \n        Args:\n            brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n            color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    \n        Returns:\n            A dictionary containing the set brightness and color temperature.\n        \"\"\"\n        return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n    \n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    // Define a function that the model can call to control smart lights\n    const setLightValuesFunctionDeclaration = {\n      name: 'set_light_values',\n      description: 'Sets the brightness and color temperature of a light.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',\n          },\n          color_temp: {\n            type: Type.STRING,\n            enum: ['daylight', 'cool', 'warm'],\n            description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',\n          },\n        },\n        required: ['brightness', 'color_temp'],\n      },\n    };\n    \n    /**\n    * Set the brightness and color temperature of a room light. (mock API)\n    * @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness\n    * @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    * @return {Object} A dictionary containing the set brightness and color temperature.\n    */\n    function setLightValues(brightness, color_temp) {\n      return {\n        brightness: brightness,\n        colorTemperature: color_temp\n      };\n    }\n    \n\n### Step 2: Call the model with function declarations\n\nOnce you have defined your function declarations, you can prompt the model to use the function. It analyzes the prompt and function declarations and decides to respond directly or to call a function. If a function is called the response object will contain a function call suggestion.\n\n### Python\n    \n    \n    from google import genai\n    \n    # Generation Config with Function Declaration\n    tools = types.Tool(function_declarations=[set_light_values_declaration])\n    config = types.GenerateContentConfig(tools=[tools])\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Define user prompt\n    contents = [\n        types.Content(\n            role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n        )\n    ]\n    \n    # Send request with function declarations\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\", config=config, contents=contents\n    )\n    \n    print(response.candidates[0].content.parts[0].function_call)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Generation Config with Function Declaration\n    const config = {\n      tools: [{\n        functionDeclarations: [setLightValuesFunctionDeclaration]\n      }]\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Define user prompt\n    const contents = [\n      {\n        role: 'user',\n        parts: [{ text: 'Turn the lights down to a romantic level' }]\n      }\n    ];\n    \n    // Send request with function declarations\n    const response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(response.functionCalls[0]); \n    \n\nThe model then returns a `functionCall` object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.\n\n### Python\n    \n    \n    id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'\n    \n\n### JavaScript\n    \n    \n    {\n      name: 'set_light_values',\n      args: { brightness: 25, color_temp: 'warm' }\n    }\n    \n\n### Step 3: Execute set_light_values function code\n\nExtract the function call details from the model's response, parse the arguments , and execute the `set_light_values` function in our code.\n\n### Python\n    \n    \n    # Extract tool call details\n    tool_call = response.candidates[0].content.parts[0].function_call\n    \n    if tool_call.name == \"set_light_values\":\n        result = set_light_values(**tool_call.args)\n        print(f\"Function execution result: {result}\")\n    \n\n### JavaScript\n    \n    \n    // Extract tool call details\n    const tool_call = response.functionCalls[0]\n    \n    let result;\n    if (tool_call.name === 'set_light_values') {\n      result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);\n      console.log(`Function execution result: ${JSON.stringify(result)}`);\n    }\n    \n\n### Step 4: Create User friendly response with function result and call the model again\n\nFinally, send the result of the function execution back to the model so it can incorporate this information into its final response to the user.\n\n### Python\n    \n    \n    # Create a function response part\n    function_response_part = types.Part.from_function_response(\n        name=tool_call.name,\n        response={\"result\": result},\n    )\n    \n    # Append function call and result of the function execution to contents\n    contents.append(types.Content(role=\"model\", parts=[types.Part(function_call=tool_call)])) # Append the model's function call message\n    contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n    \n    final_response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=config,\n        contents=contents,\n    )\n    \n    print(final_response.text)\n    \n\n### JavaScript\n    \n    \n    // Create a function response part\n    const function_response_part = {\n      name: tool_call.name,\n      response: { result }\n    }\n    \n    // Append function call and result of the function execution to contents\n    contents.push({ role: 'model', parts: [{ functionCall: tool_call }] });\n    contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });\n    \n    // Get the final response from the model\n    const final_response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(final_response.text);\n    \n\nThis completes the function calling flow. The Model successfully used the `set_light_values` function to perform the request action of the user.\n\n## Function declarations\n\nWhen you implement function calling in a prompt, you create a `tools` object, which contains one or more _`function declarations`_. You define functions using JSON, specifically with a [select subset](https://ai.google.dev/api/caching#Schema) of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schemawr) format. A single function declaration can include the following parameters:\n\n  * `name` (string): A unique name for the function (`get_weather_forecast`, `send_email`). Use descriptive names without spaces or special characters (use underscores or camelCase).\n  * `description` (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful (\"Finds theaters based on location and optionally movie title which is currently playing in theaters.\").\n  * `parameters` (object): Defines the input parameters the function expects. \n    * `type` (string): Specifies the overall data type, such as `object`.\n    * `properties` (object): Lists individual parameters, each with: \n      * `type` (string): The data type of the parameter, such as `string`, `integer`, `boolean, array`.\n      * `description` (string): A description of the parameter's purpose and format. Provide examples and constraints (\"The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.\").\n      * `enum` (array, optional): If the parameter values are from a fixed set, use \"enum\" to list the allowed values instead of just describing them in the description. This improves accuracy (\"enum\": [\"daylight\", \"cool\", \"warm\"]).\n    * `required` (array): An array of strings listing the parameter names that are mandatory for the function to operate.\n\n\n\n## Parallel Function Calling\n\nIn addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco.\n\n### Python\n    \n    \n    power_disco_ball = {\n        \"name\": \"power_disco_ball\",\n        \"description\": \"Powers the spinning disco ball.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"power\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether to turn the disco ball on or off.\",\n                }\n            },\n            \"required\": [\"power\"],\n        },\n    }\n    \n    start_music = {\n        \"name\": \"start_music\",\n        \"description\": \"Play some music matching the specified parameters.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"energetic\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is energetic or not.\",\n                },\n                \"loud\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is loud or not.\",\n                },\n            },\n            \"required\": [\"energetic\", \"loud\"],\n        },\n    }\n    \n    dim_lights = {\n        \"name\": \"dim_lights\",\n        \"description\": \"Dim the lights.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"number\",\n                    \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n                }\n            },\n            \"required\": [\"brightness\"],\n        },\n    }\n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    const powerDiscoBall = {\n      name: 'power_disco_ball',\n      description: 'Powers the spinning disco ball.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          power: {\n            type: Type.BOOLEAN,\n            description: 'Whether to turn the disco ball on or off.'\n          }\n        },\n        required: ['power']\n      }\n    };\n    \n    const startMusic = {\n      name: 'start_music',\n      description: 'Play some music matching the specified parameters.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          energetic: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is energetic or not.'\n          },\n          loud: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is loud or not.'\n          }\n        },\n        required: ['energetic', 'loud']\n      }\n    };\n    \n    const dimLights = {\n      name: 'dim_lights',\n      description: 'Dim the lights.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'\n          }\n        },\n        required: ['brightness']\n      }\n    };\n    \n\nCall the model with an instruction that could use all of the specified tools. This example uses a `tool_config`. To learn more you can read about [configuring function calling](/gemini-api/docs/function-calling#function_calling_modes).\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Set up function declarations\n    house_tools = [\n        types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])\n    ]\n    \n    config = {\n        \"tools\": house_tools,\n        \"automatic_function_calling\": {\"disable\": True},\n        # Force the model to call 'any' function, instead of chatting.\n        \"tool_config\": {\"function_calling_config\": {\"mode\": \"any\"}},\n    }\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Turn this place into a party!\")\n    \n    # Print out each of the function calls requested from this single call\n    print(\"Example 1: Forced function calling\")\n    for fn in response.function_calls:\n        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n        print(f\"{fn.name}({args})\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Set up function declarations\n    const houseFns = [powerDiscoBall, startMusic, dimLights];\n    \n    const config = {\n        tools: [{\n            functionDeclarations: houseFns\n        }],\n        // Force the model to call 'any' function, instead of chatting.\n        toolConfig: {\n            functionCallingConfig: {\n            mode: 'any'\n            }\n        }\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Create a chat session\n    const chat = ai.chats.create({\n        model: 'gemini-2.0-flash',\n        config: config\n    });\n    const response = await chat.sendMessage({message: 'Turn this place into a party!'});\n    \n    // Print out each of the function calls requested from this single call\n    console.log(\"Example 1: Forced function calling\");\n    for (const fn of response.functionCalls) {\n        const args = Object.entries(fn.args)\n            .map(([key, val]) => `${key}=${val}`)\n            .join(', ');\n        console.log(`${fn.name}(${args})`);\n    }\n    \n\nEach of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested. \n\nThe Python SDK supports a feature called [automatic function calling](/gemini-api/docs/function-calling#automatic_function_calling_python_only) which converts the Python function to declarations, handles the function call execution and response cycle for you. Following is an example for our disco use case.\n\n**Note:** Automatic Function Calling is a Python SDK only feature at the moment.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Actual implementation functions\n    def power_disco_ball_impl(power: bool) -> dict:\n        \"\"\"Powers the spinning disco ball.\n    \n        Args:\n            power: Whether to turn the disco ball on or off.\n    \n        Returns:\n            A status dictionary indicating the current state.\n        \"\"\"\n        return {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n    \n    def start_music_impl(energetic: bool, loud: bool) -> dict:\n        \"\"\"Play some music matching the specified parameters.\n    \n        Args:\n            energetic: Whether the music is energetic or not.\n            loud: Whether the music is loud or not.\n    \n        Returns:\n            A dictionary containing the music settings.\n        \"\"\"\n        music_type = \"energetic\" if energetic else \"chill\"\n        volume = \"loud\" if loud else \"quiet\"\n        return {\"music_type\": music_type, \"volume\": volume}\n    \n    def dim_lights_impl(brightness: float) -> dict:\n        \"\"\"Dim the lights.\n    \n        Args:\n            brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n    \n        Returns:\n            A dictionary containing the new brightness setting.\n        \"\"\"\n        return {\"brightness\": brightness}\n    \n    config = {\n        \"tools\": [power_disco_ball_impl, start_music_impl, dim_lights_impl],\n    }\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Do everything you need to this place into party!\")\n    \n    print(\"\\nExample 2: Automatic function calling\")\n    print(response.text)\n    # I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!\n    \n\n## Compositional Function Calling\n\nGemini 2.0 supports compositional function calling, meaning the model can chain multiple function calls together. For example, to answer \"Get the temperature in my current location\", the Gemini API might invoke both a `get_current_location()` function and a `get_weather()` function that takes the location as a parameter.\n\n**Note:** Compositional function calling is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Light control schemas\n    turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}\n    turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}\n    \n    prompt = \"\"\"\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n      \"\"\"\n    \n    tools = [\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}\n    ]\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Light control schemas\n    const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };\n    const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };\n    \n    const prompt = `\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n    `;\n    \n    const tools = [\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }\n    ];\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n## Function calling modes\n\nThe Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the `function_calling_config`.\n\n  * `AUTO (Default)`: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n  * `ANY`: The model is constrained to always predict a function call and guarantee function schema adherence. If `allowed_function_names` is not specified, the model can choose from any of the provided function declarations. If `allowed_function_names` is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call in response to every prompt (if applicable).\n  * `NONE`: The model is _prohibited_ from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.\n\n\n\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Configure function calling mode\n    tool_config = types.ToolConfig(\n        function_calling_config=types.FunctionCallingConfig(\n            mode=\"ANY\", allowed_function_names=[\"get_current_temperature\"]\n        )\n    )\n    \n    # Create the generation config\n    config = types.GenerateContentConfig(\n        temperature=0,\n        tools=[tools],  # not defined here.\n        tool_config=tool_config,\n    )\n    \n\n### JavaScript\n    \n    \n    import { FunctionCallingConfigMode } from '@google/genai';\n    \n    // Configure function calling mode\n    const toolConfig = {\n      functionCallingConfig: {\n        mode: FunctionCallingConfigMode.ANY,\n        allowedFunctionNames: ['get_current_temperature']\n      }\n    };\n    \n    // Create the generation config\n    const config = {\n      temperature: 0,\n      tools: tools, // not defined here.\n      toolConfig: toolConfig,\n    };\n    \n\n## Automatic Function Calling (Python Only)\n\nWhen using the Python SDK, you can provide Python functions directly as tools. The SDK automatically converts the Python function to declarations, handles the function call execution and response cycle for you. The Python SDK then automatically:\n\n  1. Detects function call responses from the model.\n  2. Call the corresponding Python function in your code.\n  3. Sends the function response back to the model.\n  4. Returns the model's final text response.\n\n\n\nTo use this, define your function with type hints and a docstring, and then pass the function itself (not a JSON declaration) as a tool:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Define the function with type hints and docstring\n    def get_current_temperature(location: str) -> dict:\n        \"\"\"Gets the current temperature for a given location.\n    \n        Args:\n            location: The city and state, e.g. San Francisco, CA\n    \n        Returns:\n            A dictionary containing the temperature and unit.\n        \"\"\"\n        # ... (implementation) ...\n        return {\"temperature\": 25, \"unit\": \"Celsius\"}\n    \n    # Configure the client and model\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))  # Replace with your actual API key setup\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature]\n    )  # Pass the function itself\n    \n    # Make the request\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=\"What's the temperature in London?\",\n        config=config,\n    )\n    \n    print(response.text)  # The SDK handles the function call and returns the final text\n    \n\nYou can disable automatic function calling with:\n\n### Python\n    \n    \n    # To disable automatic function calling:\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature],\n        automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n    )\n    \n\n### Automatic Function schema declaration\n\nAutomatic schema extraction from Python functions doesn't work in all cases. For example: it doesn't handle cases where you describe the fields of a nested dictionary-object. The API is able to describe any of the following types:\n\n### Python\n    \n    \n    AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])\n    \n\nTo see what the inferred schema looks like, you can convert it using [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable):\n\n### Python\n    \n    \n    def multiply(a: float, b: float):\n        \"\"\"Returns a * b.\"\"\"\n        return a * b\n    \n    fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)\n    \n    # to_json_dict() provides a clean JSON representation.\n    print(fn_decl.to_json_dict())\n    \n\n## Multi-tool use: Combine Native Tools with Function Calling\n\nWith Gemini 2.0, you can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, [Grounding with Google Search](/gemini-api/docs/grounding) and [code execution](/gemini-api/docs/code-execution), in a request using the [Live API](/gemini-api/docs/live).\n\n**Note:** Multi-tool use is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Multiple tasks example - combining lights, code execution, and search\n    prompt = \"\"\"\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n      \"\"\"\n    \n    tools = [\n        {'google_search': {}},\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.\n    ]\n    \n    # Execute the prompt with specified tools in audio modality\n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Multiple tasks example - combining lights, code execution, and search\n    const prompt = `\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n    `;\n    \n    const tools = [\n      { googleSearch: {} },\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.\n    ];\n    \n    // Execute the prompt with specified tools in audio modality\n    await run(prompt, {tools: tools, modality: \"AUDIO\"});\n    \n\nPython developers can try this out in the [Live API Tool Use notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).\n\n## Use Model Context Protocol (MCP)\n\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard to connect AI applications with external tools, data sources, and systems. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts. You can use models with MCP server using their tool calling capabilities.\n\nMCP servers expose the tools as JSON schema definitions, which can be used with Gemini compatible function declarations. This lets you to use a MCP server with Gemini models directly. Here, you can find an example of how to use a local MCP server with Gemini SDK and the `mcp` SDK.\n\n### Python\n    \n    \n    import asyncio\n    import os\n    from datetime import datetime\n    from google import genai\n    from google.genai import types\n    from mcp import ClientSession, StdioServerParameters\n    from mcp.client.stdio import stdio_client\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Create server parameters for stdio connection\n    server_params = StdioServerParameters(\n        command=\"npx\",  # Executable\n        args=[\"-y\", \"@philschmid/weather-mcp\"],  # Weather MCP Server\n        env=None,  # Optional environment variables\n    )\n    \n    async def run():\n        async with stdio_client(server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                # Prompt to get the weather for the current day in London.\n                prompt = f\"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?\"\n                # Initialize the connection between client and server\n                await session.initialize()\n    \n                # Get tools from MCP session and convert to Gemini Tool objects\n                mcp_tools = await session.list_tools()\n                tools = [\n                    types.Tool(\n                        function_declarations=[\n                            {\n                                \"name\": tool.name,\n                                \"description\": tool.description,\n                                \"parameters\": {\n                                    k: v\n                                    for k, v in tool.inputSchema.items()\n                                    if k not in [\"additionalProperties\", \"$schema\"]\n                                },\n                            }\n                        ]\n                    )\n                    for tool in mcp_tools.tools\n                ]\n    \n                # Send request to the model with MCP function declarations\n                response = client.models.generate_content(\n                    model=\"gemini-2.0-flash\",\n                    contents=prompt,\n                    config=types.GenerateContentConfig(\n                        temperature=0,\n                        tools=tools,\n                    ),\n                )\n    \n                # Check for a function call\n                if response.candidates[0].content.parts[0].function_call:\n                    function_call = response.candidates[0].content.parts[0].function_call\n                    print(function_call)\n                    # Call the MCP server with the predicted tool\n                    result = await session.call_tool(\n                        function_call.name, arguments=function_call.args\n                    )\n                    print(result.content[0].text)\n                    # Continue as shown in step 4 of \"How Function Calling Works\"\n                    # and create a user friendly response\n                else:\n                    print(\"No function call found in the response.\")\n                    print(response.text)\n    \n    # Start the asyncio event loop and run the main function\n    asyncio.run(run())\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    import { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\n    import { StdioClientTransport } from \"@modelcontextprotocol/sdk/client/stdio.js\";\n    \n    // Create server parameters for stdio connection\n    const serverParams = new StdioClientTransport({\n      command: \"npx\",\n      args: [\"-y\", \"@philschmid/weather-mcp\"]\n    });\n    \n    const client = new Client(\n      {\n        name: \"example-client\",\n        version: \"1.0.0\"\n      }\n    );\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Initialize the connection between client and server\n    await client.connect(serverParams);\n    \n    // Get tools from MCP session and convert to Gemini Tool objects\n    const mcpTools = await client.listTools();\n    const tools = mcpTools.tools.map((tool) => {\n      // Filter the parameters to exclude not supported keys\n      const parameters = Object.fromEntries(\n        Object.entries(tool.inputSchema).filter(([key]) => ![\"additionalProperties\", \"$schema\"].includes(key))\n      );\n      return {\n        name: tool.name,\n        description: tool.description,\n        parameters: parameters\n      };\n    });\n    \n    // Send request to the model with MCP function declarations\n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: \"What is the weather in London in the UK on 2024-04-04?\",\n      config: {\n        tools: [{\n          functionDeclarations: tools\n        }],\n      },\n    });\n    \n    // Check for function calls in the response\n    if (response.functionCalls && response.functionCalls.length > 0) {\n      const functionCall = response.functionCalls[0]; // Assuming one function call\n      console.log(`Function to call: ${functionCall.name}`);\n      console.log(`Arguments: ${JSON.stringify(functionCall.args)}`);\n      // Call the MCP server with the predicted tool\n      const result = await client.callTool({name: functionCall.name, arguments: functionCall.args});\n      console.log(result.content[0].text);\n      // Continue as shown in step 4 of \"How Function Calling Works\"\n      // and create a user friendly response\n    } else {\n      console.log(\"No function call found in the response.\");\n      console.log(response.text);\n    }\n    \n    // Close the connection\n    await client.close();\n    \n\n## Supported Models\n\nExperimental models are not included. You can find their capabilities on the [model overview](https://ai.google.dev/gemini-api/docs/models) page.\n\nModel | Function Calling | Parallel Function Calling | Compositional Function Calling(Live API only)  \n---|---|---|---  \nGemini 2.0 Flash | ✔️ | ✔️ | ✔️  \nGemini 2.0 Flash-Lite | X | X | X  \nGemini 1.5 Flash | ✔️ | ✔️ | ✔️  \nGemini 1.5 Pro | ✔️ | ✔️ | ✔️  \n  \n## Best Practices\n\n  * **Function and Parameter Descriptions:** Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.\n  * **Naming:** Use descriptive function names (without spaces, periods, or dashes).\n  * **Strong Typing:** Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.\n  * **Tool Selection:** While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.\n  * **Prompt Engineering:**\n    * Provide context: Tell the model its role (e.g., \"You are a helpful weather assistant.\").\n    * Give instructions: Specify how and when to use functions (e.g., \"Don't guess dates; always use a future date for forecasts.\").\n    * Encourage clarification: Instruct the model to ask clarifying questions if needed.\n  * **Temperature:** Use a low temperature (e.g., 0) for more deterministic and reliable function calls.\n  * **Validation:** If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.\n  * **Error Handling** : Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.\n  * **Security:** Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.\n  * **Token Limits:** Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.\n\n\n\n## Notes and Limitations\n\n  * Only a [subset of the OpenAPI schema](https://ai.google.dev/api/caching#FunctionDeclaration) is supported.\n  * Supported parameter types in Python are limited.\n  * Automatic function calling is a Python SDK feature only.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-07 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-07 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/image-generation#gemini",
    "title": "Generate images  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Generate images \n\nThe Gemini API supports image generation using Gemini 2.0 Flash Experimental and using Imagen 3. This guide helps you get started with both models.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Generate images using Gemini\n\nGemini 2.0 Flash Experimental supports the ability to output text and inline images. This lets you use Gemini to conversationally edit images or generate outputs with interwoven text (for example, generating a blog post with text and images in a single turn). All generated images include a [SynthID watermark](/responsible/docs/safeguards/synthid), and images in Google AI Studio include a visible watermark as well.\n\n**Note:** Make sure to include `responseModalities`: [\"Text\", \"Image\"] in your generation configuration for text and image output with `gemini-2.0-flash-exp-image-generation`. Image only is not allowed.\n\nThe following example shows how to use Gemini 2.0 to generate text-and-image output:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    from PIL import Image\n    from io import BytesIO\n    import base64\n    \n    client = genai.Client()\n    \n    contents = ('Hi, can you create a 3d rendered image of a pig '\n                'with wings and a top hat flying over a happy '\n                'futuristic scifi city with lots of greenery?')\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash-exp-image-generation\",\n        contents=contents,\n        config=types.GenerateContentConfig(\n          response_modalities=['Text', 'Image']\n        )\n    )\n    \n    for part in response.candidates[0].content.parts:\n      if part.text is not None:\n        print(part.text)\n      elif part.inline_data is not None:\n        image = Image.open(BytesIO((part.inline_data.data)))\n        image.save('gemini-native-image.png')\n        image.show()\n    \n\n### JavaScript\n\n**Note:** We've released the [Google SDK for TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai) in [preview launch stage](https://github.com/googleapis/js-genai?tab=readme-ov-file#preview-launch). Use this SDK for image generation features.\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    import * as fs from \"node:fs\";\n    \n    async function main() {\n    \n      const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n      const contents =\n        \"Hi, can you create a 3d rendered image of a pig \" +\n        \"with wings and a top hat flying over a happy \" +\n        \"futuristic scifi city with lots of greenery?\";\n    \n      // Set responseModalities to include \"Image\" so the model can generate  an image\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash-exp-image-generation\",\n        contents: contents,\n        config: {\n          responseModalities: [\"Text\", \"Image\"],\n        },\n      });\n      for (const part of response.candidates[0].content.parts) {\n        // Based on the part type, either show the text or save the image\n        if (part.text) {\n          console.log(part.text);\n        } else if (part.inlineData) {\n          const imageData = part.inlineData.data;\n          const buffer = Buffer.from(imageData, \"base64\");\n          fs.writeFileSync(\"gemini-native-image.png\", buffer);\n          console.log(\"Image saved as gemini-native-image.png\");\n        }\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl -s -X POST \\\n      \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\n        \"contents\": [{\n          \"parts\": [\n            {\"text\": \"Hi, can you create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?\"}\n          ]\n        }],\n        \"generationConfig\":{\"responseModalities\":[\"Text\",\"Image\"]}\n      }' \\\n      | grep -o '\"data\": \"[^\"]*\"' \\\n      | cut -d'\"' -f4 \\\n      | base64 --decode > gemini-native-image.png\n    \n\nAI-generated image of a fantastical flying pig\n\nDepending on the prompt and context, Gemini will generate content in different modes (text to image, text to image and text, etc.). Here are some examples:\n\n  * Text to image \n    * **Example prompt:** \"Generate an image of the Eiffel tower with fireworks in the background.\"\n  * Text to image(s) and text (interleaved) \n    * **Example prompt:** \"Generate an illustrated recipe for a paella.\"\n  * Image(s) and text to image(s) and text (interleaved) \n    * **Example prompt:** (With an image of a furnished room) \"What other color sofas would work in my space? can you update the image?\"\n  * Image editing (text and image to image) \n    * **Example prompt:** \"Edit this image to make it look like a cartoon\"\n    * **Example prompt:** [image of a cat] + [image of a pillow] + \"Create a cross stitch of my cat on this pillow.\"\n  * Multi-turn image editing (chat) \n    * **Example prompts:** [upload an image of a blue car.] \"Turn this car into a convertible.\" \"Now change the color to yellow.\"\n\n\n\n### Image editing with Gemini\n\nTo perform image editing, add an image as input. The following example demonstrats uploading base64 encoded images. For multiple images and larger payloads, check the [image input](/gemini-api/docs/vision#image-input) section.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    from PIL import Image\n    from io import BytesIO\n    \n    import PIL.Image\n    \n    image = PIL.Image.open('/path/to/image.png')\n    \n    client = genai.Client()\n    \n    text_input = ('Hi, This is a picture of me.'\n                'Can you add a llama next to me?',)\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash-exp-image-generation\",\n        contents=[text_input, image],\n        config=types.GenerateContentConfig(\n          response_modalities=['Text', 'Image']\n        )\n    )\n    \n    for part in response.candidates[0].content.parts:\n      if part.text is not None:\n        print(part.text)\n      elif part.inline_data is not None:\n        image = Image.open(BytesIO(part.inline_data.data))\n        image.show()\n    \n\n### JavaScript\n\n**Note:** We've released the [Google SDK for TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai) in [preview launch stage](https://github.com/googleapis/js-genai?tab=readme-ov-file#preview-launch). Use this SDK for image generation features.\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    import * as fs from \"node:fs\";\n    \n    async function main() {\n    \n      const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n      // Load the image from the local file system\n      const imagePath = \"path/to/image.png\";\n      const imageData = fs.readFileSync(imagePath);\n      const base64Image = imageData.toString(\"base64\");\n    \n      // Prepare the content parts\n      const contents = [\n        { text: \"Can you add a llama next to the image?\" },\n        {\n          inlineData: {\n            mimeType: \"image/png\",\n            data: base64Image,\n          },\n        },\n      ];\n    \n      // Set responseModalities to include \"Image\" so the model can generate an image\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash-exp-image-generation\",\n        contents: contents,\n        config: {\n          responseModalities: [\"Text\", \"Image\"],\n        },\n      });\n      for (const part of response.candidates[0].content.parts) {\n        // Based on the part type, either show the text or save the image\n        if (part.text) {\n          console.log(part.text);\n        } else if (part.inlineData) {\n          const imageData = part.inlineData.data;\n          const buffer = Buffer.from(imageData, \"base64\");\n          fs.writeFileSync(\"gemini-native-image.png\", buffer);\n          console.log(\"Image saved as gemini-native-image.png\");\n        }\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    IMG_PATH=/path/to/your/image1.jpeg\n    \n    if [[ \"$(base64 --version 2>&1)\" = *\"FreeBSD\"* ]]; then\n      B64FLAGS=\"--input\"\n    else\n      B64FLAGS=\"-w0\"\n    fi\n    \n    IMG_BASE64=$(base64 \"$B64FLAGS\" \"$IMG_PATH\" 2>&1)\n    \n    curl -X POST \\\n      \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY\" \\\n        -H 'Content-Type: application/json' \\\n        -d \"{\n          \\\"contents\\\": [{\n            \\\"parts\\\":[\n                {\\\"text\\\": \\\"'Hi, This is a picture of me. Can you add a llama next to me\\\"},\n                {\n                  \\\"inline_data\\\": {\n                    \\\"mime_type\\\":\\\"image/jpeg\\\",\n                    \\\"data\\\": \\\"$IMG_BASE64\\\"\n                  }\n                }\n            ]\n          }],\n          \\\"generationConfig\\\": {\\\"responseModalities\\\": [\\\"Text\\\", \\\"Image\\\"]}\n        }\"  \\\n      | grep -o '\"data\": \"[^\"]*\"' \\\n      | cut -d'\"' -f4 \\\n      | base64 --decode > gemini-edited-image.png\n    \n\n### Limitations\n\n  * For best performance, use the following languages: EN, es-MX, ja-JP, zh-CN, hi-IN.\n  * Image generation does not support audio or video inputs.\n  * Image generation may not always trigger: \n    * The model may output text only. Try asking for image outputs explicitly (e.g. \"generate an image\", \"provide images as you go along\", \"update the image\").\n    * The model may stop generating partway through. Try again or try a different prompt.\n  * When generating text for an image, Gemini works best if you first generate the text and then ask for an image with the text.\n\n\n\n## Choose a model\n\nWhich model should you use to generate images? It depends on your use case.\n\nGemini 2.0 is best for producing contextually relevant images, blending text + images, incorporating world knowledge, and reasoning about images. You can use it to create accurate, contextually relevant visuals embedded in long text sequences. You can also edit images conversationally, using natural language, while maintaining context throughout the conversation.\n\nIf image quality is your top priority, then Imagen 3 is a better choice. Imagen 3 excels at photorealism, artistic detail, and specific artistic styles like impressionism or anime. Imagen 3 is also a good choice for specialized image editing tasks like updating product backgrounds, upscaling images, and infusing branding and style into visuals. You can use Imagen 3 to create logos or other branded product designs.\n\n## Generate images using Imagen 3\n\nThe Gemini API provides access to [Imagen 3](https://deepmind.google/technologies/imagen-3/), Google's highest quality text-to-image model, featuring a number of new and improved capabilities. Imagen 3 can do the following:\n\n  * Generate images with better detail, richer lighting, and fewer distracting artifacts than previous models\n  * Understand prompts written in natural language\n  * Generate images in a wide range of formats and styles\n  * Render text more effectively than previous models\n\n**Note:** Imagen 3 is only available on the [Paid Tier](/gemini-api/docs/pricing) and always includes a [SynthID](https://deepmind.google/technologies/synthid/) watermark.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    from PIL import Image\n    from io import BytesIO\n    \n    client = genai.Client(api_key='GEMINI_API_KEY')\n    \n    response = client.models.generate_images(\n        model='imagen-3.0-generate-002',\n        prompt='Robot holding a red skateboard',\n        config=types.GenerateImagesConfig(\n            number_of_images= 4,\n        )\n    )\n    for generated_image in response.generated_images:\n      image = Image.open(BytesIO(generated_image.image.image_bytes))\n      image.show()\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    import * as fs from \"node:fs\";\n    \n    async function main() {\n    \n      const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n      const response = await ai.models.generateImages({\n        model: 'imagen-3.0-generate-002',\n        prompt: 'Robot holding a red skateboard',\n        config: {\n          numberOfImages: 4,\n        },\n      });\n    \n      let idx = 1;\n      for (const generatedImage of response.generatedImages) {\n        let imgBytes = generatedImage.image.imageBytes;\n        const buffer = Buffer.from(imgBytes, \"base64\");\n        fs.writeFileSync(`imagen-${idx}.png`, buffer);\n        idx++;\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl -X POST \\\n        \"https://generativelanguage.googleapis.com/v1beta/models/imagen-3.0-generate-002:predict?key=GEMINI_API_KEY\" \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\n            \"instances\": [\n              {\n                \"prompt\": \"Robot holding a red skateboard\"\n              }\n            ],\n            \"parameters\": {\n              \"sampleCount\": 4\n            }\n          }'\n    \n\nAI-generated image of two fuzzy bunnies in the kitchen\n\nImagen supports English only prompts at this time and the following parameters:\n\n### Imagen model parameters\n\n(Naming conventions vary by programming language.)\n\n  * `numberOfImages`: The number of images to generate, from 1 to 4 (inclusive). The default is 4.\n  * `aspectRatio`: Changes the aspect ratio of the generated image. Supported values are `\"1:1\"`, `\"3:4\"`, `\"4:3\"`, `\"9:16\"`, and `\"16:9\"`. The default is `\"1:1\"`.\n  * `personGeneration`: Allow the model to generate images of people. The following values are supported: \n    * `\"DONT_ALLOW\"`: Block generation of images of people.\n    * `\"ALLOW_ADULT\"`: Generate images of adults, but not children. This is the default.\n\n\n\n## What's next\n\n  * To learn more about prompt writing for Imagen, see the [Imagen prompt guide](/gemini-api/docs/imagen-prompt-guide).\n  * To learn more about Gemini 2.0 models, see [Gemini models](/gemini-api/docs/models/gemini) and [Experimental models](/gemini-api/docs/models/experimental-models).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/vision#youtube",
    "title": "Explore vision capabilities with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Explore vision capabilities with the Gemini API \n\nPython Node.js Go REST\n\nGemini models are able to process images and videos, enabling many frontier developer use cases that would have historically required domain specific models. Some of Gemini's vision capabilities include the ability to:\n\n  * Caption and answer questions about images\n  * Transcribe and reason over PDFs, including up to 2 million tokens\n  * Describe, segment, and extract information from videos up to 90 minutes long\n  * Detect objects in an image and return bounding box coordinates for them\n\n\n\nGemini was built to be multimodal from the ground up and we continue to push the frontier of what is possible.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## What's next\n\nThis guide shows how to upload image and video files using the File API and then generate text outputs from image and video inputs. To learn more, see the following resources:\n\n  * [File prompting strategies](/gemini-api/docs/file-prompting-strategies): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.\n  * [System instructions](/gemini-api/docs/system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.\n  * [Safety guidance](/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/vision#inline-video",
    "title": "Explore vision capabilities with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Explore vision capabilities with the Gemini API \n\nPython Node.js Go REST\n\nGemini models are able to process images and videos, enabling many frontier developer use cases that would have historically required domain specific models. Some of Gemini's vision capabilities include the ability to:\n\n  * Caption and answer questions about images\n  * Transcribe and reason over PDFs, including up to 2 million tokens\n  * Describe, segment, and extract information from videos up to 90 minutes long\n  * Detect objects in an image and return bounding box coordinates for them\n\n\n\nGemini was built to be multimodal from the ground up and we continue to push the frontier of what is possible.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## What's next\n\nThis guide shows how to upload image and video files using the File API and then generate text outputs from image and video inputs. To learn more, see the following resources:\n\n  * [File prompting strategies](/gemini-api/docs/file-prompting-strategies): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.\n  * [System instructions](/gemini-api/docs/system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.\n  * [Safety guidance](/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/imagen",
    "title": "Generate images  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Generate images \n\nThe Gemini API supports image generation using Gemini 2.0 Flash Experimental and using Imagen 3. This guide helps you get started with both models.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Generate images using Gemini\n\nGemini 2.0 Flash Experimental supports the ability to output text and inline images. This lets you use Gemini to conversationally edit images or generate outputs with interwoven text (for example, generating a blog post with text and images in a single turn). All generated images include a [SynthID watermark](/responsible/docs/safeguards/synthid), and images in Google AI Studio include a visible watermark as well.\n\n**Note:** Make sure to include `responseModalities`: [\"Text\", \"Image\"] in your generation configuration for text and image output with `gemini-2.0-flash-exp-image-generation`. Image only is not allowed.\n\nThe following example shows how to use Gemini 2.0 to generate text-and-image output:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    from PIL import Image\n    from io import BytesIO\n    import base64\n    \n    client = genai.Client()\n    \n    contents = ('Hi, can you create a 3d rendered image of a pig '\n                'with wings and a top hat flying over a happy '\n                'futuristic scifi city with lots of greenery?')\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash-exp-image-generation\",\n        contents=contents,\n        config=types.GenerateContentConfig(\n          response_modalities=['Text', 'Image']\n        )\n    )\n    \n    for part in response.candidates[0].content.parts:\n      if part.text is not None:\n        print(part.text)\n      elif part.inline_data is not None:\n        image = Image.open(BytesIO((part.inline_data.data)))\n        image.save('gemini-native-image.png')\n        image.show()\n    \n\n### JavaScript\n\n**Note:** We've released the [Google SDK for TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai) in [preview launch stage](https://github.com/googleapis/js-genai?tab=readme-ov-file#preview-launch). Use this SDK for image generation features.\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    import * as fs from \"node:fs\";\n    \n    async function main() {\n    \n      const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n      const contents =\n        \"Hi, can you create a 3d rendered image of a pig \" +\n        \"with wings and a top hat flying over a happy \" +\n        \"futuristic scifi city with lots of greenery?\";\n    \n      // Set responseModalities to include \"Image\" so the model can generate  an image\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash-exp-image-generation\",\n        contents: contents,\n        config: {\n          responseModalities: [\"Text\", \"Image\"],\n        },\n      });\n      for (const part of response.candidates[0].content.parts) {\n        // Based on the part type, either show the text or save the image\n        if (part.text) {\n          console.log(part.text);\n        } else if (part.inlineData) {\n          const imageData = part.inlineData.data;\n          const buffer = Buffer.from(imageData, \"base64\");\n          fs.writeFileSync(\"gemini-native-image.png\", buffer);\n          console.log(\"Image saved as gemini-native-image.png\");\n        }\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl -s -X POST \\\n      \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY\" \\\n      -H \"Content-Type: application/json\" \\\n      -d '{\n        \"contents\": [{\n          \"parts\": [\n            {\"text\": \"Hi, can you create a 3d rendered image of a pig with wings and a top hat flying over a happy futuristic scifi city with lots of greenery?\"}\n          ]\n        }],\n        \"generationConfig\":{\"responseModalities\":[\"Text\",\"Image\"]}\n      }' \\\n      | grep -o '\"data\": \"[^\"]*\"' \\\n      | cut -d'\"' -f4 \\\n      | base64 --decode > gemini-native-image.png\n    \n\nAI-generated image of a fantastical flying pig\n\nDepending on the prompt and context, Gemini will generate content in different modes (text to image, text to image and text, etc.). Here are some examples:\n\n  * Text to image \n    * **Example prompt:** \"Generate an image of the Eiffel tower with fireworks in the background.\"\n  * Text to image(s) and text (interleaved) \n    * **Example prompt:** \"Generate an illustrated recipe for a paella.\"\n  * Image(s) and text to image(s) and text (interleaved) \n    * **Example prompt:** (With an image of a furnished room) \"What other color sofas would work in my space? can you update the image?\"\n  * Image editing (text and image to image) \n    * **Example prompt:** \"Edit this image to make it look like a cartoon\"\n    * **Example prompt:** [image of a cat] + [image of a pillow] + \"Create a cross stitch of my cat on this pillow.\"\n  * Multi-turn image editing (chat) \n    * **Example prompts:** [upload an image of a blue car.] \"Turn this car into a convertible.\" \"Now change the color to yellow.\"\n\n\n\n### Image editing with Gemini\n\nTo perform image editing, add an image as input. The following example demonstrats uploading base64 encoded images. For multiple images and larger payloads, check the [image input](/gemini-api/docs/vision#image-input) section.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    from PIL import Image\n    from io import BytesIO\n    \n    import PIL.Image\n    \n    image = PIL.Image.open('/path/to/image.png')\n    \n    client = genai.Client()\n    \n    text_input = ('Hi, This is a picture of me.'\n                'Can you add a llama next to me?',)\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash-exp-image-generation\",\n        contents=[text_input, image],\n        config=types.GenerateContentConfig(\n          response_modalities=['Text', 'Image']\n        )\n    )\n    \n    for part in response.candidates[0].content.parts:\n      if part.text is not None:\n        print(part.text)\n      elif part.inline_data is not None:\n        image = Image.open(BytesIO(part.inline_data.data))\n        image.show()\n    \n\n### JavaScript\n\n**Note:** We've released the [Google SDK for TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai) in [preview launch stage](https://github.com/googleapis/js-genai?tab=readme-ov-file#preview-launch). Use this SDK for image generation features.\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    import * as fs from \"node:fs\";\n    \n    async function main() {\n    \n      const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n      // Load the image from the local file system\n      const imagePath = \"path/to/image.png\";\n      const imageData = fs.readFileSync(imagePath);\n      const base64Image = imageData.toString(\"base64\");\n    \n      // Prepare the content parts\n      const contents = [\n        { text: \"Can you add a llama next to the image?\" },\n        {\n          inlineData: {\n            mimeType: \"image/png\",\n            data: base64Image,\n          },\n        },\n      ];\n    \n      // Set responseModalities to include \"Image\" so the model can generate an image\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash-exp-image-generation\",\n        contents: contents,\n        config: {\n          responseModalities: [\"Text\", \"Image\"],\n        },\n      });\n      for (const part of response.candidates[0].content.parts) {\n        // Based on the part type, either show the text or save the image\n        if (part.text) {\n          console.log(part.text);\n        } else if (part.inlineData) {\n          const imageData = part.inlineData.data;\n          const buffer = Buffer.from(imageData, \"base64\");\n          fs.writeFileSync(\"gemini-native-image.png\", buffer);\n          console.log(\"Image saved as gemini-native-image.png\");\n        }\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    IMG_PATH=/path/to/your/image1.jpeg\n    \n    if [[ \"$(base64 --version 2>&1)\" = *\"FreeBSD\"* ]]; then\n      B64FLAGS=\"--input\"\n    else\n      B64FLAGS=\"-w0\"\n    fi\n    \n    IMG_BASE64=$(base64 \"$B64FLAGS\" \"$IMG_PATH\" 2>&1)\n    \n    curl -X POST \\\n      \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash-exp-image-generation:generateContent?key=$GEMINI_API_KEY\" \\\n        -H 'Content-Type: application/json' \\\n        -d \"{\n          \\\"contents\\\": [{\n            \\\"parts\\\":[\n                {\\\"text\\\": \\\"'Hi, This is a picture of me. Can you add a llama next to me\\\"},\n                {\n                  \\\"inline_data\\\": {\n                    \\\"mime_type\\\":\\\"image/jpeg\\\",\n                    \\\"data\\\": \\\"$IMG_BASE64\\\"\n                  }\n                }\n            ]\n          }],\n          \\\"generationConfig\\\": {\\\"responseModalities\\\": [\\\"Text\\\", \\\"Image\\\"]}\n        }\"  \\\n      | grep -o '\"data\": \"[^\"]*\"' \\\n      | cut -d'\"' -f4 \\\n      | base64 --decode > gemini-edited-image.png\n    \n\n### Limitations\n\n  * For best performance, use the following languages: EN, es-MX, ja-JP, zh-CN, hi-IN.\n  * Image generation does not support audio or video inputs.\n  * Image generation may not always trigger: \n    * The model may output text only. Try asking for image outputs explicitly (e.g. \"generate an image\", \"provide images as you go along\", \"update the image\").\n    * The model may stop generating partway through. Try again or try a different prompt.\n  * When generating text for an image, Gemini works best if you first generate the text and then ask for an image with the text.\n\n\n\n## Choose a model\n\nWhich model should you use to generate images? It depends on your use case.\n\nGemini 2.0 is best for producing contextually relevant images, blending text + images, incorporating world knowledge, and reasoning about images. You can use it to create accurate, contextually relevant visuals embedded in long text sequences. You can also edit images conversationally, using natural language, while maintaining context throughout the conversation.\n\nIf image quality is your top priority, then Imagen 3 is a better choice. Imagen 3 excels at photorealism, artistic detail, and specific artistic styles like impressionism or anime. Imagen 3 is also a good choice for specialized image editing tasks like updating product backgrounds, upscaling images, and infusing branding and style into visuals. You can use Imagen 3 to create logos or other branded product designs.\n\n## Generate images using Imagen 3\n\nThe Gemini API provides access to [Imagen 3](https://deepmind.google/technologies/imagen-3/), Google's highest quality text-to-image model, featuring a number of new and improved capabilities. Imagen 3 can do the following:\n\n  * Generate images with better detail, richer lighting, and fewer distracting artifacts than previous models\n  * Understand prompts written in natural language\n  * Generate images in a wide range of formats and styles\n  * Render text more effectively than previous models\n\n**Note:** Imagen 3 is only available on the [Paid Tier](/gemini-api/docs/pricing) and always includes a [SynthID](https://deepmind.google/technologies/synthid/) watermark.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    from PIL import Image\n    from io import BytesIO\n    \n    client = genai.Client(api_key='GEMINI_API_KEY')\n    \n    response = client.models.generate_images(\n        model='imagen-3.0-generate-002',\n        prompt='Robot holding a red skateboard',\n        config=types.GenerateImagesConfig(\n            number_of_images= 4,\n        )\n    )\n    for generated_image in response.generated_images:\n      image = Image.open(BytesIO(generated_image.image.image_bytes))\n      image.show()\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    import * as fs from \"node:fs\";\n    \n    async function main() {\n    \n      const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n      const response = await ai.models.generateImages({\n        model: 'imagen-3.0-generate-002',\n        prompt: 'Robot holding a red skateboard',\n        config: {\n          numberOfImages: 4,\n        },\n      });\n    \n      let idx = 1;\n      for (const generatedImage of response.generatedImages) {\n        let imgBytes = generatedImage.image.imageBytes;\n        const buffer = Buffer.from(imgBytes, \"base64\");\n        fs.writeFileSync(`imagen-${idx}.png`, buffer);\n        idx++;\n      }\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl -X POST \\\n        \"https://generativelanguage.googleapis.com/v1beta/models/imagen-3.0-generate-002:predict?key=GEMINI_API_KEY\" \\\n        -H \"Content-Type: application/json\" \\\n        -d '{\n            \"instances\": [\n              {\n                \"prompt\": \"Robot holding a red skateboard\"\n              }\n            ],\n            \"parameters\": {\n              \"sampleCount\": 4\n            }\n          }'\n    \n\nAI-generated image of two fuzzy bunnies in the kitchen\n\nImagen supports English only prompts at this time and the following parameters:\n\n### Imagen model parameters\n\n(Naming conventions vary by programming language.)\n\n  * `numberOfImages`: The number of images to generate, from 1 to 4 (inclusive). The default is 4.\n  * `aspectRatio`: Changes the aspect ratio of the generated image. Supported values are `\"1:1\"`, `\"3:4\"`, `\"4:3\"`, `\"9:16\"`, and `\"16:9\"`. The default is `\"1:1\"`.\n  * `personGeneration`: Allow the model to generate images of people. The following values are supported: \n    * `\"DONT_ALLOW\"`: Block generation of images of people.\n    * `\"ALLOW_ADULT\"`: Generate images of adults, but not children. This is the default.\n\n\n\n## What's next\n\n  * To learn more about prompt writing for Imagen, see the [Imagen prompt guide](/gemini-api/docs/imagen-prompt-guide).\n  * To learn more about Gemini 2.0 models, see [Gemini models](/gemini-api/docs/models/gemini) and [Experimental models](/gemini-api/docs/models/experimental-models).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/code-execution#input-output",
    "title": "Code execution  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Code execution \n\nPython JavaScript Go REST\n\nThe Gemini API code execution feature enables the model to generate and run Python code and learn iteratively from the results until it arrives at a final output. You can use this code execution capability to build applications that benefit from code-based reasoning and that produce text output. For example, you could use code execution in an application that solves equations or processes text.\n\n**Note:** Gemini is only able to execute code in Python. You can still ask Gemini to generate code in another language, but the model can't use the code execution tool to run it.\n\nCode execution is available in both AI Studio and the Gemini API. In AI Studio, you can enable code execution in the right panel under **Tools**. The Gemini API provides code execution as a tool, similar to [function calling](/gemini-api/docs/function-calling). After you add code execution as a tool, the model decides when to use it.\n\nThe code execution environment includes the following libraries: `altair`, `chess`, `cv2`, `matplotlib`, `mpmath`, `numpy`, `pandas`, `pdfminer`, `reportlab`, `seaborn`, `sklearn`, `statsmodels`, `striprtf`, `sympy`, and `tabulate`. You can't install your own libraries.\n\n**Note:** Only `matplotlib` is supported for graph rendering using code execution.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Get started with code execution\n\n## Input/output (I/O)\n\nStarting with [Gemini 2.0 Flash](/gemini-api/docs/models/gemini#gemini-2.0-flash), code execution supports file input and graph output. Using these new input and output capabilities, you can upload CSV and text files, ask questions about the files, and have [Matplotlib](https://matplotlib.org/) graphs generated as part of the response.\n\n### I/O pricing\n\nWhen using code execution I/O, you're charged for input tokens and output tokens:\n\n**Input tokens:**\n\n  * User prompt\n\n\n\n**Output tokens:**\n\n  * Code generated by the model\n  * Code execution output in the code environment\n  * Summary generated by the model\n\n\n\n### I/O details\n\nWhen you're working with code execution I/O, be aware of the following technical details:\n\n  * The maximum runtime of the code environment is 30 seconds.\n  * If the code environment generates an error, the model may decide to regenerate the code output. This can happen up to 5 times.\n  * The maximum file input size is limited by the model token window. In AI Studio, using Gemini Flash 2.0, the maximum input file size is 1 million tokens (roughly 2MB for text files of the supported input types). If you upload a file that's too large, AI Studio won't let you send it.\n\n| Single turn | Bidirectional (Multimodal Live API)  \n---|---|---  \nModels supported | All Gemini 2.0 models | Only Flash experimental models  \nFile input types supported | .png, .jpeg, .csv, .xml, .cpp, .java, .py, .js, .ts | .png, .jpeg, .csv, .xml, .cpp, .java, .py, .js, .ts  \nPlotting libraries supported | Matplotlib | Matplotlib  \n[Multi-tool use](/gemini-api/docs/function-calling#multi-tool-use) | No | Yes  \n  \n## Billing\n\nThere's no additional charge for enabling code execution from the Gemini API. You'll be billed at the current rate of input and output tokens based on the Gemini model you're using.\n\nHere are a few other things to know about billing for code execution:\n\n  * You're only billed once for the input tokens you pass to the model, and you're billed for the final output tokens returned to you by the model.\n  * Tokens representing generated code are counted as output tokens. Generated code can include text and multimodal output like images.\n  * Code execution results are also counted as output tokens.\n\n\n\nThe billing model is shown in the following diagram:\n\n  * You're billed at the current rate of input and output tokens based on the Gemini model you're using.\n  * If Gemini uses code execution when generating your response, the original prompt, the generated code, and the result of the executed code are labeled _intermediate tokens_ and are billed as _input tokens_.\n  * Gemini then generates a summary and returns the generated code, the result of the executed code, and the final summary. These are billed as _output tokens_.\n  * The Gemini API includes an intermediate token count in the API response, so you know why you're getting additional input tokens beyond your initial prompt.\n\n\n\n## Limitations\n\n  * The model can only generate and execute code. It can't return other artifacts like media files.\n  * In some cases, enabling code execution can lead to regressions in other areas of model output (for example, writing a story).\n  * There is some variation in the ability of the different models to use code execution successfully.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/thinking-mode",
    "title": "Use Gemini thinking  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Use Gemini thinking \n\nGemini 2.5 Pro Experimental and Gemini 2.0 Flash Thinking Experimental are models that use an internal \"thinking process\" during response generation. This process contributes to their improved reasoning capabilities and allows them to solve complex tasks. This guide shows you how to use Gemini models with thinking capabilities.\n\n[ Try Gemini 2.5 Pro Preview in Google AI Studio ](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro-preview-03-25)\n\n**Note:** 'gemini-2.5-pro-preview-03-25' is a billed model, you can continue to use 'gemini-2.5-pro-exp-03-25' on the free tier.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Use thinking models\n\nModels with thinking capabilities are available in [Google AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro-preview-03-25) and through the Gemini API. Note that the thinking process is visible within Google AI Studio but is not provided as part of the API output.\n\n### Send a basic request\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    prompt = \"Explain the concept of Occam's Razor and provide a simple, everyday example.\"\n    response = client.models.generate_content(\n        model=\"gemini-2.5-pro-exp-03-25\",  # or gemini-2.0-flash-thinking-exp\n        contents=prompt\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n      const prompt = \"Explain the concept of Occam's Razor and provide a simple, everyday example.\";\n    \n      const response = await ai.models.generateContent({\n        model: \"gemini-2.5-pro-preview-03-25\",  // or gemini-2.0-flash-thinking-exp\n        contents: prompt,\n      });\n    \n      console.log(response.text);\n    }\n    \n    main();\n    \n\n### Go\n    \n    \n    // import packages here\n    \n    func main() {\n      ctx := context.Background()\n      client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n      if err != nil {\n        log.Fatal(err)\n      }\n      defer client.Close()\n    \n      model := client.GenerativeModel(\"gemini-2.5-pro-preview-03-25\")  // or gemini-2.0-flash-thinking-exp\n      resp, err := model.GenerateContent(ctx, genai.Text(\"Explain the concept of Occam's Razor and provide a simple, everyday example.\"))\n      if err != nil {\n        log.Fatal(err)\n      }\n      fmt.Println(resp.Text())\n    }\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro-preview-03-25:generateContent?key=$YOUR_API_KEY\" \\\n     -H 'Content-Type: application/json' \\\n     -X POST \\\n     -d '{\n       \"contents\": [\n         {\n           \"parts\": [\n             {\n               \"text\": \"Explain the concept of Occam\\''s Razor and provide a simple, everyday example.\"\n             }\n           ]\n         }\n       ]\n     }'\n     ```\n    \n\n### Multi-turn thinking conversations\n\nTo take the previous chat history into account, you can use multi-turn conversations.\n\nWith the SDKs, you can create a [chat session](/gemini-api/docs/text-generation#multi-turn-conversations) to manage the state of the conversation.\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key='GEMINI_API_KEY')\n    \n    chat = client.aio.chats.create(\n        model='gemini-2.5-pro-preview-03-25',  # or gemini-2.0-flash-thinking-exp\n    )\n    response = await chat.send_message('What is your name?')\n    print(response.text)\n    response = await chat.send_message('What did you just say before this?')\n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    async function main() {\n        const chat = ai.chats.create({\n            model: 'gemini-2.5-pro-preview-03-25'  // or gemini-2.0-flash-thinking-exp\n        });\n    \n        const response = await chat.sendMessage({\n            message: 'What is your name?'\n        });\n        console.log(response.text);\n    \n        response = await chat.sendMessage({\n            message: 'What did you just say before this?'\n        });\n        console.log(response.text);\n    }\n    \n    main();\n    \n\n## Use tools with thinking models\n\nThinking models can use tools to perform actions beyond generating text. This allows them to interact with external systems, execute code, or access real-time information, incorporating the results into their reasoning and final response.\n\n### Search Tool\n\nThe [Search tool](/gemini-api/docs/grounding) allows the model to query external search engines to find up-to-date information or information beyond its training data. This is useful for questions about recent events or highly specific topics.\n\nTo configure the search tool, see [Configure the Search tool](/gemini-api/docs/grounding#configure-search-tool).\n\n**Prompt:**\n    \n    \n    What were the major scientific breakthroughs announced last week?\n      \n\n**Response (using Search Tool):**\n    \n    \n    Based on recent search results, here are some highlights from the past week in science:\n    \n    ...\n      \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Code Execution\n\nThe [Code execution tool](/gemini-api/docs/code-execution) enables the model to generate and run Python code to perform calculations, manipulate data, or solve problems that are best handled algorithmically. The model receives the code's output and can use it in its response.\n\nTo configure code execution, see [Enable code execution on the model](/gemini-api/docs/code-execution#enable-on-model).\n\n**Prompt:**\n    \n    \n    What is the sum of the first 50 prime numbers?\n    Generate and run Python code for the calculation, and make sure you get all 50.\n    Provide the final sum clearly.\n      \n\n**Response (using Code Execution Tool):**\n    \n    \n    Okay, I can calculate that. I will generate Python code to find the first 50 prime numbers and sum them up.\n    \n    ```python\n    def is_prime(num):\n        if num < 2:\n            return False\n        for i in range(2, int(num**0.5) + 1):\n            if num % i == 0:\n                return False\n        return True\n    \n    primes = []\n    num = 2\n    while len(primes) < 50:\n        if is_prime(num):\n            primes.append(num)\n        num += 1\n    \n    total_sum = sum(primes)\n    print(total_sum)\n    ```\n    \n    Running the code...\n    \n    Code Execution Result:\n    Outcome: OK\n    Output:\n    \n    5117\n    Use code with caution.\n    The sum of the first 50 prime numbers is 5117.\n    \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Structured Output\n\nWith [structured output](/gemini-api/docs/structured-output), you can constrain Gemini to respond with JSON, a structured output format suitable for automated processing. This is particularly useful for integrating the model's output into applications.\n\nYou can specify the format directly in the prompt or [supply a schema through model configuration](/gemini-api/docs/structured-output#supply-schema-in-config).\n\n**Prompt:**\n    \n    \n    Provide a list of 3 famous physicists and their key contributions\n    in JSON format.\n    Use this JSON schema:\n    \n    Physicist = {'name': str, 'contribution': str, 'era': str}\n    Return: list[Physicist]\n    \n\n**Response (instructed for JSON):**\n    \n    \n    ```json\n    [\n    {\n    \"name\": \"Albert Einstein\",\n    \"contribution\": \"Theory of Relativity (Special and General)\",\n    \"era\": \"Early 20th Century\"\n    },\n    {\n    \"name\": \"Isaac Newton\",\n    \"contribution\": \"Laws of Motion and Universal Gravitation\",\n    \"era\": \"Late 17th / Early 18th Century\"\n    },\n    {\n    \"name\": \"Marie Curie\",\n    \"contribution\": \"Pioneering research on radioactivity\",\n    \"era\": \"Late 19th / Early 20th Century\"\n    }\n    ]\n    ```\n    \n\n(gemini-2.5-pro-preview-03-25)  \n---  \n  \n## Prompting techniques for thinking models\n\nEffective prompting is crucial for maximizing the benefits of the Gemini thinking models. This section outlines some prompting practices for thinking models.\n\n### Getting started\n\nBegin with general instructions that describe the desired outcome. Start with a simple prompt and observe the model's initial thoughts on how to tackle the challenge.\n\n**Prompt:**\n    \n    \n    Describe the most unusual or striking architectural feature you've ever seen in a building or structure.\n      \n\n**Response:**\n    \n    \n    <...>\n    The most unusual architectural feature that has truly stayed with me is the \"floating roof\" of the\n      Heydar Aliyev Center in Baku, Azerbaijan, designed by Zaha Hadid Architects.\n    \n    Visually, it's breathtaking. Imagine a vast, undulating landscape of pure white, seemingly draped\n    over the building. It's not just a roof in the traditional sense; it's an extension of the ground,\n    flowing seamlessly upwards and over the building's various spaces before curving back down to meet\n    the earth again. There are no sharp angles, no rigid lines. Instead, everything is fluid, organic,\n    and continuous. It resembles a giant, pristine white wave frozen in motion, or perhaps a sand dune\n    sculpted by an unseen wind.\n    <...>\n      \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Provide Step-by-Step Instructions\n\nIf the initial attempt does not yield the desired result, refine your prompt by providing more explicit step-by-step instructions to guide Gemini through the problem-solving process for a more focused approach. This allows you steer the model's initial understanding of the task with explicit suggestions and guidance.\n\n**Prompt:**\n    \n    \n    Now imagine that architectural feature is not just for looks, but has a specific function or\n    purpose related to the building's environment or the people who use it. Explain what that\n    function could be and how it enhances the building's overall design and usability.\n      \n\n**Response:**\n    \n    \n    <...>\n    Let's imagine that the \"floating roof\" of the Heydar Aliyev Center is not purely aesthetic but\n    serves a crucial environmental and user-centric function: it's designed as a sophisticated climate\n    moderator and natural ventilation system for the Baku environment.\n    \n    Here's how this function would work and enhance the building:\n    \n    Function: Passive Climate Control and Natural Ventilation System\n    \n    Shading and Solar Heat Mitigation: The undulating curves of the white GFRC roof are strategically\n    shaped and oriented to provide optimal shading throughout the day, year-round. Baku experiences\n    intense sun, especially during the summer. The overhanging curves, acting like giant brise-soleils,\n    would block direct sunlight from hitting the building's glass facade, significantly reducing solar\n    heat gain. The varying heights and depths of the curves would create dynamic shade patterns, ensuring\n    that different parts of the building are shaded at different times of the day, optimizing comfort\n    and reducing the need for excessive air conditioning. The white color of the GFRC further enhances\n    this by reflecting a large portion of the solar radiation.\n    <...>\n      \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Multishot Prompting with Thinking\n\nMultishot prompting in conjunction with thinking can further enhance Gemini's understanding and improve the accuracy of its responses. Provide several examples of input-output pairs, demonstrating the desired behavior and output format in your prompt.\n\n**Prompt:**\n    \n    \n    Example 1:\n    User: What is the tallest mountain in the world?\n    Assistant: Mount Everest\n    \n    Example 2:\n    User: What is the largest ocean?\n    Assistant: Pacific Ocean\n    \n    User: What is the longest river in the world?\n    Assistant:\n      \n\n**Response:**\n    \n    \n    Amazon River\n      \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Defining Output and Behavior\n\nWhen building an application where users will interact directly with the model, you may want to provide guidance for how Gemini's output and responses will sound and be formatted.\n\n#### System Instructions\n\nSystem instructions are a set of instructions that the model processes before it processes prompts. They are recalled each time the model is prompted and tell the model how you want it to behave and respond. For example, you can include things like the role or persona, contextual information, and formatting instructions.\n\n**Prompt:**\n    \n    \n    System Instruction: You are a helpful and concise coding tutor. Always provide code examples and explain them briefly.\n    \n    User Prompt: Explain what is a closure in Javascript and give an example.\n      \n\n**Response:**\n    \n    \n        function outerFunction(outerVar) {\n          function innerFunction(innerVar) {\n            console.log(outerVar); // Accessing outerVar from outerFunction's scope\n            console.log(innerVar);\n          }\n          return innerFunction;\n        }\n    \n        const closureExample = outerFunction('I am from outer');\n        closureExample('I am from inner');\n    \n    Explanation:\n    \n    A closure is created when a function (innerFunction in this example) is defined inside another function\n    (outerFunction) and the inner function remembers and has access to the variables from the outer\n    function's scope, even after the outer function has finished executing.\n    \n    In the example:\n    \n    outerFunction is called with 'I am from outer' and returns innerFunction.\n    \n    closureExample now holds a reference to innerFunction.\n    \n    When closureExample('I am from inner') is called, innerFunction executes.\n    \n    Even though outerFunction has already finished, innerFunction can still access and use outerVar\n    (which is 'I am from outer') because of the closure.\n    \n    In essence, the innerFunction \"closes over\" the variables of its outer function's scope, creating\n    a closure.\n      \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n#### Verification and Reflection\n\nInclude specific verification steps in either the System Instructions or your prompts directly. For example, ask Gemini to verify its sources, review its reasoning, identify potential errors, and check its final answer.\n\n**Prompt:**\n    \n    \n        Verify the following statement and provide your sources: The capital of Brazil is Rio de Janeiro.\n      \n\n**Response:**\n    \n    \n    The statement \"The capital of Brazil is Rio de Janeiro\" is incorrect.\n    \n    The current capital of Brazil is Brasília.\n    \n    Rio de Janeiro was the capital of Brazil for a significant period, from 1763 to 1960. However, the\n    capital was moved to Brasília in 1960 as part of a plan to develop the country's interior and create\n    a more centrally located capital.\n    \n    Verification and Sources:\n    \n    You can easily verify this information from numerous reliable sources:\n    \n    Encyclopedias:\n    \n    Britannica: Searching for \"Brazil\" on Britannica will clearly state Brasília as the capital.\n    \n    https://www.britannica.com/place/Brazil (Look for the \"Capital\" information in the introductory summary or \"Geography\" section)\n    \n    Wikipedia: The Wikipedia article for \"Brazil\" also states Brasília as the capital.\n    \n    https://en.wikipedia.org/wiki/Brazil (Look in the infobox or introductory section)\n    \n    Government and Official Sources:\n    \n    CIA World Factbook: This is a highly reliable source for country information. Searching for \"Brazil\"\n    will list Brasília as the capital.\n    \n    https://www.cia.gov/the-world-factbook/countries/brazil/ (Look under \"Government\" > \"Capital\")\n    \n    Official Website of the Brazilian Government (if accessible and in English): While navigating official\n    government websites can sometimes be complex, they would definitively list Brasília as the capital.\n    \n    Geographic Resources:\n    \n    National Geographic: Their website and publications will confirm Brasília as the capital of Brazil.\n    \n    Atlases and Geography Textbooks: Any modern atlas or geography textbook will list Brasília as the capital.\n    \n    Why the confusion?\n    \n    The confusion might arise because:\n    \n    Rio de Janeiro is a very famous Brazilian city: It's a major tourist destination, known for its\n    beaches, Carnival, and iconic landmarks like Christ the Redeemer. Its past status as capital adds\n    to its historical significance and public recognition.\n    \n    Historical context: Rio de Janeiro was the capital for a long time. People who learned about Brazil\n    in the past might remember it as the capital.\n    \n    In conclusion, the statement is false. The capital of Brazil is Brasília, not Rio de Janeiro. Rio\n    de Janeiro was the former capital but is no longer the current capital.\n    \n    You can confidently use the sources listed above to verify this information.\n      \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Debugging and Steering\n\nGoogle AI Studio provides developers access to the model's thoughts in the UI. This is helpful as a developer reviewing your prompt when debugging:\n\n#### Analyze the Thought Process\n\nCarefully review Gemini's thought process to identify any areas where it may have gone astray. This analysis can provide valuable clues for refining your prompts and getting your expected response consistently in your application.\n\n_Example Scenario: If Gemini's thought process shows it misunderstood a step in a complex instruction, you can refine the prompt to clarify that specific step._\n\n**Prompt:**\n    \n    \n    Step 1: Summarize the following article about quantum physics.\n    Step 2: Translate the summary into Spanish.\n    Step 3: Extract the key entities from the Spanish summary.\n    \n    Article: ... [Article Text] ...\n    \n\n**Gemini's Thought Process Analysis (Example):**\n    \n    \n    The user wants me to process an article about Google's new quantum chip called Willow in three steps:\n    Step 1: Summarize the article.\n    Step 2: Translate the summary into Spanish.\n    Step 3: Extract the key entities from the Spanish summary.\n    \n    Let's break down each step.\n    \n    Step 1: Summarize the article.\n    I need to read the article and identify the main points. The article is about Google's new quantum\n    chip, Willow, and its achievements. The key achievements are:\n    \n    Exponential error reduction as qubits scale up.\n    \n    Outperforming supercomputers in a benchmark computation (RCS).\n    \n    Willow is a step towards commercially relevant quantum computing.\n    \n    It's the first system below threshold for quantum error correction.\n    \n    It achieved a benchmark computation in under 5 minutes that would take a supercomputer 10 septillion years.\n    \n    It was fabricated in a new facility.\n    \n    It has state-of-the-art performance and high quality qubits.\n    \n    Google is looking towards real-world applications with Willow.\n    \n    Quantum computing will benefit AI.\n    \n    I will synthesize these points into a concise summary.\n    \n    Step 2: Translate the summary into Spanish.\n    Once I have the summary in English, I will translate it into Spanish. I need to ensure the translation\n    is accurate and natural-sounding in Spanish.\n    \n    Step 3: Extract the key entities from the Spanish summary.\n    After translating the summary, I will identify and extract the key entities. Key entities are typically\n    nouns or noun phrases that represent important objects, concepts, or actors. In this context, they\n    will likely include:\n    \n    Names of chips (Willow, Sycamore)\n    \n    Companies/Organizations (Google Quantum AI, Google Research)\n    \n    Technologies (quantum chip, quantum computer, qubits, quantum error correction, superconducting\n    quantum system, classical computers, supercomputers, AI)\n    \n    Benchmarks (random circuit sampling, RCS)\n    \n    People (Julian Kelly, Peter Shor, David Deutsch, Sergio Boixo, Hartmut Neven, John Preskill)\n    \n    Concepts (exponential error reduction, below threshold, beyond breakeven, state-of-the-art performance,\n    commercially relevant applications, quantum mechanics, multiverse, T1 times, system benchmarks,\n    algorithmic benchmarks)\n    \n    I will need to carefully review the Spanish summary to identify these entities and list them out.\n    \n      \n  \n---  \n  \n#### Provide Corrective Feedback\n\nIf you observe errors or inconsistencies in Gemini's reasoning, provide corrective feedback in your prompt and/or System Instructions to anticipate where the model's response may deviate from your desired outcome.\n\n**Prompt:**\n    \n    \n    You are an expert in botany. When asked to identify a plant, focus on leaf shape, flower structure, and stem characteristics.\n    \n    Identify this plant based on these characteristics: ... [Plant Characteristics Description] ...\n    \n    Previous incorrect response focused on habitat only. Please disregard habitat and focus on morphology.\n    \n\n**Refined Prompt (if needed based on thought process):**\n    \n    \n    Previous incorrect response focused on habitat only. Please disregard habitat and focus on morphology.\n    \n\n**Response:**\n    \n    \n    Tabebuia vellosoi\n    \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n### Making the Best of Long Outputs and Longform Thinking\n\nThinking generates “thoughts” as part of the Token Output that showcases Gemini's detailed reasoning. To make the most of these outputs:\n\n#### Review the Reasoning\n\nPay close attention to Gemini's reasoning process (available in the Google AI Studio UI), how it broke down the task and the choices it made for your output. This can help you understand how it arrived at its conclusion.\n\n#### Provide Guidance in Reasoning\n\nIf your desired output is to be particularly lengthy, you may want to provide more guidance in your prompt to constrain the amount of thinking needed to accomplish the task presented in the prompt. This allows you to reserve more of the Token Output for your response.\n\n_Example: For a detailed report, you might specify sections and sub-sections in your prompt to guide Gemini's thinking and structure the output efficiently._\n\n**Prompt:**\n    \n    \n    Generate a detailed report on the impact of climate change on coastal ecosystems.\n    \n    Report Structure:\n    \n    Introduction\n    \n    Rising Sea Levels\n    2.1. Impact on Mangroves\n    2.2. Impact on Coral Reefs\n    \n    Ocean Acidification\n    3.1. Impact on Shellfish\n    3.2. Impact on Plankton\n    \n    Extreme Weather Events\n    4.1. Increased Storm Intensity\n    4.2. Coastal Erosion\n    \n    Conclusion\n    \n    Provide detailed analysis within each section.\n    \n\n**Response:**\n    \n    \n    <...>\n    Climate Change Impact on Coastal Ecosystems: A Detailed Report\n    \n    1. Introduction\n    \n    Coastal ecosystems, the dynamic interface between land and sea, are among the most productive and\n    biodiverse environments on Earth. They provide crucial ecosystem services, including coastal protection,\n    fisheries, carbon sequestration, and support for a vast array of species. However, these vital\n    ecosystems are increasingly threatened by the multifaceted impacts of climate change. Driven by\n    anthropogenic greenhouse gas emissions, climate change is manifesting in several interconnected ways\n    that are profoundly altering the physical and chemical conditions of coastal environments. This report\n    will delve into the key impacts of climate change on coastal ecosystems, focusing on rising sea levels,\n    ocean acidification, and extreme weather events, and their specific consequences for critical habitats\n    like mangroves and coral reefs, and key organisms like shellfish and plankton. Understanding these\n    impacts is crucial for developing effective conservation and management strategies to mitigate the\n    damage and enhance the resilience of these invaluable ecosystems.\n    \n    2. Rising Sea Levels\n    \n    Rising sea levels are a direct and significant consequence of global warming, driven by thermal\n    expansion of seawater and the melting of land ice (glaciers and ice sheets). This phenomenon poses a\n    direct threat to low-lying coastal ecosystems by inundating habitats, altering salinity regimes, and\n    increasing erosion rates. The rate of sea level rise is accelerating, and projections for the 21st\n    century indicate a continued upward trend, with potentially devastating consequences for coastal\n    environments.\n    \n    2.1. Impact on Mangroves\n    \n    Mangrove forests are unique coastal ecosystems found in tropical and subtropical intertidal zones.\n    They provide numerous benefits, including coastal protection against storms, nursery grounds for\n    fish and invertebrates, and significant carbon sequestration...\n    <...>\n    \n\n(gemini-2.5-pro-exp-03-25)  \n---  \n  \n## What's next?\n\n  * Try Gemini 2.5 Pro Preview in [Google AI Studio](https://aistudio.google.com/prompts/new_chat?model=gemini-2.5-pro-preview-03-25).\n  * For more info about Gemini 2.5 Pro Preview and Gemini Flash 2.0 Thinking, see the [model page](/gemini-api/docs/gemini-api/docs/models).\n  * Try more examples in the [Thinking cookbook](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_thinking.ipynb).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/gemini-v2",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/safety-settings#safety-filters",
    "title": "Safety settings  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Safety settings \n\nThe Gemini API provides safety settings that you can adjust during the prototyping stage to determine if your application requires more or less restrictive safety configuration. You can adjust these settings across five filter categories to restrict or allow certain types of content.\n\nThis guide covers how the Gemini API handles safety settings and filtering and how you can change the safety settings for your application.\n\n**Note:** Applications that use less restrictive safety settings may be subject to review. See the [Terms of Service](/gemini-api/terms#use-restrictions) for more information.\n\n## Safety filters\n\nThe Gemini API's adjustable safety filters cover the following categories:\n\nCategory | Description  \n---|---  \nHarassment |  Negative or harmful comments targeting identity and/or protected attributes.   \nHate speech |  Content that is rude, disrespectful, or profane.   \nSexually explicit |  Contains references to sexual acts or other lewd content.   \nDangerous |  Promotes, facilitates, or encourages harmful acts.   \nCivic integrity |  Election-related queries.   \nThese categories are defined in [`HarmCategory`](/api/rest/v1/HarmCategory). The Gemini models only support `HARM_CATEGORY_HARASSMENT`, `HARM_CATEGORY_HATE_SPEECH`, `HARM_CATEGORY_SEXUALLY_EXPLICIT`, `HARM_CATEGORY_DANGEROUS_CONTENT`, and `HARM_CATEGORY_CIVIC_INTEGRITY`. All other categories are used only by PaLM 2 (Legacy) models. \n\nYou can use these filters to adjust what's appropriate for your use case. For example, if you're building video game dialogue, you may deem it acceptable to allow more content that's rated as _Dangerous_ due to the nature of the game.\n\nIn addition to the adjustable safety filters, the Gemini API has built-in protections against core harms, such as content that endangers child safety. These types of harm are always blocked and cannot be adjusted.\n\n### Content safety filtering level\n\nThe Gemini API categorizes the probability level of content being unsafe as `HIGH`, `MEDIUM`, `LOW`, or `NEGLIGIBLE`.\n\nThe Gemini API blocks content based on the probability of content being unsafe and not the severity. This is important to consider because some content can have low probability of being unsafe even though the severity of harm could still be high. For example, comparing the sentences:\n\n  1. The robot punched me.\n  2. The robot slashed me up.\n\n\n\nThe first sentence might result in a higher probability of being unsafe, but you might consider the second sentence to be a higher severity in terms of violence. Given this, it is important that you carefully test and consider what the appropriate level of blocking is needed to support your key use cases while minimizing harm to end users.\n\n### Safety filtering per request\n\nYou can adjust the safety settings for each request you make to the API. When you make a request, the content is analyzed and assigned a safety rating. The safety rating includes the category and the probability of the harm classification. For example, if the content was blocked due to the harassment category having a high probability, the safety rating returned would have category equal to `HARASSMENT` and harm probability set to `HIGH`.\n\nBy default, safety settings block content (including prompts) with medium or higher probability of being unsafe across any filter. This baseline safety is designed to work for most use cases, so you should only adjust your safety settings if it's consistently required for your application.\n\nThe following table describes the block settings you can adjust for each category. For example, if you set the block setting to **Block few** for the **Hate speech** category, everything that has a high probability of being hate speech content is blocked. But anything with a lower probability is allowed.\n\nThreshold (Google AI Studio) | Threshold (API) | Description  \n---|---|---  \nBlock none | `BLOCK_NONE` | Always show regardless of probability of unsafe content  \nBlock few | `BLOCK_ONLY_HIGH` | Block when high probability of unsafe content  \nBlock some | `BLOCK_MEDIUM_AND_ABOVE` | Block when medium or high probability of unsafe content  \nBlock most | `BLOCK_LOW_AND_ABOVE` | Block when low, medium or high probability of unsafe content  \nN/A | `HARM_BLOCK_THRESHOLD_UNSPECIFIED` | Threshold is unspecified, block using default threshold  \n  \nIf the threshold is not set, the default block threshold is **Block none** (for `gemini-1.5-pro-002` and `gemini-1.5-flash-002` and all newer stable GA models) or **Block some** (in all other models) for all categories **except** the _Civic integrity_ category.\n\nThe default block threshold for the _Civic integrity_ category is **Block none** (for `gemini-2.0-flash-001` aliased as `gemini-2.0-flash`, `gemini-2.0-pro-exp-02-05`, and `gemini-2.0-flash-lite`) both for Google AI Studio and the Gemini API, and **Block most** for all other models in Google AI Studio only.\n\nYou can set these settings for each request you make to the generative service. See the [`HarmBlockThreshold`](/api/generate-content#harmblockthreshold) API reference for details.\n\n### Safety feedback\n\n[`generateContent`](/api/generate-content#method:-models.generatecontent) returns a [`GenerateContentResponse`](/api/generate-content#generatecontentresponse) which includes safety feedback.\n\nPrompt feedback is included in [`promptFeedback`](/api/generate-content#promptfeedback). If `promptFeedback.blockReason` is set, then the content of the prompt was blocked.\n\nResponse candidate feedback is included in [`Candidate.finishReason`](/api/generate-content#candidate) and [`Candidate.safetyRatings`](/api/generate-content#candidate). If response content was blocked and the `finishReason` was `SAFETY`, you can inspect `safetyRatings` for more details. The content that was blocked is not returned.\n\n## Adjust safety settings\n\nThis section covers how to adjust the safety settings in both Google AI Studio and in your code.\n\n### Google AI Studio\n\nYou can adjust safety settings in Google AI Studio, but you cannot turn them off.\n\nClick **Edit safety settings** in the **Run settings** panel to open the **Run safety settings** modal. In the modal, you can use the sliders to adjust the content filtering level per safety category:\n\n**Note:** If you set any of the category filters to **Block none** , Google AI Studio will display a reminder about the Gemini API's [Terms of Service](/gemini-api/terms#use-restrictions) with respect to safety settings.\n\nWhen you send a request (for example, by asking the model a question), a warning **No Content** message appears if the request's content is blocked. To see more details, hold the pointer over the **No Content** text and click warning **Safety**.\n\n### Gemini API SDKs\n\nThe following code snippet shows how to set safety settings in your `GenerateContent` call. This sets the thresholds for the harassment (`HARM_CATEGORY_HARASSMENT`) and hate speech (`HARM_CATEGORY_HATE_SPEECH`) categories. For example, setting these categories to `BLOCK_LOW_AND_ABOVE` blocks any content that has a low or higher probability of being harassment or hate speech. To understand the threshold settings, see Safety filtering per request.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    import PIL.Image\n    \n    img = PIL.Image.open(\"cookies.jpg\")\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=['Do these look store-bought or homemade?', img],\n        config=types.GenerateContentConfig(\n          safety_settings=[\n            types.SafetySetting(\n                category=types.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n                threshold=types.HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n            ),\n          ]\n        )\n    )\n    \n    print(response.text)\n    \n\n### Go\n    \n    \n    model := client.GenerativeModel(\"gemini-1.5-flash\")\n    \n    model.SafetySettings = []*genai.SafetySetting{\n      {\n        Category:  genai.HarmCategoryHarassment,\n        Threshold: genai.HarmBlockLowAndAbove,\n      },\n      {\n        Category:  genai.HarmCategoryHateSpeech,\n        Threshold: genai.HarmBlockLowAndAbove,\n      },\n    }\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n    const safetySettings = [\n      {\n        category: \"HARM_CATEGORY_HARASSMENT\",\n        threshold: \"BLOCK_LOW_AND_ABOVE\",\n      },\n      {\n        category: \"HARM_CATEGORY_HATE_SPEECH\",\n        threshold: \"BLOCK_LOW_AND_ABOVE\",\n      },\n    ];\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"Some potentially unsafe prompt.\",\n        config: {\n          safetySettings: safetySettings,\n        },\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### Dart (Flutter)\n    \n    \n    final safetySettings = [\n      SafetySetting(HarmCategory.harassment, HarmBlockThreshold.low),\n      SafetySetting(HarmCategory.hateSpeech, HarmBlockThreshold.low),\n    ];\n    final model = GenerativeModel(\n      model: 'gemini-1.5-flash',\n      apiKey: apiKey,\n      safetySettings: safetySettings,\n    );\n    \n\n### Kotlin\n    \n    \n    val harassmentSafety = SafetySetting(HarmCategory.HARASSMENT, BlockThreshold.LOW_AND_ABOVE)\n    \n    val hateSpeechSafety = SafetySetting(HarmCategory.HATE_SPEECH, BlockThreshold.LOW_AND_ABOVE)\n    \n    val generativeModel = GenerativeModel(\n        modelName = \"gemini-1.5-flash\",\n        apiKey = BuildConfig.apiKey,\n        safetySettings = listOf(harassmentSafety, hateSpeechSafety)\n    )\n    \n\n### Java\n    \n    \n    SafetySetting harassmentSafety = new SafetySetting(HarmCategory.HARASSMENT,\n        BlockThreshold.LOW_AND_ABOVE);\n    \n    SafetySetting hateSpeechSafety = new SafetySetting(HarmCategory.HATE_SPEECH,\n        BlockThreshold.LOW_AND_ABOVE);\n    \n    GenerativeModel gm = new GenerativeModel(\n        \"gemini-1.5-flash\",\n        BuildConfig.apiKey,\n        null, // generation config is optional\n        Arrays.asList(harassmentSafety, hateSpeechSafety)\n    );\n    \n    GenerativeModelFutures model = GenerativeModelFutures.from(gm);\n    \n\n### REST\n    \n    \n        echo '{\n        \"safetySettings\": [\n            {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_ONLY_HIGH\"},\n            {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"}\n        ],\n        \"contents\": [{\n            \"parts\":[{\n                \"text\": \"'I support Martians Soccer Club and I think Jupiterians Football Club sucks! Write a ironic phrase about them.'\"}]}]}' > request.json\n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key=$GEMINI_API_KEY\" \\\n        -H 'Content-Type: application/json' \\\n        -X POST \\\n        -d @request.json 2> /dev/null\n    \n    [safety_settings.sh](https://github.com/google-gemini/generative-ai-python/blob/61867f68149a63fd0cf3c37bd46177a75ec5d8ed/samples/rest/safety_settings.sh#L20-L33)\n    \n    \n    \n\n## Next steps\n\n  * See the [API reference](/api) to learn more about the full API.\n  * Review the [safety guidance](/gemini-api/docs/safety-guidance) for a general look at safety considerations when developing with LLMs.\n  * Learn more about assessing probability versus severity from the [Jigsaw team](https://developers.perspectiveapi.com/s/about-the-api-score)\n  * Learn more about the products that contribute to safety solutions like the [Perspective API](https://medium.com/jigsaw/reducing-toxicity-in-large-language-models-with-perspective-api-c31c39b7a4d7). * You can use these safety settings to create a toxicity classifier. See the [classification example](/examples/train_text_classifier_embeddings) to get started.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-07 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-07 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/json-mode#supply-schema-in-config",
    "title": "Generate structured output with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Generate structured output with the Gemini API \n\nPython JavaScript Go REST\n\n  \n\n\nGemini generates unstructured text by default, but some applications require structured text. For these use cases, you can constrain Gemini to respond with JSON, a structured data format suitable for automated processing. You can also constrain the model to respond with one of the options specified in an enum.\n\nHere are a few use cases that might require structured output from the model:\n\n  * Build a database of companies by pulling company information out of newspaper articles.\n  * Pull standardized information out of resumes.\n  * Extract ingredients from recipes and display a link to a grocery website for each ingredient.\n\n\n\nIn your prompt, you can ask Gemini to produce JSON-formatted output, but note that the model is not guaranteed to produce JSON and nothing but JSON. For a more deterministic response, you can pass a specific JSON schema in a [`responseSchema`](/api/rest/v1beta/GenerationConfig#FIELDS.response_schema) field so that Gemini always responds with an expected structure. To learn more about working with schemas, see More about JSON schemas.\n\nThis guide shows you how to generate JSON using the [`generateContent`](/api/rest/v1/models/generateContent) method through the SDK of your choice or using the REST API directly. The examples show text-only input, although Gemini can also produce JSON responses to multimodal requests that include [images](/gemini-api/docs/vision), [videos](/gemini-api/docs/vision), and [audio](/gemini-api/docs/audio).\n\n## More about JSON schemas\n\nWhen you configure the model to return a JSON response, you can use a `Schema` object to define the shape of the JSON data. The `Schema` represents a select subset of the [OpenAPI 3.0 Schema object](https://spec.openapis.org/oas/v3.0.3#schema-object).\n\nHere's a pseudo-JSON representation of all the `Schema` fields:\n    \n    \n    {\n      \"type\": enum (Type),\n      \"format\": string,\n      \"description\": string,\n      \"nullable\": boolean,\n      \"enum\": [\n        string\n      ],\n      \"maxItems\": string,\n      \"minItems\": string,\n      \"properties\": {\n        string: {\n          object (Schema)\n        },\n        ...\n      },\n      \"required\": [\n        string\n      ],\n      \"propertyOrdering\": [\n        string\n      ],\n      \"items\": {\n        object (Schema)\n      }\n    }\n    \n\nThe `Type` of the schema must be one of the OpenAPI [Data Types](https://spec.openapis.org/oas/v3.0.3#data-types). Only a subset of fields is valid for each `Type`. The following list maps each `Type` to valid fields for that type:\n\n  * `string` -> enum, format\n  * `integer` -> format\n  * `number` -> format\n  * `boolean`\n  * `array` -> minItems, maxItems, items\n  * `object` -> properties, required, propertyOrdering, nullable\n\n\n\nHere are some example schemas showing valid type-and-field combinations:\n    \n    \n    { \"type\": \"string\", \"enum\": [\"a\", \"b\", \"c\"] }\n    \n    { \"type\": \"string\", \"format\": \"date-time\" }\n    \n    { \"type\": \"integer\", \"format\": \"int64\" }\n    \n    { \"type\": \"number\", \"format\": \"double\" }\n    \n    { \"type\": \"boolean\" }\n    \n    { \"type\": \"array\", \"minItems\": 3, \"maxItems\": 3, \"items\": { \"type\": ... } }\n    \n    { \"type\": \"object\",\n      \"properties\": {\n        \"a\": { \"type\": ... },\n        \"b\": { \"type\": ... },\n        \"c\": { \"type\": ... }\n      },\n      \"nullable\": true,\n      \"required\": [\"c\"],\n      \"propertyOrdering\": [\"c\", \"b\", \"a\"]\n    }\n    \n\nFor complete documentation of the Schema fields as they're used in the Gemini API, see the [Schema reference](/api/caching#Schema).\n\n### Property ordering\n\nWhen you're working with JSON schemas in the Gemini API, the order of properties is important. By default, the API orders properties alphabetically and does not preserve the order in which the properties are defined (although the [Google Gen AI SDKs](/gemini-api/docs/sdks) may preserve this order). If you're providing examples to the model with a schema configured, and the property ordering of the examples is not consistent with the property ordering of the schema, the output could be rambling or unexpected.\n\nTo ensure a consistent, predictable ordering of properties, you can use the optional `propertyOrdering[]` field.\n    \n    \n    \"propertyOrdering\": [\"recipe_name\", \"ingredients\"]\n    \n\n`propertyOrdering[]` – not a standard field in the OpenAPI specification – is an array of strings used to determine the order of properties in the response. By specifying the order of properties and then providing examples with properties in that same order, you can potentially improve the quality of results.\n\n**Key Point:** To improve results when you're using a JSON schema, set `propertyOrdering[]` and provide examples with a matching property ordering.\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-pro",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-flash",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/embeddings#elastic-embedding",
    "title": "Embeddings  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Embeddings \n\n**Note:** Introducing our first Gemini embedding model, [available now to developers](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) as `gemini-embedding-exp-03-07` in the API.\n\nThe Gemini API supports several embedding models that generate embeddings for words, phrases, code, and sentences. The resulting embeddings can then be used for tasks such as semantic search, text classification, and clustering, among many others.\n\n## What are embeddings?\n\nEmbeddings are numerical representations of text (or other media formats) that capture relationships between inputs. Text embeddings work by converting text into arrays of floating point numbers, called _vectors_. These vectors are designed to capture the meaning of the text. The length of the embedding array is called the vector's _dimensionality_. A passage of text might be represented by a vector containing hundreds of dimensions.\n\nEmbeddings capture semantic meaning and context, which results in text with similar meanings having \"closer\" embeddings. For example, the sentence \"I took my dog to the vet\" and \"I took my cat to the vet\" would have embeddings that are close to each other in the vector space.\n\nYou can use embeddings to compare different texts and understand how they relate. For example, if the embeddings of the text \"cat\" and \"dog\" are close together you can infer that these words are similar in meaning, context, or both. This enables a variety of [common AI use cases](/gemini-api/docs/embeddings#use-cases).\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\n## Generate embeddings\n\nUse the `embedContent` method to generate text embeddings:\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    result = client.models.embed_content(\n            model=\"gemini-embedding-exp-03-07\",\n            contents=\"What is the meaning of life?\")\n    \n    print(result.embeddings)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    async function main() {\n    \n        const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n        const response = await ai.models.embedContent({\n            model: 'gemini-embedding-exp-03-07',\n            contents: 'What is the meaning of life?',\n        });\n    \n        console.log(response.embeddings);\n    }\n    \n    main();\n    \n\n### Go\n    \n    \n    ctx := context.Background()\n    \n    client, err := genai.NewClient(ctx, option.WithAPIKey(os.Getenv(\"GEMINI_API_KEY\")))\n    if err != nil {\n        log.Fatal(err)\n    }\n    defer client.Close()\n    \n    em := client.EmbeddingModel(\"gemini-embedding-exp-03-07\")\n    res, err := em.EmbedContent(ctx, genai.Text(\"What is the meaning of life?\"))\n    \n    if err != nil {\n        panic(err)\n    }\n    fmt.Println(res.Embedding.Values)\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-exp-03-07:embedContent?key=$GEMINI_API_KEY\" \\\n    -H 'Content-Type: application/json' \\\n    -d '{\"model\": \"models/gemini-embedding-exp-03-07\",\n         \"content\": {\n         \"parts\":[{\n         \"text\": \"What is the meaning of life?\"}]}\n        }'\n    \n\nYou can also generate embeddings for multiple chunks at once by passing them in as a list of strings.\n\n## Task types\n\nWhen building Retrieval Augmented Generation (RAG) systems, a common design is to use text embeddings to perform a similarity search. In some cases this can lead to degraded quality, because questions and their answers are not semantically similar. For example, a question like \"Why is the sky blue?\" and its answer \"The scattering of sunlight causes the blue color,\" have distinctly different meanings as statements, which means that a RAG system won't automatically recognize their relation.\n\nTask types enable you to generate optimized embeddings for specific tasks, saving you time and cost and improving performance.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client(api_key=\"GEMINI_API_KEY\")\n    \n    result = client.models.embed_content(\n            model=\"gemini-embedding-exp-03-07\",\n            contents=\"What is the meaning of life?\",\n            config=types.EmbedContentConfig(task_type=\"SEMANTIC_SIMILARITY\")\n    )\n    print(result.embeddings)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    async function main() {\n    \n        const ai = new GoogleGenAI({ apiKey: \"GEMINI_API_KEY\" });\n    \n        const response = await ai.models.embedContent({\n            model: 'gemini-embedding-exp-03-07',\n            contents: 'What is the meaning of life?',\n            config: {\n                taskType: \"SEMANTIC_SIMILARITY\",\n            }\n        });\n    \n        console.log(response.embeddings);\n    }\n    \n    main();\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-exp-03-07:embedContent?key=$GEMINI_API_KEY\" \\\n    -H 'Content-Type: application/json' \\\n    -d '{\"model\": \"models/gemini-embedding-exp-03-07\",\n         \"content\": {\n         \"parts\":[{\n         \"text\": \"What is the meaning of life?\"}]},\n         \"taskType\": \"SEMANTIC_SIMILARITY\"\n        }'\n    \n\n### Supported task types\n\nTask type | Description  \n---|---  \n`SEMANTIC_SIMILARITY` | Used to generate embeddings that are optimized to assess text similarity.  \n`CLASSIFICATION` | Used to generate embeddings that are optimized to classify texts according to preset labels.  \n`CLUSTERING` | Used to generate embeddings that are optimized to cluster texts based on their similarities.  \n`RETRIEVAL_DOCUMENT`, `RETRIEVAL_QUERY`, `QUESTION_ANSWERING`, and `FACT_VERIFICATION` | Used to generate embeddings that are optimized for document search or information retrieval.  \n`CODE_RETRIEVAL_QUERY` | Used to retrieve a code block based on a natural language query, such as sort an array or reverse a linked list. Embeddings of the code blocks are computed using `RETRIEVAL_DOCUMENT`.  \n  \n## Use cases\n\nText embeddings are used in a variety of common AI use cases, such as:\n\n  * **Information retrieval:** You can use embeddings to retrieve semantically similar text given a piece of input text.\n\n[ Document search tutorialtask ](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/document_search.ipynb)\n\n  * **Clustering:** Comparing groups of embeddings can help identify hidden trends.\n\n[ Embedding clustering tutorialbubble_chart ](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/clustering_with_embeddings.ipynb)\n\n  * **Vector database:** As you take different embedding use cases to production, it is common to store embeddings in a vector database.\n\n[ Vector database tutorialbolt ](https://github.com/google-gemini/cookbook/blob/main/examples/chromadb/Vectordb_with_chroma.ipynb)\n\n  * **Classification:** You can train a model using embeddings to classify documents into categories.\n\n[ Classification tutorialtoken ](https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/tutorials/text_classifier_embeddings.ipynb)\n\n\n\n\n## Embedding models\n\nThe Gemini API offers three models that generate text embeddings:\n\n  * [gemini-embedding-exp-03-07](/gemini-api/docs/models#gemini-embedding)\n  * [text-embedding-004](/gemini-api/docs/models/gemini#text-embedding)\n  * [embedding-001](/gemini-api/docs/models/gemini#embedding)\n\n\n\n## What's next\n\nCheck out the [embeddings quickstart notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Embeddings.ipynb).\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-03 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-03 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/function-calling#function_calling_mode",
    "title": "Function Calling with the Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Function Calling with the Gemini API \n\nFunction calling lets you connect models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:\n\n  * **Augment Knowledge:** Access information from external sources like databases, APIs, and knowledge bases.\n  * **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.\n  * **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices\n\n\n\nGet Weather Schedule Meeting Create Chart\n\n## How Function Calling Works\n\nFunction calling involves a structured interaction between your application, the model, and external functions. Here's a breakdown of the process:\n\n  1. **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.\n  2. **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.\n  3. **Execute Function Code (Your Responsibility):** The Model _does not_ execute the function itself. It's your application's responsibility to process the response and check for Function Call, if \n     * **Yes** : Extract the name and args of the function and execute the corresponding function in your application.\n     * **No:** The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).\n  4. **Create User friendly response:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.\n\n\n\nThis process can be repeated over multiple turns, allowing for complex interactions and workflows. The model also supports calling multiple functions in a single turn ([parallel function calling](/gemini-api/docs/function-calling#parallel_function_calling)) and in sequence ([compositional function calling](/gemini-api/docs/function-calling#compositional_function_calling)).\n\n### Step 1: Define Function Declaration\n\nDefine a function and its declaration within your application code that allows users to set light values and make an API request. This function could call external services or APIs.\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Define a function that the model can call to control smart lights\n    set_light_values_declaration = {\n        \"name\": \"set_light_values\",\n        \"description\": \"Sets the brightness and color temperature of a light.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n                },\n                \"color_temp\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"daylight\", \"cool\", \"warm\"],\n                    \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n                },\n            },\n            \"required\": [\"brightness\", \"color_temp\"],\n        },\n    }\n    \n    # This is the actual function that would be called based on the model's suggestion\n    def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n        \"\"\"Set the brightness and color temperature of a room light. (mock API).\n    \n        Args:\n            brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n            color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    \n        Returns:\n            A dictionary containing the set brightness and color temperature.\n        \"\"\"\n        return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n    \n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    // Define a function that the model can call to control smart lights\n    const setLightValuesFunctionDeclaration = {\n      name: 'set_light_values',\n      description: 'Sets the brightness and color temperature of a light.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',\n          },\n          color_temp: {\n            type: Type.STRING,\n            enum: ['daylight', 'cool', 'warm'],\n            description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',\n          },\n        },\n        required: ['brightness', 'color_temp'],\n      },\n    };\n    \n    /**\n    * Set the brightness and color temperature of a room light. (mock API)\n    * @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness\n    * @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n    * @return {Object} A dictionary containing the set brightness and color temperature.\n    */\n    function setLightValues(brightness, color_temp) {\n      return {\n        brightness: brightness,\n        colorTemperature: color_temp\n      };\n    }\n    \n\n### Step 2: Call the model with function declarations\n\nOnce you have defined your function declarations, you can prompt the model to use the function. It analyzes the prompt and function declarations and decides to respond directly or to call a function. If a function is called the response object will contain a function call suggestion.\n\n### Python\n    \n    \n    from google import genai\n    \n    # Generation Config with Function Declaration\n    tools = types.Tool(function_declarations=[set_light_values_declaration])\n    config = types.GenerateContentConfig(tools=[tools])\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Define user prompt\n    contents = [\n        types.Content(\n            role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n        )\n    ]\n    \n    # Send request with function declarations\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\", config=config, contents=contents\n    )\n    \n    print(response.candidates[0].content.parts[0].function_call)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Generation Config with Function Declaration\n    const config = {\n      tools: [{\n        functionDeclarations: [setLightValuesFunctionDeclaration]\n      }]\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Define user prompt\n    const contents = [\n      {\n        role: 'user',\n        parts: [{ text: 'Turn the lights down to a romantic level' }]\n      }\n    ];\n    \n    // Send request with function declarations\n    const response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(response.functionCalls[0]); \n    \n\nThe model then returns a `functionCall` object in an OpenAPI compatible schema specifying how to call one or more of the declared functions in order to respond to the user's question.\n\n### Python\n    \n    \n    id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'\n    \n\n### JavaScript\n    \n    \n    {\n      name: 'set_light_values',\n      args: { brightness: 25, color_temp: 'warm' }\n    }\n    \n\n### Step 3: Execute set_light_values function code\n\nExtract the function call details from the model's response, parse the arguments , and execute the `set_light_values` function in our code.\n\n### Python\n    \n    \n    # Extract tool call details\n    tool_call = response.candidates[0].content.parts[0].function_call\n    \n    if tool_call.name == \"set_light_values\":\n        result = set_light_values(**tool_call.args)\n        print(f\"Function execution result: {result}\")\n    \n\n### JavaScript\n    \n    \n    // Extract tool call details\n    const tool_call = response.functionCalls[0]\n    \n    let result;\n    if (tool_call.name === 'set_light_values') {\n      result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);\n      console.log(`Function execution result: ${JSON.stringify(result)}`);\n    }\n    \n\n### Step 4: Create User friendly response with function result and call the model again\n\nFinally, send the result of the function execution back to the model so it can incorporate this information into its final response to the user.\n\n### Python\n    \n    \n    # Create a function response part\n    function_response_part = types.Part.from_function_response(\n        name=tool_call.name,\n        response={\"result\": result},\n    )\n    \n    # Append function call and result of the function execution to contents\n    contents.append(types.Content(role=\"model\", parts=[types.Part(function_call=tool_call)])) # Append the model's function call message\n    contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n    \n    final_response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        config=config,\n        contents=contents,\n    )\n    \n    print(final_response.text)\n    \n\n### JavaScript\n    \n    \n    // Create a function response part\n    const function_response_part = {\n      name: tool_call.name,\n      response: { result }\n    }\n    \n    // Append function call and result of the function execution to contents\n    contents.push({ role: 'model', parts: [{ functionCall: tool_call }] });\n    contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });\n    \n    // Get the final response from the model\n    const final_response = await ai.models.generateContent({\n      model: 'gemini-2.0-flash',\n      contents: contents,\n      config: config\n    });\n    \n    console.log(final_response.text);\n    \n\nThis completes the function calling flow. The Model successfully used the `set_light_values` function to perform the request action of the user.\n\n## Function declarations\n\nWhen you implement function calling in a prompt, you create a `tools` object, which contains one or more _`function declarations`_. You define functions using JSON, specifically with a [select subset](https://ai.google.dev/api/caching#Schema) of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schemawr) format. A single function declaration can include the following parameters:\n\n  * `name` (string): A unique name for the function (`get_weather_forecast`, `send_email`). Use descriptive names without spaces or special characters (use underscores or camelCase).\n  * `description` (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful (\"Finds theaters based on location and optionally movie title which is currently playing in theaters.\").\n  * `parameters` (object): Defines the input parameters the function expects. \n    * `type` (string): Specifies the overall data type, such as `object`.\n    * `properties` (object): Lists individual parameters, each with: \n      * `type` (string): The data type of the parameter, such as `string`, `integer`, `boolean, array`.\n      * `description` (string): A description of the parameter's purpose and format. Provide examples and constraints (\"The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.\").\n      * `enum` (array, optional): If the parameter values are from a fixed set, use \"enum\" to list the allowed values instead of just describing them in the description. This improves accuracy (\"enum\": [\"daylight\", \"cool\", \"warm\"]).\n    * `required` (array): An array of strings listing the parameter names that are mandatory for the function to operate.\n\n\n\n## Parallel Function Calling\n\nIn addition to single turn function calling, you can also call multiple functions at once. Parallel function calling lets you execute multiple functions at once and is used when the functions are not dependent on each other. This is useful in scenarios like gathering data from multiple independent sources, such as retrieving customer details from different databases or checking inventory levels across various warehouses or performing multiple actions such as converting your apartment into a disco.\n\n### Python\n    \n    \n    power_disco_ball = {\n        \"name\": \"power_disco_ball\",\n        \"description\": \"Powers the spinning disco ball.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"power\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether to turn the disco ball on or off.\",\n                }\n            },\n            \"required\": [\"power\"],\n        },\n    }\n    \n    start_music = {\n        \"name\": \"start_music\",\n        \"description\": \"Play some music matching the specified parameters.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"energetic\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is energetic or not.\",\n                },\n                \"loud\": {\n                    \"type\": \"boolean\",\n                    \"description\": \"Whether the music is loud or not.\",\n                },\n            },\n            \"required\": [\"energetic\", \"loud\"],\n        },\n    }\n    \n    dim_lights = {\n        \"name\": \"dim_lights\",\n        \"description\": \"Dim the lights.\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"brightness\": {\n                    \"type\": \"number\",\n                    \"description\": \"The brightness of the lights, 0.0 is off, 1.0 is full.\",\n                }\n            },\n            \"required\": [\"brightness\"],\n        },\n    }\n    \n\n### JavaScript\n    \n    \n    import { Type } from '@google/genai';\n    \n    const powerDiscoBall = {\n      name: 'power_disco_ball',\n      description: 'Powers the spinning disco ball.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          power: {\n            type: Type.BOOLEAN,\n            description: 'Whether to turn the disco ball on or off.'\n          }\n        },\n        required: ['power']\n      }\n    };\n    \n    const startMusic = {\n      name: 'start_music',\n      description: 'Play some music matching the specified parameters.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          energetic: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is energetic or not.'\n          },\n          loud: {\n            type: Type.BOOLEAN,\n            description: 'Whether the music is loud or not.'\n          }\n        },\n        required: ['energetic', 'loud']\n      }\n    };\n    \n    const dimLights = {\n      name: 'dim_lights',\n      description: 'Dim the lights.',\n      parameters: {\n        type: Type.OBJECT,\n        properties: {\n          brightness: {\n            type: Type.NUMBER,\n            description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'\n          }\n        },\n        required: ['brightness']\n      }\n    };\n    \n\nCall the model with an instruction that could use all of the specified tools. This example uses a `tool_config`. To learn more you can read about [configuring function calling](/gemini-api/docs/function-calling#function_calling_modes).\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Set up function declarations\n    house_tools = [\n        types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])\n    ]\n    \n    config = {\n        \"tools\": house_tools,\n        \"automatic_function_calling\": {\"disable\": True},\n        # Force the model to call 'any' function, instead of chatting.\n        \"tool_config\": {\"function_calling_config\": {\"mode\": \"any\"}},\n    }\n    \n    # Configure the client\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Turn this place into a party!\")\n    \n    # Print out each of the function calls requested from this single call\n    print(\"Example 1: Forced function calling\")\n    for fn in response.function_calls:\n        args = \", \".join(f\"{key}={val}\" for key, val in fn.args.items())\n        print(f\"{fn.name}({args})\")\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    \n    // Set up function declarations\n    const houseFns = [powerDiscoBall, startMusic, dimLights];\n    \n    const config = {\n        tools: [{\n            functionDeclarations: houseFns\n        }],\n        // Force the model to call 'any' function, instead of chatting.\n        toolConfig: {\n            functionCallingConfig: {\n            mode: 'any'\n            }\n        }\n    };\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Create a chat session\n    const chat = ai.chats.create({\n        model: 'gemini-2.0-flash',\n        config: config\n    });\n    const response = await chat.sendMessage({message: 'Turn this place into a party!'});\n    \n    // Print out each of the function calls requested from this single call\n    console.log(\"Example 1: Forced function calling\");\n    for (const fn of response.functionCalls) {\n        const args = Object.entries(fn.args)\n            .map(([key, val]) => `${key}=${val}`)\n            .join(', ');\n        console.log(`${fn.name}(${args})`);\n    }\n    \n\nEach of the printed results reflects a single function call that the model has requested. To send the results back, include the responses in the same order as they were requested. \n\nThe Python SDK supports a feature called [automatic function calling](/gemini-api/docs/function-calling#automatic_function_calling_python_only) which converts the Python function to declarations, handles the function call execution and response cycle for you. Following is an example for our disco use case.\n\n**Note:** Automatic Function Calling is a Python SDK only feature at the moment.\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Actual implementation functions\n    def power_disco_ball_impl(power: bool) -> dict:\n        \"\"\"Powers the spinning disco ball.\n    \n        Args:\n            power: Whether to turn the disco ball on or off.\n    \n        Returns:\n            A status dictionary indicating the current state.\n        \"\"\"\n        return {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n    \n    def start_music_impl(energetic: bool, loud: bool) -> dict:\n        \"\"\"Play some music matching the specified parameters.\n    \n        Args:\n            energetic: Whether the music is energetic or not.\n            loud: Whether the music is loud or not.\n    \n        Returns:\n            A dictionary containing the music settings.\n        \"\"\"\n        music_type = \"energetic\" if energetic else \"chill\"\n        volume = \"loud\" if loud else \"quiet\"\n        return {\"music_type\": music_type, \"volume\": volume}\n    \n    def dim_lights_impl(brightness: float) -> dict:\n        \"\"\"Dim the lights.\n    \n        Args:\n            brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n    \n        Returns:\n            A dictionary containing the new brightness setting.\n        \"\"\"\n        return {\"brightness\": brightness}\n    \n    config = {\n        \"tools\": [power_disco_ball_impl, start_music_impl, dim_lights_impl],\n    }\n    \n    chat = client.chats.create(model=\"gemini-2.0-flash\", config=config)\n    response = chat.send_message(\"Do everything you need to this place into party!\")\n    \n    print(\"\\nExample 2: Automatic function calling\")\n    print(response.text)\n    # I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!\n    \n\n## Compositional Function Calling\n\nGemini 2.0 supports compositional function calling, meaning the model can chain multiple function calls together. For example, to answer \"Get the temperature in my current location\", the Gemini API might invoke both a `get_current_location()` function and a `get_weather()` function that takes the location as a parameter.\n\n**Note:** Compositional function calling is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Light control schemas\n    turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}\n    turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}\n    \n    prompt = \"\"\"\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n      \"\"\"\n    \n    tools = [\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}\n    ]\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Light control schemas\n    const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };\n    const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };\n    \n    const prompt = `\n      Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?\n    `;\n    \n    const tools = [\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }\n    ];\n    \n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n## Function calling modes\n\nThe Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the `function_calling_config`.\n\n  * `AUTO (Default)`: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n  * `ANY`: The model is constrained to always predict a function call and guarantee function schema adherence. If `allowed_function_names` is not specified, the model can choose from any of the provided function declarations. If `allowed_function_names` is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call in response to every prompt (if applicable).\n  * `NONE`: The model is _prohibited_ from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions.\n\n\n\n\n### Python\n    \n    \n    from google.genai import types\n    \n    # Configure function calling mode\n    tool_config = types.ToolConfig(\n        function_calling_config=types.FunctionCallingConfig(\n            mode=\"ANY\", allowed_function_names=[\"get_current_temperature\"]\n        )\n    )\n    \n    # Create the generation config\n    config = types.GenerateContentConfig(\n        temperature=0,\n        tools=[tools],  # not defined here.\n        tool_config=tool_config,\n    )\n    \n\n### JavaScript\n    \n    \n    import { FunctionCallingConfigMode } from '@google/genai';\n    \n    // Configure function calling mode\n    const toolConfig = {\n      functionCallingConfig: {\n        mode: FunctionCallingConfigMode.ANY,\n        allowedFunctionNames: ['get_current_temperature']\n      }\n    };\n    \n    // Create the generation config\n    const config = {\n      temperature: 0,\n      tools: tools, // not defined here.\n      toolConfig: toolConfig,\n    };\n    \n\n## Automatic Function Calling (Python Only)\n\nWhen using the Python SDK, you can provide Python functions directly as tools. The SDK automatically converts the Python function to declarations, handles the function call execution and response cycle for you. The Python SDK then automatically:\n\n  1. Detects function call responses from the model.\n  2. Call the corresponding Python function in your code.\n  3. Sends the function response back to the model.\n  4. Returns the model's final text response.\n\n\n\nTo use this, define your function with type hints and a docstring, and then pass the function itself (not a JSON declaration) as a tool:\n\n### Python\n    \n    \n    from google import genai\n    from google.genai import types\n    \n    # Define the function with type hints and docstring\n    def get_current_temperature(location: str) -> dict:\n        \"\"\"Gets the current temperature for a given location.\n    \n        Args:\n            location: The city and state, e.g. San Francisco, CA\n    \n        Returns:\n            A dictionary containing the temperature and unit.\n        \"\"\"\n        # ... (implementation) ...\n        return {\"temperature\": 25, \"unit\": \"Celsius\"}\n    \n    # Configure the client and model\n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))  # Replace with your actual API key setup\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature]\n    )  # Pass the function itself\n    \n    # Make the request\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=\"What's the temperature in London?\",\n        config=config,\n    )\n    \n    print(response.text)  # The SDK handles the function call and returns the final text\n    \n\nYou can disable automatic function calling with:\n\n### Python\n    \n    \n    # To disable automatic function calling:\n    config = types.GenerateContentConfig(\n        tools=[get_current_temperature],\n        automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)\n    )\n    \n\n### Automatic Function schema declaration\n\nAutomatic schema extraction from Python functions doesn't work in all cases. For example: it doesn't handle cases where you describe the fields of a nested dictionary-object. The API is able to describe any of the following types:\n\n### Python\n    \n    \n    AllowedType = (int | float | bool | str | list['AllowedType'] | dict[str, AllowedType])\n    \n\nTo see what the inferred schema looks like, you can convert it using [`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable):\n\n### Python\n    \n    \n    def multiply(a: float, b: float):\n        \"\"\"Returns a * b.\"\"\"\n        return a * b\n    \n    fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)\n    \n    # to_json_dict() provides a clean JSON representation.\n    print(fn_decl.to_json_dict())\n    \n\n## Multi-tool use: Combine Native Tools with Function Calling\n\nWith Gemini 2.0, you can enable multiple tools combining native tools with function calling at the same time. Here's an example that enables two tools, [Grounding with Google Search](/gemini-api/docs/grounding) and [code execution](/gemini-api/docs/code-execution), in a request using the [Live API](/gemini-api/docs/live).\n\n**Note:** Multi-tool use is a [Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.\n\n### Python\n    \n    \n    # Multiple tasks example - combining lights, code execution, and search\n    prompt = \"\"\"\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n      \"\"\"\n    \n    tools = [\n        {'google_search': {}},\n        {'code_execution': {}},\n        {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.\n    ]\n    \n    # Execute the prompt with specified tools in audio modality\n    await run(prompt, tools=tools, modality=\"AUDIO\")\n    \n\n### JavaScript\n    \n    \n    // Multiple tasks example - combining lights, code execution, and search\n    const prompt = `\n      Hey, I need you to do three things for me.\n    \n        1.  Turn on the lights.\n        2.  Then compute the largest prime palindrome under 100000.\n        3.  Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.\n    \n      Thanks!\n    `;\n    \n    const tools = [\n      { googleSearch: {} },\n      { codeExecution: {} },\n      { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.\n    ];\n    \n    // Execute the prompt with specified tools in audio modality\n    await run(prompt, {tools: tools, modality: \"AUDIO\"});\n    \n\nPython developers can try this out in the [Live API Tool Use notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).\n\n## Use Model Context Protocol (MCP)\n\n[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open standard to connect AI applications with external tools, data sources, and systems. MCP provides a common protocol for models to access context, such as functions (tools), data sources (resources), or predefined prompts. You can use models with MCP server using their tool calling capabilities.\n\nMCP servers expose the tools as JSON schema definitions, which can be used with Gemini compatible function declarations. This lets you to use a MCP server with Gemini models directly. Here, you can find an example of how to use a local MCP server with Gemini SDK and the `mcp` SDK.\n\n### Python\n    \n    \n    import asyncio\n    import os\n    from datetime import datetime\n    from google import genai\n    from google.genai import types\n    from mcp import ClientSession, StdioServerParameters\n    from mcp.client.stdio import stdio_client\n    \n    client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n    \n    # Create server parameters for stdio connection\n    server_params = StdioServerParameters(\n        command=\"npx\",  # Executable\n        args=[\"-y\", \"@philschmid/weather-mcp\"],  # Weather MCP Server\n        env=None,  # Optional environment variables\n    )\n    \n    async def run():\n        async with stdio_client(server_params) as (read, write):\n            async with ClientSession(read, write) as session:\n                # Prompt to get the weather for the current day in London.\n                prompt = f\"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?\"\n                # Initialize the connection between client and server\n                await session.initialize()\n    \n                # Get tools from MCP session and convert to Gemini Tool objects\n                mcp_tools = await session.list_tools()\n                tools = [\n                    types.Tool(\n                        function_declarations=[\n                            {\n                                \"name\": tool.name,\n                                \"description\": tool.description,\n                                \"parameters\": {\n                                    k: v\n                                    for k, v in tool.inputSchema.items()\n                                    if k not in [\"additionalProperties\", \"$schema\"]\n                                },\n                            }\n                        ]\n                    )\n                    for tool in mcp_tools.tools\n                ]\n    \n                # Send request to the model with MCP function declarations\n                response = client.models.generate_content(\n                    model=\"gemini-2.0-flash\",\n                    contents=prompt,\n                    config=types.GenerateContentConfig(\n                        temperature=0,\n                        tools=tools,\n                    ),\n                )\n    \n                # Check for a function call\n                if response.candidates[0].content.parts[0].function_call:\n                    function_call = response.candidates[0].content.parts[0].function_call\n                    print(function_call)\n                    # Call the MCP server with the predicted tool\n                    result = await session.call_tool(\n                        function_call.name, arguments=function_call.args\n                    )\n                    print(result.content[0].text)\n                    # Continue as shown in step 4 of \"How Function Calling Works\"\n                    # and create a user friendly response\n                else:\n                    print(\"No function call found in the response.\")\n                    print(response.text)\n    \n    # Start the asyncio event loop and run the main function\n    asyncio.run(run())\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from '@google/genai';\n    import { Client } from \"@modelcontextprotocol/sdk/client/index.js\";\n    import { StdioClientTransport } from \"@modelcontextprotocol/sdk/client/stdio.js\";\n    \n    // Create server parameters for stdio connection\n    const serverParams = new StdioClientTransport({\n      command: \"npx\",\n      args: [\"-y\", \"@philschmid/weather-mcp\"]\n    });\n    \n    const client = new Client(\n      {\n        name: \"example-client\",\n        version: \"1.0.0\"\n      }\n    );\n    \n    // Configure the client\n    const ai = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY });\n    \n    // Initialize the connection between client and server\n    await client.connect(serverParams);\n    \n    // Get tools from MCP session and convert to Gemini Tool objects\n    const mcpTools = await client.listTools();\n    const tools = mcpTools.tools.map((tool) => {\n      // Filter the parameters to exclude not supported keys\n      const parameters = Object.fromEntries(\n        Object.entries(tool.inputSchema).filter(([key]) => ![\"additionalProperties\", \"$schema\"].includes(key))\n      );\n      return {\n        name: tool.name,\n        description: tool.description,\n        parameters: parameters\n      };\n    });\n    \n    // Send request to the model with MCP function declarations\n    const response = await ai.models.generateContent({\n      model: \"gemini-2.0-flash\",\n      contents: \"What is the weather in London in the UK on 2024-04-04?\",\n      config: {\n        tools: [{\n          functionDeclarations: tools\n        }],\n      },\n    });\n    \n    // Check for function calls in the response\n    if (response.functionCalls && response.functionCalls.length > 0) {\n      const functionCall = response.functionCalls[0]; // Assuming one function call\n      console.log(`Function to call: ${functionCall.name}`);\n      console.log(`Arguments: ${JSON.stringify(functionCall.args)}`);\n      // Call the MCP server with the predicted tool\n      const result = await client.callTool({name: functionCall.name, arguments: functionCall.args});\n      console.log(result.content[0].text);\n      // Continue as shown in step 4 of \"How Function Calling Works\"\n      // and create a user friendly response\n    } else {\n      console.log(\"No function call found in the response.\");\n      console.log(response.text);\n    }\n    \n    // Close the connection\n    await client.close();\n    \n\n## Supported Models\n\nExperimental models are not included. You can find their capabilities on the [model overview](https://ai.google.dev/gemini-api/docs/models) page.\n\nModel | Function Calling | Parallel Function Calling | Compositional Function Calling(Live API only)  \n---|---|---|---  \nGemini 2.0 Flash | ✔️ | ✔️ | ✔️  \nGemini 2.0 Flash-Lite | X | X | X  \nGemini 1.5 Flash | ✔️ | ✔️ | ✔️  \nGemini 1.5 Pro | ✔️ | ✔️ | ✔️  \n  \n## Best Practices\n\n  * **Function and Parameter Descriptions:** Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.\n  * **Naming:** Use descriptive function names (without spaces, periods, or dashes).\n  * **Strong Typing:** Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.\n  * **Tool Selection:** While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.\n  * **Prompt Engineering:**\n    * Provide context: Tell the model its role (e.g., \"You are a helpful weather assistant.\").\n    * Give instructions: Specify how and when to use functions (e.g., \"Don't guess dates; always use a future date for forecasts.\").\n    * Encourage clarification: Instruct the model to ask clarifying questions if needed.\n  * **Temperature:** Use a low temperature (e.g., 0) for more deterministic and reliable function calls.\n  * **Validation:** If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.\n  * **Error Handling** : Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.\n  * **Security:** Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.\n  * **Token Limits:** Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.\n\n\n\n## Notes and Limitations\n\n  * Only a [subset of the OpenAPI schema](https://ai.google.dev/api/caching#FunctionDeclaration) is supported.\n  * Supported parameter types in Python are limited.\n  * Automatic function calling is a Python SDK feature only.\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-07 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-07 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/api-overview#json",
    "title": "Gemini API reference  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n  * [ API Reference ](https://ai.google.dev/api)\n\n\n\nSend feedback \n\n#  Gemini API reference \n\nThe Gemini API lets you access the latest generative models from Google. This API reference provides detailed information for the classes and methods available in the Gemini API SDKs. Pick a language and follow the setup steps to get started building.\n\nPython JavaScript Go Apps Script\n\n## Install the Gemini API library\n\n**Note:** We're rolling out a new set of Gemini API libraries, the [Google Gen AI SDK](/gemini-api/docs/sdks).\n\n## Make your first request\n\nUse the [`generateContent`](/api/generate-content#method:-models.generatecontent) method to send a request to the Gemini API.\n\n## What's next\n\nIf you're just getting started, check out the following guides, which will help you understand the Gemini API programming model:\n\n  * [Gemini API quickstart](/gemini-api/docs/quickstart)\n  * [Gemini model guide](/gemini-api/docs/models/gemini)\n  * [Prompt design](/gemini-api/docs/prompting-intro)\n\n\n\nYou might also want to check out the capabilities guides, which introduce different Gemini API features and provide code examples:\n\n  * [Text generation](/gemini-api/docs/text-generation)\n  * [Context caching](/gemini-api/docs/caching)\n  * [Embeddings](/gemini-api/docs/embeddings)\n  * [Tuning](/gemini-api/docs/model-tuning)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-04 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-04 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/gemini-for-research#gemini-academic-program",
    "title": "Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\n# Accelerate discovery with Gemini for Research\n\n[Get a Gemini API Key](https://aistudio.google.com/apikey)\n\nGemini models can be used to advance foundational research across disciplines. Here are ways that you can explore Gemini for your research:\n\n  * **Fine-tuning** : You can fine-tune Gemini models for a variety of modalities to advance your research. [Learn more](/gemini-api/docs/model-tuning/tutorial).\n  * **Analyze and control model outputs** : For further analysis, you can examine a response candidate generated by the model using tools like `Logprobs` and `CitationMetadata`. You can also configure options for model generation and outputs, such as `responseSchema`, `topP`, and `topK`. [Learn more](/api/generate-content).\n  * **Multimodal inputs** : Gemini can process images, audio, and videos, enabling a multitude of exciting research directions. [Learn more](/gemini-api/docs/vision).\n  * **Long-context capabilities** : Gemini 1.5 Flash comes with a 1-million-token context window, and Gemini 1.5 Pro comes with a 2-million-token context window. [Learn more](/gemini-api/docs/long-context).\n  * **Grow with Google** : Quickly access Gemini models through the API and Google AI Studio for production use cases. If you're looking for a Google Cloud-based platform, Vertex AI can provide additional supporting infrastructure.\n\n\n\nTo support academic research and drive cutting-edge research, Google provides access to Gemini API credits for scientists and academic researchers through the [Gemini Academic Program](/gemini-api/docs/gemini-for-research#gemini-academic-program).\n\n## Get started with Gemini\n\nThe Gemini API and Google AI Studio help you start working with Google's latest models and turn your ideas into applications that scale.\n\n### Python\n    \n    \n    from google import genai\n    \n    client = genai.Client(api_key=\"YOUR_API_KEY\")\n    response = client.models.generate_content(\n        model=\"gemini-2.0-flash\",\n        contents=\"How large is the universe?\",\n    )\n    \n    print(response.text)\n    \n\n### JavaScript\n    \n    \n    import { GoogleGenAI } from \"@google/genai\";\n    \n    const ai = new GoogleGenAI({ apiKey: \"YOUR_API_KEY\" });\n    \n    async function main() {\n      const response = await ai.models.generateContent({\n        model: \"gemini-2.0-flash\",\n        contents: \"How large is the universe?\",\n      });\n      console.log(response.text);\n    }\n    \n    await main();\n    \n\n### REST\n    \n    \n    curl \"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=YOUR_API_KEY\" \\\n    -H 'Content-Type: application/json' \\\n    -X POST \\\n    -d '{\n      \"contents\": [{\n        \"parts\":[{\"text\": \"How large is the universe?\"}]\n        }]\n       }'\n    \n\n## Featured academics\n\n\"Our research investigates Gemini as a visual language model (VLM) and its agentic behaviors in diverse environments from robustness and safety perspectives. So far, we have evaluated Gemini's robustness against distractions such as pop-up windows when VLM agents perform computer tasks, and have leveraged Gemini to analyze social interaction, temporal events as well as risk factors based on video input.\"\n\n[](https://cs.stanford.edu/~diyiy/)\n\n\"Gemini Pro and Flash, with their long context window, have been helping us in OK-Robot, our open-vocabulary mobile manipulation project. Gemini enables complex natural language queries and commands over the robot's \"memory\": in this case, previous observations made by the robot over a long operation duration. Mahi Shafiullah and I are also using Gemini to decompose tasks into code that the robot can execute in the real world.\"\n\n[](https://www.lerrelpinto.com/)\n\n## Gemini Academic Program\n\nQualified academic researchers (such as faculty, staff, and PhD students) in [supported countries](/gemini-api/docs/available-regions) can apply to receive Gemini API credits and higher rate limits for research projects. This support enables higher throughput for scientific experiments and advances research.\n\nWe are particularly interested in the research areas in the following section, but we welcome applications from diverse scientific disciplines:\n\n  * **Evaluations and benchmarks** : Community-endorsed evaluation methods that can provide a strong performance signal in areas such as factuality, safety, instruction following, reasoning, and planning.\n\n  * **Accelerating scientific discovery to benefit humanity** : Potential applications of AI in interdisciplinary scientific research, including areas such as rare and neglected diseases, experimental biology, materials science, and sustainability.\n\n  * **Embodiment and interactions** : Utilizing large language models to investigate novel interactions within the fields of embodied AI, ambient interactions, robotics, and human-computer interaction.\n\n  * **Emergent capabilities** : Exploring new agentic capabilities required to enhance reasoning and planning, and how capabilities can be expanded during inference (e.g., by utilizing Gemini Flash).\n\n  * **Multimodal interaction and understanding** : Identifying gaps and opportunities for multimodal foundational models for analysis, reasoning, and planning across a variety of tasks.\n\n\n\n\nEligibility: Only individuals (faculty members, researchers or equivalent) affiliated with a valid academic institution, or academic research organization can apply. Note that API access and credits will be granted and removed at Google's discretion. We review applications on a monthly basis.\n\n### Start researching with the Gemini API\n\n[Apply now](https://forms.gle/HMviQstU8PxC5iCt5)\n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-01 UTC.\n\n[[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-01 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/pricing#veo-2",
    "title": "Gemini Developer API Pricing  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\n#  Gemini Developer API Pricing \n\nThe Gemini API \"free tier\" is offered through the API service with lower rate limits for testing purposes. Google AI Studio usage is completely free in all available countries. The Gemini API \"paid tier\" comes with [higher rate limits](/gemini-api/docs/rate-limits), additional features, and different data handling.\n\n[Upgrade to the Paid Tier](https://aistudio.google.com/apikey)\n\n## Gemini 2.5 Pro Preview\n\n[Try it in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\nOur state-of-the-art multipurpose model, which excels at coding and complex reasoning tasks.\n\nPreview models may change before becoming stable and generally available.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge, use \"gemini-2.5-pro-exp-03-25\" | $1.25, prompts <= 200k tokens  \n$2.50, prompts > 200k tokens  \nOutput price (including thinking tokens) | Free of charge, use \"gemini-2.5-pro-exp-03-25\" | $10.00, prompts <= 200k tokens  \n$15.00, prompts > 200k  \nContext caching price | Not available | Not available  \nGrounding with Google Search | Free of charge, up to 500 RPD | 1,500 RPD (free), then $35 / 1,000 requests  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Gemini 2.0 Flash\n\n[Try it in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash)\n\nOur most balanced multimodal model with great performance across all tasks, with a 1 million token context window, and built for the era of Agents.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | $0.10 (text / image / video)  \n$0.70 (audio)  \nOutput price | Free of charge | $0.40  \nContext caching price | Free of charge | $0.025 / 1,000,000 tokens (text/image/video)  \n$0.175 / 1,000,000 tokens (audio)  \nAvailable April 15, 2025  \nContext caching (storage) | Free of charge, up to 1,000,000 tokens of storage per hour  \nAvailable April 15, 2025 | $1.00 / 1,000,000 tokens per hour  \nAvailable April 15, 2025  \nTuning price | Not available | Not available  \nGrounding with Google Search | Free of charge, up to 500 RPD | 1,500 RPD (free), then $35 / 1,000 requests  \nLive API | Free of charge | **Beginning April 11, 2025:**  \nInput: $0.35 (text), $2.10 (audio / image [video])  \nOutput: $1.50 (text), $8.50 (audio)  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Gemini 2.0 Flash-Lite\n\n[Try it in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\nOur smallest and most cost effective model, built for at scale usage.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | $0.075  \nOutput price | Free of charge | $0.30  \nContext caching price | Available April 15, 2025 | Available April 15, 2025  \nContext caching (storage) | Available April 15, 2025 | Available April 15, 2025  \nTuning price | Not available | Not available  \nGrounding with Google Search | Not available | Not available  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Imagen 3\n\n[Try it in ImageFX](https://labs.google/fx/tools/image-fx)\n\nOur state-of-the-art image generation model, available to developers on the paid tier of the Gemini API.\n\n| Free Tier | Paid Tier, per Image in USD  \n---|---|---  \nImage price | Not available | $0.03  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Veo 2\n\n[Try the API](/gemini-api/docs/video)\n\nOur state-of-the-art video generation model, available to developers on the paid tier of the Gemini API.\n\n| Free Tier | Paid Tier, per second in USD  \n---|---|---  \nVideo price | Not available | $0.35  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Gemma 3\n\n[Try Gemma 3](https://aistudio.google.com/prompts/new_chat?model=gemma-3-27b-it)\n\nOur lightweight, state-of the art, open model built from the same technology that powers our Gemini models.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | Not available  \nOutput price | Free of charge | Not available  \nContext caching price | Free of charge | Not available  \nContext caching (storage) | Free of charge | Not available  \nTuning price | Not available | Not available  \nGrounding with Google Search | Not available | Not available  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Gemini 1.5 Flash\n\n[Try it in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\nOur fastest multimodal model with great performance for diverse, repetitive tasks and a 1 million token context window.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | $0.075, prompts <= 128k tokens  \n$0.15, prompts > 128k tokens  \nOutput price | Free of charge | $0.30, prompts <= 128k tokens  \n$0.60, prompts > 128k tokens  \nContext caching price | Free of charge, up to 1 million tokens of storage per hour | $0.01875, prompts <= 128k tokens  \n$0.0375, prompts > 128k tokens  \nContext caching (storage) | Free of charge | $1.00 per hour  \nTuning price | Token prices are the same for tuned models  \nTuning service is free of charge. | Token prices are the same for tuned models  \nTuning service is free of charge.  \nGrounding with Google Search | Not available | $35 / 1K grounding requests (for up to 5K requests per day).  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Gemini 1.5 Flash-8B\n\n[Try it in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash-8b)\n\nOur smallest model for lower intelligence use cases, with a 1 million token context window.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | $0.0375, prompts <= 128k tokens  \n$0.075, prompts > 128k tokens  \nOutput price | Free of charge | $0.15, prompts <= 128k tokens  \n$0.30, prompts > 128k tokens  \nContext caching price | Free of charge, up to 1 million tokens of storage per hour | $0.01, prompts <= 128k tokens  \n$0.02, prompts > 128k tokens  \nContext caching (storage) | Free of charge | $0.25 per hour  \nTuning price | Token prices are the same for tuned models  \nTuning service is free of charge. | Token prices are the same for tuned models  \nTuning service is free of charge.  \nGrounding with Google Search | Not available | $35 / 1K grounding requests (for up to 5K requests per day).  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Gemini 1.5 Pro\n\n[Try it in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\nOur highest intelligence Gemini 1.5 series model, with a breakthrough 2 million token context window.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | $1.25, prompts <= 128k tokens  \n$2.50, prompts > 128k tokens  \nOutput price | Free of charge | $5.00, prompts <= 128k tokens  \n$10.00, prompts > 128k tokens  \nContext caching price | Not available | $0.3125, prompts <= 128k tokens  \n$0.625, prompts > 128k tokens  \nContext caching (storage) | Not available | $4.50 per hour  \nTuning price | Not available | Not available  \nGrounding with Google Search | Not available | $35 / 1K grounding requests (for up to 5K requests per day).  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n## Text Embedding 004\n\nOur state-of-the-art text embedding model.\n\n| Free Tier | Paid Tier, per 1M tokens in USD  \n---|---|---  \nInput price | Free of charge | Not available  \nOutput price | Free of charge | Not available  \nTuning price | Not available | Not available  \nUsed to improve our products | [Yes](/gemini-api/terms) | [No](/gemini-api/terms)  \n  \n[*] Google AI Studio usage is free of charge in all [available regions](/gemini-api/docs/available-regions). See [Billing FAQs](/gemini-api/docs/billing) for details.\n\n[**] Prices may differ from the prices listed here and the prices offered on Vertex AI. For Vertex prices, see the [Vertex AI pricing page](https://cloud.google.com/vertex-ai/generative-ai/pricing).\n\n[***] If you are using [dynamic retrieval](/gemini-api/docs/grounding) to optimize costs, only requests that contain at least one grounding support URL from the web in their response are charged for Grounding with Google Search. Costs for Gemini always apply. Rate limits are subject to change.\n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-09 UTC.\n\n[[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-09 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/models#veo-2",
    "title": "Gemini models  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Gemini models \n\n2.5 Pro experiment\n\nOur most powerful thinking model with maximum response accuracy and state-of-the-art performance \n\n  * Input audio, images, video, and text, get text responses\n  * Tackle difficult problems, analyze large databases, and more\n  * Best for complex coding, reasoning, and multimodal understanding\n\n\n\n2.0 Flash spark\n\nOur newest multimodal model, with next generation features and improved capabilities \n\n  * Input audio, images, video, and text, get text responses\n  * Generate code and images, extract data, analyze files, generate graphs, and more\n  * Low latency, enhanced performance, built to power agentic experiences\n\n\n\n2.0 Flash-Lite \n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency \n\n  * Input audio, images, video, and text, get text responses\n  * Outperforms 1.5 Flash on the majority of benchmarks\n  * A 1 million token context window and multimodal input, like Flash 2.0\n\n\n\n## Model variants\n\nThe Gemini API offers different models that are optimized for specific use cases. Here's a brief overview of Gemini variants that are available:\n\nModel variant | Input(s) | Output | Optimized for  \n---|---|---|---  \nGemini 2.5 Pro Preview   \n`gemini-2.5-pro-preview-03-25` | Audio, images, videos, and text | Text | Enhanced thinking and reasoning, multimodal understanding, advanced coding, and more  \nGemini 2.0 Flash   \n`gemini-2.0-flash` | Audio, images, videos, and text | Text, images (experimental), and audio (coming soon) | Next generation features, speed, thinking, realtime streaming, and multimodal generation  \nGemini 2.0 Flash-Lite   \n`gemini-2.0-flash-lite` | Audio, images, videos, and text | Text | Cost efficiency and low latency  \nGemini 1.5 Flash   \n`gemini-1.5-flash` | Audio, images, videos, and text | Text | Fast and versatile performance across a diverse variety of tasks  \nGemini 1.5 Flash-8B   \n`gemini-1.5-flash-8b` | Audio, images, videos, and text | Text | High volume and lower intelligence tasks  \nGemini 1.5 Pro   \n`gemini-1.5-pro` | Audio, images, videos, and text | Text |  Complex reasoning tasks requiring more intelligence   \nGemini Embedding   \n`gemini-embedding-exp` | Text | Text embeddings |  Measuring the relatedness of text strings   \nImagen 3   \n`imagen-3.0-generate-002` | Text | Images | Our most advanced image generation model  \nVeo 2   \n`veo-2.0-generate-001` | Text, images | Video | High quality video generation  \nGemini 2.0 Flash Live   \n`gemini-2.0-flash-live-001` | Audio, video, and text | Text, audio | Low-latency bidirectional voice and video interactions  \n  \nYou can view the rate limits for each model on the [rate limits page](/gemini-api/docs/rate-limits).\n\n### Gemini 2.5 Pro Preview\n\nGemini 2.5 Pro is our state-of-the-art thinking model, capable of reasoning over complex problems in code, math, and STEM, as well as analyzing large datasets, codebases, and documents using long context. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.5-pro-preview-03-25)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | Paid: `gemini-2.5-pro-preview-03-25`, Experimental: `gemini-2.5-pro-exp-03-25`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 65,536  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search grounding** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Not supported **Live API** Not supported **Thinking** Supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.5-pro-preview-03-25`\n  * Experimental: `gemini-2.5-pro-exp-03-25`\n\n  \ncalendar_monthLatest update | March 2025  \ncognition_2Knowledge cutoff | January 2025  \n  \n### Gemini 2.0 Flash\n\nGemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, native tool use, multimodal generation, and a 1M token context window. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text, images (experimental), and audio(coming soon)  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Coming soon **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Experimental **Native tool use** Supported **Audio generation** Coming soon **Live API** Experimental **Thinking** Experimental  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash`\n  * Stable: `gemini-2.0-flash-001`\n  * Experimental: `gemini-2.0-flash-exp` and `gemini-2.0-flash-exp-image-generation` point to the same underlying model\n  * Experimental: `gemini-2.0-flash-thinking-exp-01-21`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 2.0 Flash-Lite\n\nA Gemini 2.0 Flash model optimized for cost efficiency and low latency.\n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-lite)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-lite`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Not supported **Code execution** Not supported **Search** Not supported **Image generation** Not supported **Native tool use** Not supported **Audio generation** Not supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-2.0-flash-lite`\n  * Stable: `gemini-2.0-flash-lite-001`\n\n  \ncalendar_monthLatest update | February 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini 1.5 Flash\n\nGemini 1.5 Flash is a fast and versatile multimodal model for scaling across diverse tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-latest`\n  * Latest stable: `gemini-1.5-flash`\n  * Stable:\n    * `gemini-1.5-flash-001`\n    * `gemini-1.5-flash-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Gemini 1.5 Flash-8B\n\nGemini 1.5 Flash-8B is a small model designed for lower intelligence tasks. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-flash)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-flash-8b`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 3,600 **Maximum video length** 1 hour **Maximum audio length** Approximately 9.5 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-flash-8b-latest`\n  * Latest stable: `gemini-1.5-flash-8b`\n  * Stable:\n    * `gemini-1.5-flash-8b-001`\n\n  \ncalendar_monthLatest update | October 2024  \n  \n### Gemini 1.5 Pro\n\nTry [Gemini 2.0 Pro Experimental](/gemini-api/docs/models/experimental-models#available-models), our most advanced Gemini model to date. \n\nGemini 1.5 Pro is a mid-size multimodal model that is optimized for a wide-range of reasoning tasks. 1.5 Pro can process large amounts of data at once, including 2 hours of video, 19 hours of audio, codebases with 60,000 lines of code, or 2,000 pages of text. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-1.5-pro)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-1.5-pro`  \nsaveSupported data types |  **Inputs** Audio, images, video, and text **Output** Text  \ntoken_autoToken limits[*] |  **Input token limit** 2,097,152 **Output token limit** 8,192  \nmovie_infoAudio/visual specs |  **Maximum number of images per prompt** 7,200 **Maximum video length** 2 hours **Maximum audio length** Approximately 19 hours  \nhandymanCapabilities |  **System instructions** Supported **JSON mode** Supported **JSON schema** Supported **Adjustable safety settings** Supported **Caching** Supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Live API** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Latest: `gemini-1.5-pro-latest`\n  * Latest stable: `gemini-1.5-pro`\n  * Stable:\n    * `gemini-1.5-pro-001`\n    * `gemini-1.5-pro-002`\n\n  \ncalendar_monthLatest update | September 2024  \n  \n### Imagen 3\n\nImagen 3 is our highest quality text-to-image model, capable of generating images with even better detail, richer lighting and fewer distracting artifacts than our previous models. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `imagen-3.0-generate-002`  \nsaveSupported data types |  **Input** Text **Output** Images  \ntoken_autoToken limits[*] |  **Input token limit** N/A **Output images** Up to to 4  \ncalendar_monthLatest update | February 2025  \n  \n### Veo 2\n\nVeo 2 is our high quality text- and image-to-video model, capable of generating detailed videos, capturing the artistic nuance in your prompts. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `veo-2.0-generate-001`  \nsaveSupported data types |  **Input** Text, image **Output** Video  \ntoken_autoLimits |  **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2  \ncalendar_monthLatest update | April 2025  \n  \n### Gemini 2.0 Flash Live\n\nThe Gemini 2.0 Flash Live model works with the Live API to enable low-latency bidirectional voice and video interactions with Gemini. The model can process text, audio, and video input, and it can provide text and audio output. \n\n[Try in Google AI Studio](https://aistudio.google.com?model=gemini-2.0-flash-live-001)\n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/gemini-2.0-flash-live-001`  \nsaveSupported data types |  **Inputs** Audio, video, and text **Output** Text, and audio  \ntoken_autoToken limits[*] |  **Input token limit** 1,048,576 **Output token limit** 8,192  \nhandymanCapabilities |  **Structured outputs** Supported **Caching** Not supported **Tuning** Not supported **Function calling** Supported **Code execution** Supported **Search** Supported **Image generation** Not supported **Native tool use** Supported **Audio generation** Supported **Thinking** Not supported  \n123Versions |  Read the [model version patterns](/gemini-api/docs/models/gemini#model-versions) for more details.\n\n  * Preview: `gemini-2.0-flash-live-001`\n\n  \ncalendar_monthLatest update | April 2025  \ncognition_2Knowledge cutoff | August 2024  \n  \n### Gemini Embedding Experimental\n\n`Gemini embedding` achieves a [SOTA performance](https://deepmind.google/research/publications/157741/) across many key dimensions including code, multi-lingual, and retrieval. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `gemini-embedding-exp-03-07`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 8,192 **Output dimension size** Elastic, supports: 3072, 1536, or 768  \ncalendar_monthLatest update | March 2025  \n  \n### Text Embedding and Embedding\n\n#### Text Embedding\n\nTry our new [experimental Gemini embedding model](https://developers.googleblog.com/en/gemini-embedding-text-model-now-available-gemini-api/) which achieves state-of-the-art performance. \n\n[Text embeddings](/gemini-api/docs/embeddings) are used to measure the relatedness of strings and are widely used in many AI applications. \n\n`text-embedding-004` achieves a [stronger retrieval performance and outperforms existing models](https://arxiv.org/pdf/2403.20327) with comparable dimensions, on the standard MTEB embedding benchmarks. \n\n##### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  **Gemini API** `models/text-embedding-004`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | April 2024  \n  \n#### Embedding\n\n**Note:** Text Embedding is the newer version of the Embedding model. If you're creating a new project, use Text Embedding. \n\nYou can use the Embedding model to generate [text embeddings](/gemini-api/docs/embeddings) for input text.\n\nThe Embedding model is optimized for creating embeddings with 768 dimensions for text of up to 2,048 tokens. \n\n##### Embedding model details\n\nProperty | Description  \n---|---  \nid_cardModel code |  `models/embedding-001`  \nsaveSupported data types |  **Input** Text **Output** Text embeddings  \ntoken_autoToken limits[*] |  **Input token limit** 2,048 **Output dimension size** 768  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Not supported  \ncalendar_monthLatest update | December 2023  \n  \n### AQA\n\nYou can use the AQA model to perform [Attributed Question-Answering](/gemini-api/docs/semantic_retrieval) (AQA)–related tasks over a document, corpus, or a set of passages. The AQA model returns answers to questions that are grounded in provided sources, along with estimating answerable probability. \n\n#### Model details\n\nProperty | Description  \n---|---  \nid_cardModel code | `models/aqa`  \nsaveSupported data types |  **Input** Text **Output** Text  \nlanguageSupported language | English  \ntoken_autoToken limits[*] |  **Input token limit** 7,168 **Output token limit** 1,024  \nswap_driving_apps_wheelRate limits[**] | 1,500 requests per minute  \nencryptedAdjustable safety settings | Supported  \ncalendar_monthLatest update | December 2023  \n  \nSee the [examples](/examples) to explore the capabilities of these model variations.\n\n[*] A token is equivalent to about 4 characters for Gemini models. 100 tokens are about 60-80 English words. \n\n## Model version name patterns\n\nGemini models are available in either _preview_ or _stable_ versions. In your code, you can use one of the following model name formats to specify which model and version you want to use.\n\n  * **Latest:** Points to the cutting-edge version of the model for a specified generation and variation. The underlying model is updated regularly and might be a preview version. Only exploratory testing apps and prototypes should use this alias.\n\nTo specify the latest version, use the following pattern: `<model>-<generation>-<variation>-latest`. For example, `gemini-1.0-pro-latest`.\n\n  * **Latest stable:** Points to the most recent stable version released for the specified model generation and variation.\n\nTo specify the latest stable version, use the following pattern: `<model>-<generation>-<variation>`. For example, `gemini-1.0-pro`.\n\n  * **Stable:** Points to a specific stable model. Stable models usually don't change. Most production apps should use a specific stable model.\n\nTo specify a stable version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-1.0-pro-001`.\n\n  * **Experimental:** Points to an experimental model which may not be suitable for production use. We release experimental models to gather feedback and get our latest updates into the hands of developers quickly.\n\nTo specify an experimental version, use the following pattern: `<model>-<generation>-<variation>-<version>`. For example, `gemini-2.0-pro-exp-02-05`.\n\n\n\n\n## Experimental models\n\nIn addition to the production ready models, the Gemini API offers experimental models which may not be suitable for production use.\n\nWe release experimental models to gather feedback, get our latest updates into the hands of developers quickly, and highlight the pace of innovation happening at Google. What we learn from experimental launches informs how we release models more widely. An experimental model can be swapped for another without prior notice. We don't guarantee that an experimental model will become a stable model in the future.\n\n### Previous experimental models\n\nAs new versions or stable releases become available, we remove and replace experimental models. You can find the previous experimental models we released in the following section along with the replacement version:\n\nModel code | Base model | Replacement version  \n---|---|---  \n`gemini-2.0-pro-exp-02-05` | Gemini 2.0 Pro Experimental | `gemini-2.5-pro-exp-03-25`  \n`gemini-2.0-flash-exp` | Gemini 2.0 Flash | `gemini-2.0-flash`  \n`gemini-exp-1206` | Gemini 2.0 Pro | `gemini-2.0-pro-exp-02-05`  \n`gemini-2.0-flash-thinking-exp-1219` | Gemini 2.0 Flash Thinking | `gemini-2.0-flash-thinking-exp-01-21`  \n`gemini-exp-1121` | Gemini | `gemini-exp-1206`  \n`gemini-exp-1114` | Gemini | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0827` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-pro-exp-0801` | Gemini 1.5 Pro | `gemini-exp-1206`  \n`gemini-1.5-flash-8b-exp-0924` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n`gemini-1.5-flash-8b-exp-0827` | Gemini 1.5 Flash-8B | `gemini-1.5-flash-8b`  \n  \n## Supported languages\n\nGemini models are trained to work with the following languages:\n\n  * Arabic (`ar`)\n  * Bengali (`bn`)\n  * Bulgarian (`bg`)\n  * Chinese simplified and traditional (`zh`)\n  * Croatian (`hr`)\n  * Czech (`cs`)\n  * Danish (`da`)\n  * Dutch (`nl`)\n  * English (`en`)\n  * Estonian (`et`)\n  * Finnish (`fi`)\n  * French (`fr`)\n  * German (`de`)\n  * Greek (`el`)\n  * Hebrew (`iw`)\n  * Hindi (`hi`)\n  * Hungarian (`hu`)\n  * Indonesian (`id`)\n  * Italian (`it`)\n  * Japanese (`ja`)\n  * Korean (`ko`)\n  * Latvian (`lv`)\n  * Lithuanian (`lt`)\n  * Norwegian (`no`)\n  * Polish (`pl`)\n  * Portuguese (`pt`)\n  * Romanian (`ro`)\n  * Russian (`ru`)\n  * Serbian (`sr`)\n  * Slovak (`sk`)\n  * Slovenian (`sl`)\n  * Spanish (`es`)\n  * Swahili (`sw`)\n  * Swedish (`sv`)\n  * Thai (`th`)\n  * Turkish (`tr`)\n  * Ukrainian (`uk`)\n  * Vietnamese (`vi`)\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-08 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-08 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/usage-policies#abuse-monitoring",
    "title": "Additional usage policies  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Additional usage policies \n\nThis page includes additional usage policies for the Gemini API.\n\n## Abuse monitoring\n\nGoogle is committed to the responsible development and use of AI. To ensure the safety and integrity of the Gemini API, we have created these policy guidelines. By using the Gemini API, you agree to the following guidelines, the [Gemini API Additional Terms of Service](/gemini-api/terms) and Generative AI [Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy).\n\n### How We Monitor for Misuse\n\nGoogle's Trust and Safety Team employs a combination of automated and manual processes to detect potential misuse of the Gemini API and enforce our policies.\n\n  * **Automated Detection:** Automated systems scan API usage for violations of our Prohibited Use Policy, such as hate speech, harassment, sexually explicit content, and dangerous content.\n  * **Manual Detection:** If a project consistently exhibits suspicious activity, it may be flagged for manual review by authorized Google personnel.\n\n\n\n### How We Handle Data\n\nTo help with abuse monitoring, Google retains the following data for fifty-five (55) days:\n\n  * **Prompts:** The text prompts you submit to the API.\n  * **Contextual Information:** Any additional context you provide with your prompts.\n  * **Output:** The responses generated by the Gemini API.\n\n\n\n### How We Investigate Potential Issues\n\nWhen prompts or model outputs are flagged by safety filters and abuse detection systems described above, authorized Google employees may assess the flagged content, and either confirm or correct the classification or determination based on predefined guidelines and policies. Data can be accessed for human review only by authorized Google employees via an internal governance assessment and review management platform. When data is logged for abuse monitoring, it is used solely for the purpose of policy enforcement and is not used to train or fine-tune any AI/ML models.\n\n### Working with You on Policy Compliance\n\nIf your use of Gemini doesn't align with our policies, we may take the following steps:\n\n  * **Get in touch:** We may reach out to you through email to understand your use case and explore ways to bring your usage into compliance.\n  * **Temporary usage limits:** We may limit your access to the Gemini API.\n  * **Temporary suspension:** We may temporarily pause your access to the Gemini API.\n  * **Account closure:** As a last resort, and for serious violations, we may permanently close your access to the Gemini API and other Google services.\n\n\n\n### Scope\n\nThese policy guidelines apply to the use of the Gemini API and AI Studio.\n\n## Inline Preference Voting\n\nIn Google AI Studio, you might occasionally see a side-by-side comparison of two different responses to your prompt. This is part of our Inline Preference Voting system. You'll be asked to choose which response you prefer. This helps us understand which model outputs users find most helpful.\n\n### Why are we doing this?\n\nWe're constantly working to improve our AI models and services. Your feedback through Inline Preference Voting helps us provide, improve, and develop Google products and services and machine learning technologies, including Google's enterprise features, products and services, consistent with the [Gemini API Additional Terms of Service](/gemini-api/terms) and [Privacy Policy](https://policies.google.com/privacy).\n\n### What data is included in Feedback?\n\nTo make informed decisions about our models, we collect certain data when you participate in Inline Preference Voting:\n\n  * **Prompts and Responses:** We record all prompts and responses, including any uploaded content, in the conversation you submitted feedback about. We also record the two response options that you selected from. This helps us understand the context of your preference.\n  * **Your Vote:** We record which response you preferred. This is the core of the feedback we're collecting.\n  * **Usage Details:** This includes information about which model generated the response and other technical and operational details about your usage of this feature.\n\n\n\n### Your Privacy\n\nWe take your privacy seriously. Google takes steps to protect your privacy as part of this process. This includes disconnecting this data from your Google Account, API key, and Cloud project before reviewers see or annotate it. **Do not submit feedback on conversations that include sensitive, confidential, or personal information.**\n\n### Opting Out\n\nYou'll have the option to skip the Inline Preference Voting when it appears.\n\nThank you for helping us improve Google AI Studio!\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-03-24 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-03-24 UTC.\"],[],[]] \n"
  },
  {
    "url": "https://ai.google.dev/gemini-api/docs/video#generate-from-images",
    "title": "Generate video using Veo  |  Gemini API  |  Google AI for Developers",
    "content": "Gemini 2.5 Pro Preview is now available for production use! [Learn more](https://blog.google/technology/google-deepmind/gemini-model-thinking-updates-march-2025/)\n\n  * [ Home ](https://ai.google.dev/)\n  * [ Gemini API ](https://ai.google.dev/gemini-api)\n  * [ Models ](https://ai.google.dev/gemini-api/docs)\n\n\n\nSend feedback \n\n#  Generate video using Veo \n\nThe Gemini API provides access to [Veo 2](https://deepmind.google/technologies/veo/), Google's state-of-the-art video generation model. Veo is designed to help you build next-generation AI applications that transform user prompts and images into high quality video assets.\n\nThis guide will help you get started with Veo using the Gemini API.\n\n## About Veo\n\n**Note:** Veo is a **paid feature** and will not run in the Free tier. Visit the [Pricing](/gemini-api/docs/pricing#veo-2) page for more details.\n\nVeo is Google's most capable video generation model to date. It generates videos in a wide range of cinematic and visual styles, capturing prompt nuance to render intricate details consistently across frames.\n\nTo learn more and see example output, check out the [Google DeepMind Veo overview](https://deepmind.google/technologies/veo/).\n\n### Specifications\n\n**Modalities** | \n\n  * Text-to-video generation\n  * Image-to-video generation\n\n  \n---|---  \n**Request latency** | \n\n  * Min: 11 seconds\n  * Max: 6 minutes (during peak hours)\n\n  \n**Variable length generation** | 5-8 seconds  \n**Resolution** | 720p  \n**Frame rate** | 24fps  \n**Aspect ratio** | \n\n  * 16:9 - landscape\n  * 9:16 - portrait\n\n  \n**Input languages (text-to-video)** | English  \n**Note:** Check out the [Models](/gemini-api/docs/models#veo-2), [Pricing](/gemini-api/docs/pricing#veo-2), and [Rate limits](/gemini-api/docs/rate-limits) pages for more usage limitations for Veo.\n\nVideos created by Veo are watermarked using [SynthID](https://deepmind.google/technologies/synthid/), our tool for watermarking and identifying AI-generated content, and are passed through safety filters and memorization checking processes that help mitigate privacy, copyright and bias risks.\n\n### Before you begin\n\nBefore calling the Gemini API, ensure you have [your SDK of choice](/gemini-api/docs/downloads) installed, and a [Gemini API key](/gemini-api/docs/api-key) configured and ready to use.\n\nTo use Veo with the Google Gen AI SDKs, ensure that you have one of the following versions installed:\n\n  * [Python](https://pypi.org/project/google-genai/) v1.10.0 or later\n  * [TypeScript and JavaScript](https://www.npmjs.com/package/@google/genai) v0.8.0 or later\n  * [Go](https://pkg.go.dev/google.golang.org/genai) v1.0.0 or later\n\n\n\n## Generate videos\n\nThis section provides code examples for generating videos using text prompts and using images.\n\n### Generate from text\n\nYou can use the following code to generate videos with Veo:\n\n### Python\n    \n    \n    import time\n    from google import genai\n    from google.genai import types\n    \n    client = genai.Client()  # read API key from GOOGLE_API_KEY\n    \n    operation = client.models.generate_videos(\n        model=\"veo-2.0-generate-001\",\n        prompt=\"Panning wide shot of a calico kitten sleeping in the sunshine\",\n        config=types.GenerateVideosConfig(\n            person_generation=\"dont_allow\",  # \"dont_allow\" or \"allow_adult\"\n            aspect_ratio=\"16:9\",  # \"16:9\" or \"9:16\"\n        ),\n    )\n    \n    while not operation.done:\n        time.sleep(20)\n        operation = client.operations.get(operation)\n    \n    for n, generated_video in enumerate(operation.response.generated_videos):\n        client.files.download(file=generated_video.video)\n        generated_video.video.save(f\"video{n}.mp4\")  # save the video\n    \n    \n\n### REST\n    \n    \n    # Use curl to send a POST request to the predictLongRunning endpoint\n    # The request body includes the prompt for video generation\n    curl \"${BASE_URL}/models/veo-2.0-generate-001:predictLongRunning?key=${GOOGLE_API_KEY}\" \\\n      -H \"Content-Type: application/json\" \\\n      -X \"POST\" \\\n      -d '{\n        \"instances\": [{\n            \"prompt\": \"Panning wide shot of a calico kitten sleeping in the sunshine\"\n          }\n        ],\n        \"parameters\": {\n          \"aspectRatio\": \"16:9\",\n          \"personGeneration\": \"dont_allow\",\n        }\n      }' | tee result.json | jq .name | sed 's/\"//g' > op_name\n    \n\nThis code takes about 2-3 minutes to run, though it may take longer if resources are constrained. Once it's done running, you should see a video that looks something like this:\n\nIf you see an error message instead of a video, this means that resources are constrained and your request couldn't be completed. In this case, run the code again.\n\nGenerated videos are stored on the server for 2 days, after which they are removed. If you want to save a local copy of your generated video, you must run `result()` and `save()` within 2 days of generation.\n\n### Generate from images\n\nYou can also generate videos using images. The following code generates an image using Imagen, then uses the generated image as the starting frame for the generated video.\n\nFirst, generate an image using [Imagen](/gemini-api/docs/image-generation#imagen):\n\n### Python\n    \n    \n    prompt=\"Panning wide shot of a calico kitten sleeping in the sunshine\",\n    \n    imagen = client.models.generate_images(\n        model=\"imagen-3.0-generate-002\",\n        prompt=prompt,\n        config=types.GenerateImagesConfig(\n          aspect_ratio=\"16:9\",\n          number_of_images=1\n        )\n    )\n    \n    images[0]\n    \n\nThen, generate a video using the resulting image as the first frame:\n\n### Python\n    \n    \n    operation = client.models.generate_videos(\n        model=\"veo-2.0-generate-001\",\n        prompt=prompt,\n        image = images[0],\n        config=types.GenerateVideosConfig(\n          # person_generation only accepts \"dont_allow\" for image-to-video\n          aspect_ratio=\"16:9\",  # \"16:9\" or \"9:16\"\n          number_of_videos=2\n        ),\n    )\n    \n    # Wait for videos to generate\n     while not operation.done:\n      time.sleep(20)\n      operation = client.operations.get(operation)\n    \n    for n, video in enumerate(operation.response.generated_videos):\n        fname = f'with_image_input{n}.mp4'\n        print(fname)\n        video.save(fname)\n    \n\n## Veo model parameters\n\n(Naming conventions vary by programming language.)\n\n  * `prompt`: The text prompt for the video. When present, the `image` parameter is optional.\n  * `image`: The image to use as the first frame for the video. When present, the `prompt` parameter is optional.\n  * `negativePrompt`: Text string that describes anything you want to _discourage_ the model from generating\n  * `aspectRatio`: Changes the aspect ratio of the generated video. Supported values are `\"16:9\"` and `\"9:16\"`. The default is `\"16:9\"`.\n  * `personGeneration`: Allow the model to generate videos of people. The following values are supported: \n    * Text-to-video generation: \n      * `\"dont_allow\"`: Don't allow the inclusion of people or faces.\n      * `\"allow_adult\"`: Generate videos that include adults, but not children.\n    * Image-to-video generation: \n      * `\"dont_allow\"`: Default and only value for image-to-video generation.\n  * `numberOfVideos`: Output videos requested, either `1` or `2`.\n  * `durationSeconds`: Length of each output video in seconds, between `5` and `8`.\n  * `enhance_prompt`: Enable or disable the prompt rewriter. Enabled by default.\n\n\n\n## Things to try\n\nTo get the most out of Veo, incorporate video-specific terminology into your prompts. Veo understands a wide range of terms related to:\n\n  * **Shot composition:** Specify the framing and number of subjects in the shot (e.g., \"single shot,\" \"two shot,\" \"over-the-shoulder shot\").\n  * **Camera positioning and movement:** Control the camera's location and movement using terms like \"eye level,\" \"high angle,\" \"worms eye,\" \"dolly shot,\" \"zoom shot,\" \"pan shot,\" and \"tracking shot.\"\n  * **Focus and lens effects:** Use terms like \"shallow focus,\" \"deep focus,\" \"soft focus,\" \"macro lens,\" and \"wide-angle lens\" to achieve specific visual effects.\n  * **Overall style and subject:** Guide Veo's creative direction by specifying styles like \"sci-fi,\" \"romantic comedy,\" \"action movie,\" or \"animation.\" You can also describe the subjects and backgrounds you want, such as \"cityscape,\" \"nature,\" \"vehicles,\" or \"animals.\"\n\n\n\n## Veo prompt guide\n\nThis section of the Veo guide contains examples of videos you can create using Veo, and shows you how to modify prompts to produce distinct results.\n\n### Safety filters\n\nVeo applies safety filters across Gemini to help ensure that generated videos and uploaded photos don't contain offensive content. Prompts that violate our [terms and guidelines](/gemini-api/docs/usage-policies#abuse-monitoring) are blocked.\n\n### Prompt writing basics\n\nGood prompts are descriptive and clear. To get your generated video as close as possible to what you want, start with identifying your core idea, and then refine your idea by adding keywords and modifiers.\n\nThe following elements should be included in your prompt:\n\n  * **Subject** : The object, person, animal, or scenery that you want in your video.\n  * **Context** : The background or context in which the subject is placed.\n  * **Action** : What the subject is doing (for example, _walking_ , _running_ , or _turning their head_).\n  * **Style** : This can be general or very specific. Consider using specific film style keywords, such as _horror film_ , _film noir_ , or animated styles like _cartoon_ style.\n  * **Camera motion** : [Optional] What the camera is doing, such as _aerial view_ , _eye-level_ , _top-down shot_ , or _low-angle shot_.\n  * **Composition** : [Optional] How the shot is framed, such as _wide shot_ , _close-up_ , or _extreme close-up_.\n  * **Ambiance** : [Optional] How the color and light contribute to the scene, such as _blue tones_ , _night_ , or _warm tones_.\n\n\n\n#### More tips for writing prompts\n\nThe following tips help you write prompts that generate your videos:\n\n  * **Use descriptive language** : Use adjectives and adverbs to paint a clear picture for Veo.\n  * **Provide context** : If necessary, include background information to help your model understand what you want.\n  * **Reference specific artistic styles** : If you have a particular aesthetic in mind, reference specific artistic styles or art movements.\n  * **Utilize prompt engineering tools** : Consider exploring prompt engineering tools or resources to help you refine your prompts and achieve optimal results. For more information, visit [Introduction to prompt design](/gemini-api/docs/prompting-intro).\n  * **Enhance the facial details in your personal and group images** : Specify facial details as a focus of the photo like using the word _portrait_ in the prompt.\n\n\n\n### Example prompts and output\n\nThis section presents several prompts, highlighting how descriptive details can elevate the outcome of each video.\n\n#### Icicles\n\nThis video demonstrates how you can use the elements of prompt writing basics in your prompt.\n\n**Prompt** | **Generated output**  \n---|---  \nClose up shot (composition) of melting icicles (subject) on a frozen rock wall (context) with cool blue tones (ambiance), zoomed in (camera motion) maintaining close-up detail of water drips (action).  |   \n  \n#### Man on the phone\n\nThese videos demonstrate how you can revise your prompt with increasingly specific details to get Veo to refine the output to your liking.\n\n**Prompt** | **Generated output** | **Analysis**  \n---|---|---  \nThe camera dollies to show a close up of a desperate man in a green trench coat. He's making a call on a rotary-style wall phone with a green neon light. It looks like a movie scene. |  | This is the first generated video based on the prompt.  \nA close-up cinematic shot follows a desperate man in a weathered green trench coat as he dials a rotary phone mounted on a gritty brick wall, bathed in the eerie glow of a green neon sign. The camera dollies in, revealing the tension in his jaw and the desperation etched on his face as he struggles to make the call. The shallow depth of field focuses on his furrowed brow and the black rotary phone, blurring the background into a sea of neon colors and indistinct shadows, creating a sense of urgency and isolation. |  | A more detailed prompt results in a video that is more focused with a richer environment.  \nA video with smooth motion that dollies in on a desperate man in a green trench coat, using a vintage rotary phone against a wall bathed in an eerie green neon glow. The camera starts from a medium distance, slowly moving closer to the man's face, revealing his frantic expression and the sweat on his brow as he urgently dials the phone. The focus is on the man's hands, his fingers fumbling with the dial as he desperately tries to connect. The green neon light casts long shadows on the wall, adding to the tense atmosphere. The scene is framed to emphasize the isolation and desperation of the man, highlighting the stark contrast between the vibrant glow of the neon and the man's grim determination. |  | Adding more detail gives the subject a realistic expression and creates an intense and vibrant scene.  \n  \n#### Snow leopard\n\nThis example demonstrates the output Veo might generate for a simple prompt.\n\n**Prompt** | **Generated output**  \n---|---  \nA cute creature with snow leopard-like fur is walking in winter forest, 3D cartoon style render. |   \n  \n#### Running snow leopard\n\nThis prompt has more detail and demonstrates generated output that might be closer to what you want in your video.\n\n**Prompt** | **Generated output**  \n---|---  \nCreate a short 3D animated scene in a joyful cartoon style. A cute creature with snow leopard-like fur, large expressive eyes, and a friendly, rounded form happily prances through a whimsical winter forest. The scene should feature rounded, snow-covered trees, gentle falling snowflakes, and warm sunlight filtering through the branches. The creature's bouncy movements and wide smile should convey pure delight. Aim for an upbeat, heartwarming tone with bright, cheerful colors and playful animation. |   \n  \n### Examples by writing elements\n\nThese examples show you how to refine your prompts by each basic element.\n\n#### Subject\n\nThis example shows you how to specify a subject description.\n\n**Subject description** | **Prompt** | **Generated output**  \n---|---|---  \nThe description can include a subject, or multiple subjects and actions. Here, our subject is \"white concrete apartment building.\" | An architectural rendering of a white concrete apartment building with flowing organic shapes, seamlessly blending with lush greenery and futuristic elements |   \n  \n#### Context\n\nThis example shows you how to specify context.\n\n**Context** | **Prompt** | **Generated output**  \n---|---|---  \nThe background or context in which the subject will be placed is very important. Try placing your subject in a variety of backgrounds like on a busy street, or in outer space. | A satellite floating through outer space with the moon and some stars in the background. |   \n  \n#### Action\n\nThis example shows you how to specify action.\n\n**Action** | **Prompt** | **Generated output**  \n---|---|---  \nWhat is the subject doing like walking, running, or turning their head. | A wide shot of a woman walking along the beach, looking content and relaxed towards the horizon at sunset. |   \n  \n#### Style\n\nThis example shows you how to specify style.\n\n**Style** | **Prompt** | **Generated output**  \n---|---|---  \nYou can add keywords to improve generation quality and steer it closer to intended style, such as shallow depth of field, movie still, minimalistic, surreal, vintage, futuristic, or double-exposure. | Film noir style, man and woman walk on the street, mystery, cinematic, black and white. |   \n  \n#### Camera motion\n\nThis example shows you how to specify camera motion.\n\n**Camera motion** | **Prompt** | **Generated output**  \n---|---|---  \nOptions for camera motion include POV shot, aerial view, tracking drone view, or tracking shot. | A POV shot from a vintage car driving in the rain, Canada at night, cinematic. |   \n  \n#### Composition\n\nThis example shows you how to specify composition.\n\n**Composition** | **Prompt** | **Generated output**  \n---|---|---  \nHow the shot is framed (wide shot, close-up, low angle). | Extreme close-up of a an eye with city reflected in it. |   \n| Create a video of a wide shot of surfer walking on a beach with a surfboard, beautiful sunset, cinematic. |   \n  \n#### Ambiance\n\nThis example shows you how to specify ambiance.\n\n**Ambiance** | **Prompt** | **Generated output**  \n---|---|---  \nColor palettes play a vital role in photography, influencing the mood and conveying intended emotions. Try things like \"muted orange warm tones,\" \"natural light,\" \"sunrise\" or \"sunset\". For example, a warm, golden palette can infuse a romantic and atmospheric feel into a photograph. | A close-up of a girl holding adorable golden retriever puppy in the park, sunlight. |   \n| Cinematic close-up shot of a sad woman riding a bus in the rain, cool blue tones, sad mood. |   \n  \n### Use reference images to generate videos\n\nYou can bring images to life by using Veo's [image-to-video](/gemini-api/docs/video#generate-from-images) capability. You can use existing assets, or try [Imagen](/gemini-api/docs/image-generation) to generate something new.\n\n**Prompt** | **Generated output**  \n---|---  \nBunny with a chocolate candy bar. |   \nBunny runs away. |   \n  \n### Negative prompts\n\nNegative prompts can be a powerful tool to help specify elements you _don't_ want in the video. Describe what you want to discourage the model from generating after the phrase \"Negative prompt\". Follow these tips:\n\n  * ❌ Don't use instructive language or words like _no_ or _don't_. For example, \"No walls\" or \"don't show walls\".\n\n  * ✅ Do describe what you don't want to see. For example, \"wall, frame\", which means that you don't want a wall or a frame in the video.\n\n\n**Prompt** | **Generated output**  \n---|---  \nGenerate a short, stylized animation of a large, solitary oak tree with leaves blowing vigorously in a strong wind. The tree should have a slightly exaggerated, whimsical form, with dynamic, flowing branches. The leaves should display a variety of autumn colors, swirling and dancing in the wind. The animation should use a warm, inviting color palette. |   \nGenerate a short, stylized animation of a large, solitary oak tree with leaves blowing vigorously in a strong wind. The tree should have a slightly exaggerated, whimsical form, with dynamic, flowing branches. The leaves should display a variety of autumn colors, swirling and dancing in the wind. The animation should use a warm, inviting color palette.   \n  \nWith negative prompt - urban background, man-made structures, dark, stormy, or threatening atmosphere. |   \n  \n### Aspect ratios\n\nGemini Veo video generation supports the following two aspect ratios:\n\n**Aspect ratio** | **Description**  \n---|---  \nWidescreen or 16:9 | The most common aspect ratio for televisions, monitors, and mobile phone screens (landscape). Use this when you want to capture more of the background, like in scenic landscapes.  \nPortrait or 9:16 | Rotated widescreen. This aspect ratio has been popularized by short form video applications, such as Youtube shorts. Use this for portraits or tall objects with strong vertical orientations, such as buildings, trees, waterfall, or buildings.  \n  \n  \n#### Widescreen\n\nThis prompt is an example of the widescreen aspect ratio of 16:9.\n\n**Prompt** | **Generated output**  \n---|---  \nCreate a video with a tracking drone view of a man driving a red convertible car in Palm Springs, 1970s, warm sunlight, long shadows. |   \n  \n#### Portrait\n\nThis prompt is an example of the portrait aspect ratio of 9:16.\n\n**Prompt** | **Generated output**  \n---|---  \nCreate a video highlighting the smooth motion of a majestic Hawaiian waterfall within a lush rainforest. Focus on realistic water flow, detailed foliage, and natural lighting to convey tranquility. Capture the rushing water, misty atmosphere, and dappled sunlight filtering through the dense canopy. Use smooth, cinematic camera movements to showcase the waterfall and its surroundings. Aim for a peaceful, realistic tone, transporting the viewer to the serene beauty of the Hawaiian rainforest. |   \n  \n## What's next\n\n  * Gain more experience generating AI videos with the [Veo Colab](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Get_started_Veo.ipynb).\n\n\n\nSend feedback \n\nExcept as otherwise noted, the content of this page is licensed under the [Creative Commons Attribution 4.0 License](https://creativecommons.org/licenses/by/4.0/), and code samples are licensed under the [Apache 2.0 License](https://www.apache.org/licenses/LICENSE-2.0). For details, see the [Google Developers Site Policies](https://developers.google.com/site-policies). Java is a registered trademark of Oracle and/or its affiliates.\n\nLast updated 2025-04-09 UTC.\n\nNeed to tell us more?  [[[\"Easy to understand\",\"easyToUnderstand\",\"thumb-up\"],[\"Solved my problem\",\"solvedMyProblem\",\"thumb-up\"],[\"Other\",\"otherUp\",\"thumb-up\"]],[[\"Missing the information I need\",\"missingTheInformationINeed\",\"thumb-down\"],[\"Too complicated / too many steps\",\"tooComplicatedTooManySteps\",\"thumb-down\"],[\"Out of date\",\"outOfDate\",\"thumb-down\"],[\"Samples / code issue\",\"samplesCodeIssue\",\"thumb-down\"],[\"Other\",\"otherDown\",\"thumb-down\"]],[\"Last updated 2025-04-09 UTC.\"],[],[]] \n"
  }
]